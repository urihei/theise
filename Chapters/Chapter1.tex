% Chapter 1

\chapter{Introduction} % Main chapter title

\label{intro} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------
\section{Different models}
\subsection{Graphical Models}
\subsubsection{Notations and Definitions}
Probability is the chance that a certain event will take place.
Hence we need to define all the possible events.
We will denote an event by $x \in \cX$, and we will restrict it to be discrete $ |\cX| \in \naturalNumbers$
\footnote{Through out this thesis I will focus only on discrete Graphical models, even though continuous Graphical Models is very useful model}.
An event can be a composite of simpler $p \in \naturalNumbers$ sub events, having $\xx = [x_1, \ldots, x_{p}] \in \cX^p$ to be the event.
After defining the event the probability is  a function from the space of events to a number between zero and one $\pr:\cX^{p} \to (0,1]$ with an important restriction that $\sum_{\xx \in \cX^{p}} \pr(\xx) = 1$\footnote{Note that we exclude zero since we require positive distribution.}\footnote{Without loss of generality for ease of notation we assumed that all variables have the same domain. This assumption can be easily removed.}.

In many real world situations each variable (sub-event) is directly depend on only small subset of other variables.
In probability terminology this sum down to saying that $P(x_i |\xx_{\cI}) = P(x_i|\xx_{[1,\ldots,p] \setminus i}),\  \forall \xx \in \cX^{P}$ where $\cI \subset \{1,\ldots,p\}$  is a small set of variable's indexes.
We will denote by $\nei{i}$ the smallest set of vertices indexes that satisfy this relation and by $d_i = |\nei{i}|$ the number of neighbors of variable $i$.  
% $\nei{i} = \arg\min \{|\cI| : \cI \subseteq \{1,\ldots,p\}, P(x_i|\xx_{[1,\ldots,p] \setminus i}) = P(x_i |\xx_{\cI}) \}$.
Now this dependency relation can be describe by a graph $G(V,E)$ where $V = \{v_1,\ldots, v_p\}$ is the set of vertices and  $E \subseteq V \times V$ is the graph edges.
Each variable $x_i$ will be assign a vertex $v_i$. 
An edge between $v_i$ and $v_j$ exist if  $j \in \nei{i}$\footnote{if $i \in \nei{j}$ than $j \in \nei{i}$ otherwise $P(\xx) = P(\xx_{[1,\ldots,p]\setminus i,j}) P(x_j|\xx_{\nei{j}}) P(x_i|x_j,\xx_{\nei{i}\setminus j})$ and $P(\xx) = P(\xx_{[1,\ldots,p]\setminus i,j}) P(x_i|\xx_{[1,\ldots,p]\setminus i,j}) P(x_j|\xx_{\nei(j)})$ which result in $P(x_i|x_j,\xx_{\nei{i}\setminus j}) =   P(x_i|\xx_{[1,\ldots,p]\setminus i,j})$ in contradiction to $j \in \nei{i}$}.
Now by the Hammersleyâ€“Clifford theorem\cite{hammersley1971markov}\footnote{We defined the probability as positive}, we can write out the probability as
\be
\label{eq:ciluqe_prob}
P(\xx; \thetav) = \frac{1}{Z(\thetav)} \exp{\sum_{c \in \cC}\theta_{c}(\xx_{c})}
\ee
Where $\theta_{c}(\xx_{c}): \cX^{|c|} \to \reals$  is a function, $\cC$ is the set of all maximal cliques\footnote{a clique is a set of vertices that have an edge to all vertices in the set. A maximal clique mean that no other vertex can be added to the clique and it still be a clique.} in $G$ and $Z = \sum_{\xx \in \cX^p} \exp{\sum_{c \in \cC}\theta_{c}(\xx_{c})}$ is the normalization constant called the partition function.
Even from the ability to describe the probability in this compact form\footnote{the naive description length is $O(|\cX|^p)$ while with this representation it is $O(\sum_{c \in \cC} |\cX|^{|c|})$}the merit of connecting probability and graph is clear, in other words the potential of graphical models \cite{koller2009probabilistic}.


In this thesis we will assume that $|c| = 2$, this in itself does not restrict the probability function but it can increase the cardinality of $\cX$\footnote{Basically each clique is assigned to a variable and an edge exist with cliques sharing some variables. Note that new cardinality of the variable is the product the cardinalities of the clique variables }.
With this restriction the probability can be written as
\be
\label{eq:basic_model}
P(\xx; \thetav) = \frac{1}{Z(\thetav)} \exp{\sum_{ij \in E} \theta_{ij}(x_i,x_j) + \sum_{i \in V} \theta_i(x_i)}
\ee
were we define $\thetav \in \Omega^G $\footnote{In times when the dependence in $G$ is clear it will be drooped.}  as concatenation of $\theta_i(x_i)$ and $\theta_{ij}(x_i,x_j)$ for all $i \in V$ $ij \in E$ and $x_i,x_j \in \cX$ hence $\Omega^G$ is the space of real vectors with dimension $d = |V||\cX|+ |E||\cX|^2$ ordered in specific way $\Omega^G \equiv \Re^{d}$.\footnote{I will assume (if not said otherwise) that $\thetav$ is unique - some of the vectors values are fixed to zero.}
The normalization constant $Z(\thetav)$ - the partition function, is defined as, 
\be
\label{eq:partition_function}
Z(\thetav) = \sum_{\xx \in\cX}\exp{\sum_{ij \in E} \theta_{ij}(x_i,x_j) + \sum_{i \in V} \theta_i(x_i)}
\ee
In what follow the log of the partition function will be of importance, hence it will be denoted as $A(\thetav) = \log(Z(\thetav)$. Moreover the derivative of it has an interesting characteristic \cite{wainwright2008graphical},
\bea
\label{eq:pratition_derivative}
\frac{ \partial A(\thetav)}{\partial \theta_{k}(x_k)} &=& \sum_{\substack{\xx \in \cX\\
s.t.\ \xx_k = x_k}} \frac{1}{Z(\thetav)}\exp{\sum_{ij \in E} \theta_{ij}(x_i,x_j) + \sum_{i \in V} \theta_i(x_i)} = \mu^{\thetav}_i(x_i)\\
\frac{ \partial A(\thetav)}{\partial \theta_{ij}(x_i,x_j)} &=& \mu^{\thetav}_{ij}(x_i, x_j)\\
\eea
Where we define the marginals as $\mu_i^{\thetav}(x_i) \equiv P(x_i;\thetav)$ and $\mu_{ij}^{\thetav}(x_i,x_j) \equiv P(x_i,x_j;\thetav)$.
Note that when clear, the dependence in $\thetav$ will be drooped. 
Moreover ordering the marginals the same way as $\thetav$ will result in a vector $\muv$.

The fact that the derivative of $A(\thetav)$ by $\thetav_{ij}(x_i,x_j)$ equals the marginal probability of these variables,
illustrate its importance. 
Other feature of it would be presented at \secref{sec:variational_methods}.
\subsubsection{What can we do with it?}
There are two basic questions that are of interest.
The most basic question is which event is the most likely, in other words: what is the assignment that is most probable (MAP - maximum a posteriori).
\be
\label{eq:MAP}
\xx_{MAP} = \arg \max_{\xx \in \cX^p} P(\xx; \thetav) 
\ee
Now sometimes we are not interested in the whole event but only in small fraction of its variables.
We are looking for the marginal probability of some sub-event $\xx_{\cI}$.
This can be calculated by summing over all the rest of variables:
\be
\label{eq:MAR}
P(\xx_{\cI};\thetav) = \sum_{\substack{\zz \in \cX^{P}\\
 s.t.\  \zz_{\cI} = \xx_{\cI} }} P(\zz;\thetav)
\ee
Note that these two problems can be asked when part of the variables was seen hene exponential number of different queries on the same model. 
Both inference tasks can easily be infer by going over all the possible event.
This however is too expensive since it is exponential in the number of variables.
And indeed in the general case MAP\cite{shimony1994finding} is NP complete problem while MAR\cite{cooper1990computational} is \#P complete (MAR is even hard to approximate \cite{roth1996hardness}).

The situation is not as hopeless as was described.
First, the complexity parameter is not the number of variables but a feature of the graph called tree-width\cite{robertson1983graph,robertson1994quickly}.
The tree width of the graph is usually smaller than the number of vertices, but given a graph it is NP-complete to determine if the graph has tree-width smaller than $k$\cite{Arnborg:1987}.
For example if we restrict the structure of the dependency graph to a tree both problems can be solved in polynomial time since the tree-width of a tree is one.
But there are other model restriction that is known to allow polynomial inference, such as planar graph \cite{jaakkola2007approximate} and fast mixing models\cite{jerrum1993polynomial}.
However in general case the inference problem is hard and approximate algorithm must be used. I will discuss approximation algorithm  in \secref{sec:approx} . 
\subsubsection{Pros and cons}
We just claim that inference is hard so why should one use such model.
I will now go over the reasons why one should use graphical models.
One may answer is that it is optimal - in classification for example, if we know the exact conditional distribution and can infer the MAP we will have the optimal solution. This situation is very unrealistic: we usually do not know the true model moreover the inference is NP hard so we usually use some approximation. So we return to the question when should we use it?

In many situations we need to understand our model. In biology, we learn the structure of the model to better understand the learned biology system. If we rule out a user misuse, we may need the ability to explain our decision. In graphical models each variable has meaning hence understanding the learned model is easier then other model - SVM for example.

The model in graphical models usually lack the distinction between features and outcome. In situation that such separation is artificial graphical model give power that is not found else where. We can ask different queries on the same model and have consist answers. Take for example medical situation, we are interested on the probability of several symptoms, each is a different query.

This bring us the next two important features, handling missing values and structured output. In many situations we do not always see all of the variables. Graphical models, handle missing values inherently and such cases need not special treatment ( as is the case in other models). Lastly, the queries result is not necessarily a single variable. Queries on several variables are possible and will take into account the relation between these variables.

To conclude, graphical models is a very powerful toll. This power come with a price,  inferring and learning (as we will see later) are hard and approximation algorithm must be used in most cases.
\subsection{Support Vector Machine}
\subsubsection{Notations and Definitions}
One basic problem in machine learning is classification.
Given features of an instance we want the ability to classify - assign one label out of to small discrete group.
Formally, given $\xx \in \Re^p$, a classifying function (more accurately class of functions $f \in \mathcal{F}$) is defined by $f: \Re^p \times \Re^q \to \cL$ where $|\cL|,q \in \naturalNumbers$  - it take as input the instance to classify and the parameters of $f$  and return a label.
Learning such a function mean to learn its parameters: we want to find parameters  $\ww$ that minimize $\expect{(\xx,y) \sim P}{l(f,\xx, \ww ,y)}$ where $l : \mathcal{F} \times\Re^p\times \Re^q \times \cL \to \Re^{+}$ is the loss function - the cost of returning label $f(\xx,\ww)$ where the true label is $y$.

The above objective is not achievable in most cases, hence some relaxation of the problem must take place.  
First, we usually do not know the real distribution, but only have a sample of it: given a samples $D = \{(\xx_i,y_i)\}_{i=1}^N$  where $\xx_i \in \Re^p, y_i \in \cL$ sampled from some fixed distribution $P$.
We will want to minimize our objective on this sample - minimizing the empirical loss\footnote{Since the data sample is finite we need to regularize the parameters, here we select the euclidean (or Frobenius  norm in the multiclass classification) to be the regularize and $\lambda$ to be its weight's parameter.}.
Moreover we will restrict $f$ to be linear $f(\xx, W) = \argmax_{r \in \cL}\{ \ww_r \cdot \phi(\xx)\}$ on some known feature map $\phi: \Re^p \to \Re^q, q \in \naturalNumbers$ \footnote{We abuse notation, here by giving the instance of group $\cL$ and its index the same notation}.
Finally even though, the most intuitive loss function is the zero one loss $\deltaF{f(\xx) \neq y}$.
This function is very hard to optimize since it is non-convex.
One known surrogate is the hinge loss function $\max\{0,1- y(\ww \cdot \phi(\xx))\}$ where $\cL = \{-1,1\}$ while in the multiclass case we have the extension $ \max_{r \in \cL}\{\deltaF{r \ne y} - \ww_y \cdot \phi(\xx) + \ww_r \cdot \phi(\xx)\}$ \cite{crammer2002algorithmic}. All together the objective is:
\be
\label{eq:svm_obj}
\min_{W \in \Re^{|\cL|,q}} \sum_{i=1}^M \max_{r \in \cL}\{\deltaF{r \ne y} - \ww_{y_i} \cdot \phi(\xx_{i}) + \ww_r \cdot \phi(\xx_i)\} + \lambda \|W\|^2
\ee

Taking the dual and some algebra gives:
\bea
\label{eq:svm_obj_dual}
\max_{A \in \Re^{M,|\cL|}}&& -\sum_{i,j}^M(\phi(\xx_i)\cdot\phi(\xx_j))(\alphav_i\cdot\alphav_j) + \beta\sum_i^M\alphav_i \cdot \bold{1}_{y_i}\\
\text{subject to}:&& \forall i\ \ \alphav_i \leq \bold{1}_i, \text{ and } \alphav_i \cdot \bold{1} = 0
\eea
Where $\lambda$ is the relularization parameter, $\bold{1}_i$ is the all zero vector except the $i$ coordinate and $\bold{1}$ is the all one vector.
Note that in this form it is enough to calculate the dot product between any two samples - no need to calculate the map $\phi$.
This is know as the kernel trick \cite{hofmann2008kernel}.
\subsubsection{Pros and Cons}
Support Vector Machine, optimize objective that is very close to the optimal one. Learning is a convex optimization problem hence efficient and classification time complexity is linear in the number of features. In short its is very efficient method.
But this efficiency come with the price of power of expression since we consider only linear models, to overcome this shortcoming we use kernels.

When using kernel SVM the efficiency depend on the number of samples\footnote{More accurately it depend on the margin of the problem but in practice it is not usually come into effect.} since usually the number of samples that participate in the classifier scales with the number of samples( the number of non-zero $\alphav_i$). Hence both learning and classification can be relatively costing.
Moreover in both linear and kernel SVM, there is no principle way to integrate prior knowledge. This has grate importance in many fields where there is strong dependencies between the features and known structure for example the task of object recognition.

\subsection{Neural Networks}
Neural networks, as SVM, try to solve classification problems.
But instead of restricting  the function class to linear function, it choose a different class of cassification functions that we will now describe.
Remember that a classification function is a function $f: \Re^p \times \Re^q \to \cL$ so a neural networks have the form:\bea
\label{eq:neural_networks}
f(\xx, \wparams) &=& \argmax_{r \in \cL} \zz_{L}(\xx, \wparamsi{L})_r\\
\zz_{k}(\xx, \wparamsi{k}) &=& \sgn{W_k \zz_{k-1}(\xx, \wparamsi{k-1}}\\
\zz_{1}(\xx,W_1) &=&  \sgn{W_1 \xx}
\eea
Where $\sgn{\xx}$ is a function $\Re^q \to \Re^q$, $\wparamsi{k} = W_1,\ldots,W_k$ and $\wparams = \wparamsi{L}$ where $L$ is the depth of the network.
The $\sgn{x}$ function is known as the activation function a simple non linear function.
Examples for such functions are $\sgn{x} = \deltaF{x > 0}$ (zero one), $\sgn{x} = \deltaF{x > 0}x$ (ReLu) or $\sgn{x} = \frac{1}{1+\exp{-x}}$ (sigmoid)\footnote{We abuse notation by using the same symbol for function on scalars and vectors - applying the scalar function independently on each of the vector coordinates (result is a vector).}.

This functions class result in a non convex function hence non convex optimization problem.
But it allow for a quick calculation of the derivative using the chain role and optimization by back propagation\cite{williams1986learning}.

\section{Approximate inference}
\label{sec:approx}
Graphical models is a way to model distributions.
In order it to be of use an algorithm should be found that could answer queries based on the model, in other words preform inference.
As written above, exact inference in the general is NP-complete.
Hence the importance of finding an approximate inference.
I will focus in this introduction only on the problem of finding the marginals (not the MAP problem) since in learning it is of more use as I will show later.

The approximate inference algorithm can be divided in to two main categories: variational inference, and sampling.
I will now try to go over the main principles of the two categories.
\subsection{Variational Methods}
\label{sec:variational_methods}
I will follow in this section the excellent work by \cite{wainwright2008graphical}.
We are interesting in finding $\muv$ the marginals.
First we need to define our search space - the marginal-ploytope $\margpoly^G$.
\be
\margpoly^G = \left\{ \muv \in [0,1]^d\ \left| 
\begin{array}{lr}
  \exists \thetav \in \Omega^G\ s.t. \\
  \forall i \in V \land \forall x_i \in \cX &   P(x_i;\thetav) = \mu_i(x_i)\\
  \forall ij \in E \land \forall x_i, x_j \in \cX &P(x_i,x_j;\thetav) = \mu_{ij}(x_i,x_j)
\end{array} \right. \right\}
\ee
Now we can define the conjugate dual 
\be
A^*(\muv) = \sup_{\thetav \in \Omega^G} \left\{\muv \cdot \thetav - A(\thetav)\right\}
\ee
Two things should be noted. The first is that if $\muv \in \margpoly^G$ than $A^*(\muv) = -H(P(\xx;\thetav(\muv)))$ the conjugate of the log partition function is equals to the entropy of the probability that its marginals are $\muv$.
Second that this is similar to the maximum likelihood objective (see \secref{sec:max_likelihood}).
It can be shown that $A(\thetav)$ is a convex function of $\thetav$ hence ${A^{*}}^* = A$ hence we can write the variational expression of $A(\thetav)$ as
\bean
A(\thetav) &=& \sup_{\muv \in \margpoly^G}\left \{ \muv \cdot \thetav + H(P(\xx;\thetav(\muv))) \right\} \label{eq:variation_A} \\
\muv^{\thetav}&=& \arg \sup_{\muv \in \margpoly^G}\left \{ \muv \cdot \thetav + H(P(\xx;\thetav(\muv))) \right\} \label{eq:arg_variation_A}
\eean
\eqref{eq:variation_A}and \eqref{eq:arg_variation_A} present a new way to infer  the marginals and  calculate the partition function.
Not surprisingly, solving this optimization is hard in itself for two reasons.
The first, the marginal-polytope is a very complex  body - its description is exponential in $d$ the size of $\muv$.
The second is calculating the entropy is a hard problem by itself.
But the merit in these representation is the ability to approximate these two quantities - the entropy and the marginal polytope.

I will present two method of approximation, the first limit both the marginal-polytope and the entropy to a sub-graph of $G$ such both are easy to calculate. 
For example if we select the independent graph it sum down to  the mean-field method \cite{peterson1987mean}.
The second method give different approximation to the entropy and marginal polytope.
If we outer bound the marginal-polytope by the local-polytope ( defined in \eqref{eq:local_polytope}) and the entropy by the Bethe-entropy it result in the  Bethe approximation that is related to the known algorithm Belief-Propagation\cite{pearl1986fusion, yedidia2000generalized}.
\subsubsection{Mean Field}
\subsubsection{Belief Propagation}
Belief-Propagation (BP) is a message passing algorithm.
In each cycle\footnote{The order of the messages can effect convergence, hence can be arbitrarily be changed. see for example \cite{elidan2012residual}.} a message is passed from a vertex to all its neighbors.
The message reflect the source vertex belief on the destination vertex probability.
The message is a function of the factor connecting the two vertices and the messages from all neighbors except the destination neighbor.
It continue to cycle until the difference in the messages is very small or after a fix number of cycles.
The messages can be written as, 
\be
\label{eq:belief_propagation}
m_{i \to j}^{t}(x_j) \propto \sum_{x_i \in\cX} \exp{\theta_{i,j}(x_i,x_j)+\theta_{i}(x_i)}\prod_{k \in \nei{i} \setminus j } m_{k \to i}^{t-1} (x_i)
\ee 
Now the resulted pseudo-marginals is calculated by,
\bean
\tau_i(x_i) &\propto& \exp{\theta_i(x_i)} \prod_{k \in \nei{i}} m^T_{k \to i}(x_i) \label{eq:bp_single_marginal}\\
\tau_{ij}(x_i,x_j) &\propto& \exp{\theta_{ij}(x_i,x_j)+\theta_i(x_i)+\theta_j(x_j)} \prod_{k \in \nei{i}\setminus j} m_{k \to i}^{T} (x_i) \prod_{k \in \nei{j}\setminus i}m_{k \to j}^{T} (x_j)\label{eq:bp_pairwise_marginal}
\eean

If the model graph is a tree - BP is exact, the pseudo-marginals are the exact marginals $\tauv^{\thetav} = \muv^{\thetav}$ .
If there are loops in the graph the quality of the pseudo-marginals is unknown. 
Moreover, there are cases where BP  does not converge and even when it does the resulted pseudo-marginals may depend on the initial messages $\bold{m}^0$.
Despite the above BP give good results in practice \cite{}.

The first step in understanding BP is to give meaning to the resulted pseudo-marginals.
This was done by \cite{yedidia2000generalized, yedidia2003understanding} they found that the fix point of BP are local minima of the Bethe free energy of the system\footnote{Later \cite{heskes2002stable} refined this result to \textbf{stable} fix point of BP are  local minima of the Bethe approximation}.
So it gave meaning (from physics) to the fix point of BP.
Moreover it allow theoretical analysis of BP. 
I will now return on this result.

As mention earlier, the approximation of \eqref{eq:variation_A} include two parts:
first lets defined the Bethe entropy - an approximation of the true entropy,
\bean
H_B(\tauv) &=& -\sum_{i} (1-d_i)\sum_{x_i}\tau_i(x_i)\log\tau_i(x_i) -\sum_{ij}\sum_{x_i,x_j}\tau_{ij}(x_i,x_j)\log\tau_{ij}(x_i,x_j)\label{eq:bethe_entropy}\\
&=&-\sum_{i}\sum_{x_i}\tau_i(x_i)\log\tau_i(x_i) -\sum_{ij}\sum_{x_i,x_j}\tau_{ij}(x_i,x_j)\log\frac{\tau_{ij}(x_i,x_j)}{\tau_i(x_i)\tau_j(x_j)} \label{eq:bethe_entorpy_information}
\eean
Second  the approximation of the marginal-polytope with the local-polytope.
\be
\label{eq:local_polytope}
\lclmargpoly = \left\{\tauv \in \Re^d\left| 
\begin{array}{lr}
\forall i \in V & \sum_{x_i} \tau_i(x_i) = 1\\
\forall i \in V, \forall x_i \in \cX,\ \forall j \in \nei{j}& \sum_{x_j}\tau_{ij}(x_i,x_j) = \tau_i(x_i)\\
\forall i \in V,\ \forall ij \in E,\ x_i,x_j \in \cX & \tau_i(x_i) \geq 0,\ \tau_{ij}(x_i,x_j) \geq 0
\end{array}\right.\right\}
\ee 
We are now able to write the Bethe approximation to the partition function.
\be
\label{eq:bethe_approximation}
A_B(\thetav) = \sup_{\tauv \in \lclmargpoly} \left\{\thetav \cdot \tauv + H_B(\tauv)\right\}
\ee
And the Bethe energy is $-A_B(\thetav)$.\\
In contrast to the variational free energy, the Bethe free energy is simple to calculate, since the local polytope is describe by a linear number of  constrains and the Bethe entropy calculation is straight forward .
This however come with a price.
First the result must not belong to the marginal-polytope hence not a ``real'' marginal.
Second, the resulted optimization is no longer convex - the Bethe entropy is non-convex function while the true one is convex.
Hence the complexity of optimizing this objective is not clear - I am not aware on any such result.
%So the importance of \eqref{eq:bethe_approximation} to inference, is its connection to BP.

After defining the Bethe energy , to see the connection to BP we just need to write the Lagrangian of this optimization,
\bea
\mathcal{L}(\thetav,\tauv,\lambdav) &=& -\thetav \cdot \tauv - H_B(\tauv) \\
&+& \sum_i \lambda_i \left(1-\sum_{x_i} \tau_i(x_i)\right) + \sum_{ij}\lambda_{ij}\left(1-\sum_{x_i,x_j}\tau_{ij}(x_i,x_j)\right)\\
&+& \sum_{i} \sum_{j \in \nei{i}}\sum_{x_i}\lambda_{j \to i, x_i}\left(\tau_i(x_i)-\sum_{x_j} \tau_{ij}(x_i,x_j)\right)
\eea
Using \eqref{eq:bethe_entorpy_information} for the Bethe entropy and remembering the derivative of $x\log\frac{x}{a}$ is $\log\frac{x}{a}+1$ the derivatives compare to zero give\footnote{And using the constrain $\sum_{x_i}\tau_{ij}(x_i,x_j) = \tau_j(x_j)$},
\bea
\log{\tau_i(x_i)} &=& \theta_i(x_i)+ \sum_{j \in \nei{i}} \lambda_{j \to i,x_i}-(d_i-1)+\lambda_i\\
\log{\tau_{ij}(x_i,x_j)} &=&  \theta_{ij}(x_i,x_j) - \lambda_{j \to i,x_i} -  \lambda_{i \to j,x_j} +\log \tau_{i}(x_i) +\log \tau_{j}(x_j) -1 +\lambda_{ij}
%\frac{\partial \mathcal{L}(\thetav,\tauv,\lambdav)}{\partial \tau_i(x_i)} &=& \theta_i(x_i) - \log{\tau_i(x_i)}-1 - \sum_{j \in \nei{i}} \lambda_{i \to j,x_i}\\
%\frac{\partial \mathcal{L}(\thetav,\tauv,\lambdav)}{\partial \tau_{ij}(x_i,x_j)} &=& \theta_{ij}(x_i,x_j) + \log{\tau_{ij}(x_i,x_j)} + 1 + \lambda_{i \to j,x_i} + \lambda_{j \to i,x_j}
\eea
taking the exponent and rearranging\footnote{$\lambda_{ij}$,and$\lambda_i$ are part of the normalization a long with the constants.} we have,
\bea
\tau_i(x_i) &\propto& \exp{\theta_i(x_i)}\prod_{j \in \nei{i}} \exp{\lambda_{j \to i,x_i}}\\
\tau_{ij}(x_i,x_j) &\propto&  \exp{\theta_{ij}(x_i,x_j)+\theta_i(x_i)+\theta_j(x_j)} \prod_{k \in \nei{i}\setminus j} \exp{\lambda_{k \to i,x_i}} \prod_{k \in \nei{j}\setminus i} \exp{\lambda_{k \to j,x_j}}
\eea
Which is exactly as \eqref{eq:bp_single_marginal} and \eqref{eq:bp_pairwise_marginal} when BP converges.
Hence, BP fix points are local minima of the Bethe energy.

This result give us some insight to the results BP may return and to how BP should be used.
As was written before, BP could have  more than one fix point.
This empirical fact has now have theoretical reasoning. 
Since local minima of the Bethe free energy is a stable fix point of BP.
The Bethe free energy is a non-convex function hence multiple minima may exist.
Hence BP may have multiple stable fix point.
Note that not all fix point have the same quality (at least regarding the solution of \eqref{eq:bethe_approximation}).
Two fix points where one have lower Bethe free energy should not be treated the same - the one of the lower energy could be a solution to \eqref{eq:bethe_approximation} while the other could not.
This imply against a common practice of initializing the massages $\bold{m}^0$ to uniform distribution and running BP only once.
The above imply that random initialization and multiple running of BP may improve the quality of BP results.

The connection between BP and the Bethe energy promote a wide research.
The most immediate extension was the Generalized Belief Propagation \cite{yedidia2000generalized} where instead of maximizing the Bethe energy it maximize the Kikuchi free energy - an extension from only pairwise to larger areas.
Another noted algorithm is the Tree Re-Weighted Belief Propagation (TRW) \cite{wainwright2003tree}.  One of draw back of the Bethe approximation is the fact that the Bethe entropy is not a concave function. TRW suggest a surrogate entropy  which is concave. It doing so by giving weights to the information part (the second summation) in \eqref{eq:bethe_entorpy_information}.  It's name come from the interpretation of these weights.
One can choose a probability on the trees that are sub-graph of model graph.
Such probability induce a probability of each edge to appear in any one of the trees.
These probabilities are the weights of the information part.
This gave another advantage to TRW,  the resulted log partition function is always upper bound the true partition function. 
This easily can be seen by using Jensen inequality\footnote{Remember that the partition function is a convex function of $\thetav$.} $Z(\thetav) = z\left(\sum_{\mathcal{T}} t_{\mathcal{T}}\thetav_{\mathcal{T}}\right) \leq \sum_{\mathcal{T}} t_{\mathcal{T}} Z(\thetav_{\mathcal{T}})$ which is exactly the TRW approximation to the partition function.



Note that the above connect only the fix point of BP and the Bethe free energy.
It say nothing on intermediate results (BP results before convergence) or what BP is trying to achieve.
I think the answer is seeing BP is re-parametrization algorithm that try to change the parametrization of $\thetav$ such that it will belong to the local-polytope.
For full proof please see \cite{wainwright2002stochastic}.

More on Bethe-BP:
\begin{itemize}
\item Convergence conditions. Bethe tree \cite{tatikonda2002loopy}, BP error \cite{ihler05b} Zeta function \cite{YusukeNips2009}, 
\item Bounding the partition function. Bounding log-supermodular\cite{RuozziNips2012},\cite{AlanNips2007},
\end{itemize}
\ignore{
\be
\mu_k(x_k;\thetav) = \frac{1}{Z(\thetav)}\sum_{\substack{\xx \\
s.t.\  \xx_k=x_k}}e^{\theta_k(x_k) + \sum_{j \in \nei{k}}\theta_{k,j}(x_k,x_j)}e^{\sum_{i \in V \setminus k}\theta_{i}(x_i) +\sum_{\substack{ij \in E\\
 s.t.\  i,j \ne k}}\theta_{ij}(x_i,x_j)}
%} {\sum_{\hat{x}_k}e^{\theta_k(\hat{x}_k) + \sum_{j \in \nei{k}}\theta_{k,j}(\hat{x}_k,x_j)}e^{\sum_{i \in V \setminus k}\theta_{i}(x_i) +\sum_{\substack{ij \in E\\
% s.t.\  i,j \ne k}}\theta_{ij}(x_i,x_j)}}
\ee
Denote by $\thetav^{\setminus k}$ the model where we remove all factors involve the vertex $k$.
Now the marginal of the neighbors of $k$ in  that model is
\be
\muv_{\nei{k}}(\xx_{\nei{k}}; \thetav^{\setminus k}) \approx \sum_{\substack{\hat{\xx}\\
s.t. \hat{\xx}_{\nei{k}} = \xx_{\nei{k}}}}  e^{\sum_{i \in V \setminus k}\theta_{i}(\hat{x}_i) +\sum_{\substack{ij \in E\\
 s.t.\  i,j \ne k}}\theta_{ij}(\hat{x}_i,\hat{x}_j)}
\ee
 With this we can write
\bea
\mu_k(x_k;\thetav)  &\approx& \sum_{\xx_{\nei{k}}} e^{\theta_k(x_k) + \sum_{j \in \nei{k}}\theta_{k,j}(x_k,x_j)} \muv_{\nei{k}}(\xx_{\nei{k}}; \thetav^{\setminus k})\\
 &\approx& e^{\theta_k(x_k)}  \prod_{j \in \nei{k}} \sum_{ x_j } e^{\theta_{k,j}(x_k,x_j)} \muv_{j}(x_j; \thetav^{\setminus k})\\
\eea
}
\subsubsection{}
\subsection{Sampling}
\label{sec:sampling}
\section{Learning Graphical Models}
Understanding why you are doing something before doing it, is good practice in life. 
So I will start by presenting two reasons why we learned graphical models.
In many domains the main interest is in understanding the model.
For example, we want to know what is the minimum set of variables that can explain specific variable - which variables are its neighbors.
Or which variable has the greatest effect on another variable etc.
For this purpose we are interested in learning the exact model - a model that is as similar as possible to the ``real'' model \footnote{
In structure learning this sum-down the difference between the two edges sets $ \min_{E} |\exact{E}\setminus (\exact{E} \cap E)| + |E \setminus (\exact{E} \cap E)|$.
When learning the parameters, the functions $\theta_{ij}(x,y)$, defining similarity become more elaborate since two parameters $\thetav \neq \tilde{\thetav}$ while $P(\xx; \thetav) = P(\xx; \tilde{\thetav}), \forall \xx$.
This feature is know as reparameterization and it play important roll in Belief Propagation as was presented \ref{}\todo{FILL REF }}.

On the other hand, in many domains we seek the ability to answer queries.
When asking our model a query we want the result to be similar as if it is the result of the real process.
For example, if we learned a model of digits in an image (the variables are the image pixels and the digit in the picture).
We are looking for a model that given a new image infer the correct digit - the digit that is in the image.
This is regardless of how much the graph structure or the parameters are close to the real model.
In a world with no computations limitation this two objective are the same - we will learned the exact model and preform exact inference.
Unfortunately, exact inference is NP-hard in many cases, hence we will be forced to use approximate inference (see \secref{sec:approx}).
Remember that our objective is to correctly answer queries, taking into account the performance of the inference algorithm is a must.

Most of the research on learning graphical models(GM) seek to reconstruct the real model.
I will give a short survey on learning when the objective is reconstruction of the exact model.
It will be divided into the two main stages of the learning.
First is learning the dependency graph - the edges of the graph $E$.
Than learning the parameters of the model $\thetav$.
\subsection{Structure learning}
First lets define what exactly is our goal.
Having a probability $P(\xx;\exact{\thetav})$ defined as in \eqref{eq:ciluqe_prob}.
We are looking to find the exact edge set $\exact{E}$ given a data set of $N$ identical independent distributed (i.i.d) samples from $P(\xx;\exact{\thetav})$.

I will start by presenting the sample and time complexity of the above problem. 
First if the graph is known to be a tree, the exact structure can be found by maximum spanning tree.  Where the  weights are the empirical information between any two vertices \cite{chowLiu}. 
Later \cite{tan2011learning} prove sample complexity for trees is $\Omega(\log(p))$.
In the global setting, there seem to be four parameters that govern the complexity of structure learning.
The first is  the number of vertices $p$.
The second is the maximum degree in the graph - $\dmax = \max_{i \in V} d_i$.
The third and the forth parameters quantify the strength of the interaction between the variables.
In the Ising model\footnote{ The Ising model is a pairwise binary model where $\cX = \{-1,1\}$ and $P(\xx;\thetav) = \frac{1}{Z(\thetav)} \exp{\sum_{i \in V}\theta_i + \sum_{ij}\theta_{ij}x_ix_j}$} this parameters are easily expressed by (following \cite{santhanam2012information}) $\lambda = \min_{ij} \theta_{ij}$ and $\omega = \max_i \sum_{\nei{i}} \theta_{ij}$.
While in other models (where the cardinality is greater than $|\cX|>2$ or the size of factors greater than $\exists c \in \cC, |c|>2$) it is unknown how to express this quantities - the strength of the interaction between the variables as a function of the model parameters.
Hence lower bounds  on non-binary models ignore these two factors\footnote{In proving upper bounds some conditions on the probability instead of the model is given. In my opinion the above two parameters hide inside the condition see for example \cite{bresler2008reconstruction}}.

The sample complexity bounds are $N > \Omega(\dmax \log p)$ and $N > \Omega(\max\{\dmax^2, \lambda^{-2}\}log p) $ to the general case and the Ising model respectively. This mean that we need at least $N$ samples in order to guarantee recovery of $\exact{E}$ in probability more that $0.5$.
Lower bound on sample complexity of $\Omega(p^{\frac{\dmax}{2}})$ was given by \cite{bresler2014structure} for statistical algorithms \cite{feldman2013statistical}\footnote{Algorithm that use an oracle that return the expectation of any function - no direct access to the data}.

Algorithm for structural graph are abundant, I will give only a short list.
Moreover the algorithms differ what is assumed to be given as input , what is the underline model and other assumption so compression is hard.
I will denote by $a$ parameters (other than $p$, $\dmax$) that depend on the model - usually some bound on interaction strength.
 \begin{itemize}
    \item Gaussian multivariate distributions  is an example where it is clear that the problem is easier since it  is equivalent to the estimation of the non-zeros cell in the inverse of the covariance matrix. An example of algorithm solving this problem are \cite{meinshausen2006high,yuan2007model, friedman2008sparse}. 
    \item As seen before $\dmax$ the maximum degree, play an important role in the lower bounds. Many algorithms \cite{bresler2008reconstruction, abbeel2006learning} assume  $\dmax$ is given as  part of the algorithm input. Knowing this quantity, an exhaustive search over all subsets of size $\dmax$(or a function of it) is preformed in-order to find each vertex neighbors.  In\cite{bresler2008reconstruction} their algorithm's time complexity is $O(p^{2\dmax+1}\log p)$ and sample complexity is $O(\dmax a \log p)$ . While in \cite{abbeel2006learning} their algorithm does not guarantee to reconstruct the model (edge set), but to be as close as needed in a Kullbackâ€“Leibler divergence manner to the true one. Its time complexity bounds are $O(p^{a d})$ and sample complexity $O(\dmax a \log p)$. Another paper in this class is \cite{wu2013learning},  instead of the max degree as an input they require the two sizes. Conditioning on some sets with size smaller or equals to the given quantities insure dependence . In other words $ij \not \in E$ the following take place $\exists S \subset V, |S|< D_1, \forall T \subset V, |T|<D_2    X_i \bot X_j | X_T, X_S$ , and the two quantities are $D_1$ and $D_2$\footnote{In the Ising model $D_1 = \dmax$ and $D_2 = \dmax -1$. When allowing factors with higher cardinality two variables can be independent (and any test will indicate they are so) but given another variable they can be dependent - there exit an edge between the two variables. These two quantities bound the size of such sets. }. 
    \item Focusing on Ising models when a threshold (the minimum difference of the probability when one of its neighbors change its assignment) is given as an input,  \cite{bresler2015efficiently} give an algorithm with time complexity of $O(p^2\log p)$ and sample complexity of $O( a \log p)$. In the paper a more accurate is given -  it take into account the interaction strength . Their algorithm has two stages,  first it collect pseudo-neighborhood for specific vertex - a set that include the correct neighborhood. Than by iterating over all this set, it remove vertices that does not change the conditional probability. Note that this algorithm can not be extended to non pairwise models.
\item In \cite{ravikumar2010high} give another flavor of algorithm. Their algorithm is based on optimization of the conditional likelihood of a vertex given all the other vertices with $l_1$ regularization - $\max_{\thetav} \frac{1}{N}\sum_n^N \log p(x^n_i| \xx^n_{\setminus i};\thetav) - \lambda \sum_{ij} |\theta_{ij}|$. Now the neighborhood set is defined by $\nei{i} = \{\theta_{ij} : |\theta_{ij} > \alpha\}$ where $\alpha$ is a threshold based on the graph parameters. Under certain conditions the algorithm reconstruct $E$ in high probability with time complexity of $O(p^4)$  and sample complexity of $O( d^3 \log p)$. Note that this algorithm is not very sensitive to the selection of $\alpha$ (They suggest how to approximate it) and very practical. Its main draw back is (to my opinion), that the conditions (or parameters) that guarantee its success are not directly connected to the model parameters. Hence it is very hard to compare it to other such algorithms. Another problem is the assumption of pairwise interaction.     
\item Correlation decay is an important concept in structure learning \cite{montanari2009graphical}. The idea is simple, Let denote by $B_{\tau}(i)$ all the vertices that are within distance(a formal definition will be given later) $\tau$ from vertex $i$ and denote by $\partial B_{\tau}(i)$ the boundary of that group. We will require that the assignment of $\partial B_{\tau}(i)$ have negligible effect on the probability of variable $x_i$ for all $i \in V$. It is now clear, if the learned model have this characteristic when deciding what are the neighbors of vertex $i$ only a subset of vertices needed to be considered. By adding this condition \cite{bresler2008reconstruction} approved its time complexity to $O(d^{ad})$. If we further assume that $B_{\tau}(i)$ is a tree (note, the whole graph is not necessarily a tree) we can apply greedy algorithms \cite{netrapalli2010greedy, anandkumar2013learning}. In \cite{anandkumar2013learning} they preform a greedy algorithm with time complexity of $O(d^{2d}p)$ and sample complexity of $O(d^2\log p)$ but focused only on pairwise case.  
\end{itemize}
We can summarize the current state as follows. 
In the general case the maximum degree must be given. Having it as an input, the sample and time complexity are very close to the known lower bounds. 
Hence The big question remain, what needed to be assumed in order to allow efficient learning. 
Correlation decay was considered as an option\cite{montanari2009graphical} but \cite{bresler2014structure} shows that it indeed not a necessary one. 
Another difficulty (which is sometimes ignored) is the jump of difficulty when the factor size is not bounded (or as large as $\dmax$). 
I believe this can be a very int resting research direction, under what conditions  a model with bounded factor size and unbounded degree $\dmax$ can be learned efficiently\footnote{Not exponential in $\dmax$}.
\subsection{Parameters Learning}
As in the previous section I will start with the problem definition. 
In parameters learning it is assumed that the structure is given (but not necessarily the exact one). The most trivial objective will be to find $\exact{\thetav}$.  
This  objective is not well define,  since two parameters can differ $\hat{\thetav} \neq \tilde{\thetav}$ but still give the same probability $\forall\ \xx \in \cX^p\ p(\xx;\hat{\thetav}) = p(\xx;\tilde{\thetav})$.
Hence the objective will be to find the parameters that will define the same probability: given a set of i.i.d observations, sampled from $p(\xx;\exact{\thetav})$ find $\thetav$ such that for all input $\forall \xx \in \cX^{p}$ we will have the same (or as close as possible) probability $p(\xx;\thetav) = p(\xx;\exact{\thetav})$.
Note that even to check this requirement is exponential of the size of $\xx$.
So other objective, or other way to achieve this goal is required.
I will start with the  most intuitive approach - maximum likelihood(ML).

\subsubsection{Maximum Likelihood}
\label{sec:max_likelihood}
In ML we are looking for the parameters that will maximize the probability to sample the seen data-set.
Note, that just giving probability of $\frac{1}{N}$ to each of the samples is the maximum likelihood solution, but it suffer from extreme overfitting.
So some regularization is need , in GM it usually done by forcing a structure (forcing Independence assumptions) and the size of the factors.
I will now define the maximum likelihood objective and than discuss its merits and drawbacks of it.

It is more convenient to maximize the log of the likelihood and to normalize it by the sample size $N$, this changes do not change the parameters that maximize the  function $\arg \max_x\frac{1}{N}\log(f(x)) = \arg\max_x f(x)$. 
Denote by $X = [\xx^1, \ldots, \xx^N]$ the data-set.
With this the ML objective can be  formally defined  by  using the model in \eqref{eq:basic_model} as, 
\bea
\likelihood{X;\thetav} &=& \frac{1}{N}\log\left(\prod_{n=1}^N p(\xx^n;\thetav)\right) \\
&=& \frac{1}{N}\sum_{n=1}^N\log\left(p(\xx^n;\thetav)\right)\\
&=& \frac{1}{N}\sum_{n=1}^N\log\left(\frac{1}{Z(\thetav)} \exp{\sum_{ij \in E} \theta_{ij}(x^n_i,x^n_j) + \sum_{n \in V} \theta_i(x^n_i)}\right)\\
&=& \frac{1}{N}\sum_{n=1}^N \sum_{ij \in E} \theta_{ij}(x^n_i,x^n_j) + \sum_{i \in V} \theta_i(x^n_i) -\log(Z(\thetav))\\
&=& \sum_{ij \in E} \sum_{x_i,x_j \in \cX}\mub_{ij}(x_i,x_j)\theta_{ij}(x_i,x_j) + \sum_{i \in V} \sum_{x_i \in \cX}\mub_i(x_i)\theta_i(x_i) -\log(Z(\thetav))\\
&=&  \mubv \cdot \thetav - \log(Z(\thetav))
\eea
Where $\mubv$ is the empirical marginals
\be
\mub_i(x_i) = \frac{1}{N}\sum_{n=1}^{N} \deltaF{x_i=x_i^n },\ \
\mub_{ij}(x_i,x_j)  = \frac{1}{N}\sum_{n=1}^{N} \deltaF{x_i=x_i^n \land  x_j=x_j^n}
\ee
ordered in a vector in the same way $\thetav$.

We are looking for $\thetav$ that maximize ML. 
So taking the derivative and compare to zero is the prompted step:
\bean
\label{eq:ml_derv_single}
\frac{ \partial \likelihood{X;\thetav}}{\partial \theta_{i}(x_i)} &= \mub_{i}(x_i) - \frac{ \partial \log(Z(\thetav))}{\partial \theta_{i}(x_i)} =& \mub_{i}(x_i) - p(x_i;\thetav) \\
\label{eq:ml_derv_pairs}
\frac{ \partial \likelihood{X;\thetav}}{\partial \theta_{ij}(x_i,x_j)} &= \mub_{ij}(x_i,x_j) - \frac{ \partial \log(Z(\thetav))}{\partial \theta_{ij}(x_i,x_j)} =& \mub_{ij}(x_i,x_j) - p(x_i,x_j;\thetav)
\eean
 Where the derivative of the partition function is given by \eqref{eq:pratition_derivative}.
We can conclude that the ML parameter $\ml{\thetav}$ must satisfy the moment matching characters - the marginals of the model must equals to the empirical marginals.
\be
\label{eq:moment_matching}
\mub_i(x_i) = p(x_i;\thetav), \ \ \ \mub_{ij}(x_i,x_j) = p(x_i,x_j;\thetav)
\ee
This feature is very intuitive, we want the resulted probability to be able to re-generate the sample's marginals.
ML has other desirable qualities (under some conditions): 
It is consist - when $N \to \infty$, we will recover the exact parameters $\lim_{N \to \infty} \arg \max_{\thetav} p(X^N;\theta) =  \exact{\thetav}$\footnote{This does not contradict the above discussion - part of the conditions is uniqueness $\exact{\thetav}$.}. 
It is efficient - in the limit no other consist estimator has lower mean square error than ML.   
Asymptotic Normality: with sufficient data, the ML estimator mean will be the true parameter with add Gaussian noise, the probability to get very wrong parameter go down exponentially fast.

It seem that ML is indeed all we need, this is however not the whole picture.
ML suffer from several drawback.
The first and most important it is not time efficient.
Calculating the ML is $\#P$ since in involve calculating the partition function.
Moreover, even calculating its derivative is $\#P$ since it involve inferring the marginals of the model.
So direct gradient descent is not possible since it involve calculating the current model marginals.
Even if we tackle the time complexity of finding the marginals, a result by \cite{bresler2014hardness,montanari2015computational} show that if $NP \neq RP$ no algorithm that use only the model marginals ( marginals corresponding to the model parameters) can learn the model parameters. 
Hence if a time efficient algorithm that find the ML parameter is to be found, it must use higher order of marginals.

We will no consider an approximation to the ML Pseudo-Likelihood.
\subsubsection{Pseudo-Likelihood}
Pseudo-Likelihood (PL)\cite{besag1975statistical} is a way to approximate the likelihood.
It is the first of larger family of likelihood approximation called composite likelihood \cite{lindsay1988composite}.
Specifically, if we will note that the likelihood can be written as $ p(\xx;\thetav) = \prod_{i=1}^p p(x_i |x_1,\ldots x_{i-1})$
The PL approximation is
\be
pl(\xx;\thetav) = \prod_{i=1}^p p(x_i |\xx_{\setminus i};\thetav) = \prod_{i=1}^p p(x_i |\xx_{\nei{i}};\thetav)
\ee
Note that with using model \eqref{eq:basic_model}, we can write:
\bea
p(x_k |\xx_{\setminus k};\thetav) 
%&=& \frac{p(\xx;\thetav)}{p(\xx_{\setminus k};\thetav)}\\
&=& \frac{p(\xx;\thetav)}{\sum_{x_k \in \cX }p(\xx;\thetav)}\\
%&=&\frac{\exp{\sum_{ij \in E} \theta_{ij}(x_i,x_j) + \sum_{ i \in V} \theta_i(x_i)}}{\sum_{ \tilde{x}_{k} \in \cX }\exp{\sum_{ij \in E} \theta_{ij}(x_i,x_j) + \sum_{i \in V} \theta_i(x_i)}}\\
&=&\frac{\exp{\sum_{j \in \nei{k}} \theta_{kj}(x_k,x_j) + \theta_k(x_k)}}{\sum_{\tilde{x}_k \in \cX}\exp{\sum_{j \in \nei{k}} \theta_{kj}(\tilde{x}_k,x_j) + \theta_k(\tilde{x}_k)}}\\
\eea
%Now if all we want is to find the parameters we can just maximize the conditional likelihood namely:
Now we can write PL as,
\bea
&&\frac{1}{N} \log \left(\prod_{n=1}^N \prod_{i \in V} p(x^n_i | \xx^n_{\nei{i};\thetav})\right) \\ 
%&=&\sum_{i \in V}\frac{1}{N}\sum_{n=1}^N \sum_{j \in \nei{i}} \theta_{ij}(x^n_i,x^n_j) + \theta_i(x^n_i)\\
%&&-\log\left(\sum_{\tilde{x}_i \in \cX}\exp{\sum_{j \in \nei{i}} \theta_{ij}(\tilde{x}_i,x^n_j) + \theta_i(\tilde{x}_i)}\right) \\
&=& \sum_{i \in V} \sum_{j \in \nei{i}}\sum_{x_i,x_j \in \cX} \mub_{ij}(x_i,x_j)\theta_{ij}(x_i,x_j) + \sum_{x_i \in \cX}\mub_i(x_i)\theta_i(x_i)\\
&&-\sum_{\xx_{\nei{i}} \in \cX^{|\nei{i}|}} \mub_{\nei{i}}(\xx_{\nei{i}})\log\left(\sum_{\tilde{x}_i \in \cX}\exp{\sum_{j \in \nei{i}} \theta_{ij}(\tilde{x}_i,x_j) + \theta_i(\tilde{x}_i)}\right) \\
\eea
And the derivatives are,
\bea
\frac{\partial pl(X^n;\thetav)}{\partial \theta_{ij}(x_i,x_j)} &=&
2 \mub_{ij}(x_i,x_j) \\
&&- \sum_{\xx_{\nei{i}\setminus j} \in \cX^{|\nei{i}-1|}} \mub_{\nei{i}}(x_j,  \xx_{\nei{i}\setminus j}) p(x_i|x_j, \xx_{\nei{i}\setminus j};\thetav)\\
&& - \sum_{\xx_{\nei{j}\setminus i} \in \cX^{|\nei{j}-1|}} \mub_{\nei{j}}(x_i,  \xx_{\nei{j}\setminus i}) p(x_j|x_i, \xx_{\nei{i}\setminus j};\thetav)\\
\frac{\partial pl(X^n;\thetav)}{\partial \theta_{i}(x_i)} &=& 
\mub_i(x_i) - \sum_{\xx_{\nei{i}} \in \cX^{|\nei{i}|}} \mub_{\nei{i}}(\xx_{\nei{i}}) p(x_i|\xx_{\nei{i}};\thetav)
\eea
Note first that it is easy to calculate $ p(x_i|\xx_{\nei{i}};\thetav)$ since the partition function is summation only on the values of $x_i$.
Hence optimizing by gradient descent is plausible.  
Second, this condition is similar to the moment matching condition of \eqref{eq:moment_matching}.
Unfortunately, for each derivative we not only need to a approximate the pairwise marginals but the marginals of the whole neighborhood.
But if we consider the result by \cite{bresler2014hardness,montanari2015computational} to have any time efficient  algorithm we will need moments with higher dimensions hence pay in sample complexity. 
Other characteristic of PL, such as $\thetav$ is asymptotically normally distributed,  can be derived from the more general family of composite likelihood see \cite{varin2011overview}.

Another view point of PL is, that instead of approximating the likelihood, it just preforming ML on the conditional marginals. 
We are looking for the parameters that maximize the conditional probability for each vertex separately.
The derivative of the singleton parameter is the same.
While the pairwise change, so it include only one of the summation - hence a more moment matching like equation.


The first reason to use PL and not ML is time complexity.
While PL parameters can easily be computed via gradient descent.
No efficient way is known to recover ML parameters.
The question whether there are cases when PL give better result than ML is still open.
In the well specified case (the model structure and factors are exact) is was shown that ML is more efficient (sample wise) than PL \cite{liang2008asymptotic}
While in the miss specified case (the more realistic case) is is not clear which method is better(see \cite{varin2011overview} for discussion).
An interesting direction for research is to design an inference algorithm for PL.
Meaning, design an efficient inference algorithm such that it is promised to return the empirical marginals when the model parameters was learned using PL.
   
\subsubsection{Other Methods}
Contrastive Divergance (CD) \cite{hinton2002training}  was designed for models where part of the variables are hidden ( variables that there are no samples include them), but it could be used to general models.
The idea is simple, re-call from \eqref{eq:ml_derv_single} and \eqref{eq:ml_derv_pairs}, that the reason gradient-descent is impractical,  is  the hardness of evaluating the marginals according to the parameters $p(x_i,x_j;\thetav)$. 
One way to approximate these marginals  is by MCMC \ref{}.
If we only preform small number of steps (usually one) to evaluate $p(x_i,x_j;\thetav)$ we result with CD.
Justification to this method was given by \cite{bengio2009justifying} and \cite{carreira2005contrastive} but it success in learning Restricted Boltzmann Machine (RBM) gain her its importance.

In contrast to the previous methods   \cite{abbeel2006learning} give a closed form equation for parameters estimation.
Using the ideas from the proof of the Hammersley-Clifford theorem,  it estimate the canonical factors  - hence learned the graph parameters.
More specifically, any distribution can be written by(using the model in \eqref{eq:ciluqe_prob}) 
\be
p(\xx;\exact{\thetav}) = \frac{1}{Z(\thetav)} \exp{\sum_{c \in \cC}\thetav_c(\xx_c)} = p(\tilde{\xx})\exp{\sum_{c\in\cC} \thetav^{\dagger}_c(\xx_c)}
\ee
Now the canonical factors $\thetav^{\dagger}$ are defined by,
\be
\thetav^{\dagger}_{c}(\xx_c) = \sum_{b \subseteq c} -1^{|c \setminus b|} \log{p(f(b,\xx_c,\tilde{\xx}))}
\ee
where  $f(b,\xx_c,\tilde{\xx}) : \cI \times \cX^{|c|} \times \cX^{|d|}\to \cX^{|d|}$  such that $b \subseteq c\subseteq d$ and for all $i \in d$ if $i \in b$ than $x_i = [\xx_c]_i$ otherwise $x_i = \tilde{\xx_i}$ for the fixed assignment $\tilde{\xx}$.
This can be father simplify by using independence 
\be
\label{eq:abbeel}
\thetav^{\dagger}_{c}(\xx_c) = \sum_{b \subseteq c} -1^{|c \setminus b|} \log{p(f(b,\xx_c,\tilde{\xx}_c) |\tilde{\xx}_{\nei{c}})}
\ee
So a factor involve only the clique and the clique's neighbors.
Hence to estimate the parameters we just need to change $p(f(b,\xx_c,\tilde{\xx}_c) |\tilde{\xx}_{\nei{c}})$ by the corespondent empirical marginals.
In continues  paper \cite{roy2009learning} farther simplify the above equation but with no  implication to parameters learning.
Note the resemblance of  \eqref{eq:abbeel} to the PL equation.
Indeed in \cite{bradley2012sample} it is proved that the two methods give the same results.
They include a compression of the sample complexity of this method, PL(more accurately composite likelihoods)  and ML.

Other methods goes in the same lines - avoiding the partition function. 
For example \cite{hyvarinen2007some} use the ratio between the probability of two assignments, hence the partition function canceled.
Note that using the ratio is not very different than using the conditional probability and indeed \cite{marlin2010inductive} and \cite{marlin2012asymptotic} compare the methods to pseudo-likelihood and explain the differences.
Other methods like \cite{mizrahi2014icml} just cut off each clique with its neighbors and learn each factor independently\footnote{It assume independence of the clique neighbors from their other neighbors. But it is not clear how it effect the sample complexity a non is given in the paper. }.

To conclude, ML is provably the most sample complexity efficient,   but no time efficient algorithm is known.
Moreover, it was proven by \cite{bresler2014hardness,montanari2015computational} that no algorithm that use only the sufficient statistics in the clique order can optimize ML.
On the other hand there is PL that is time efficient but its sample complexity is worst that that of PL.
Methods like composite likelihood can give better sample complexity results while still be time efficient.

  
