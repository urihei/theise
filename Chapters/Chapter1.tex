% Chapter 1

\chapter{Introduction} % Main chapter title

\label{intro} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------
\section{Different models}
\subsection{Graphical Models}
\subsubsection{Notations and Definitions}
Probability is the chance that a certain event will take place.
Hence we need to define all the possible.
We will denote an event by $x \in \cX$, and we will restrict it to be discrete $ |\cX| \in \naturalNumbers$
\footnote{Through out this thesis I will focus only on discrete Graphical models, even though continuous Graphical Models is very useful model}.
An event can be a composite of simpler $p \in \naturalNumbers$ sub events, having $\xx = [x_1, \ldots, x_{p}] \in \cX^p$ to be the event.
After defining the event the probability is  a function from the space of events to a number between zero and one $\pr:\cX^{p} \to (0,1]$ with an important restriction that $\sum_{\xx \in \cX^{p}} \pr(\xx) = 1$\footnote{Note that we exclude zero since we require positive distribution.}\footnote{Without loss of generality for ease of notation we assumed that all variables have the same domain. This assumption can be easily removed.}.

In many real world situations each variable (sub-event) is directly depend on only small subset of other variables.
In probability terminology this sum down to saying that $P(x_i |\xx_{\cI}) = P(x_i|\xx_{[1,\ldots,p] \setminus i}),\  \forall \xx \in \cX^{P}$ where $\cI \subset \{1,\ldots,p\}$  is a small set of variable's indexes.
We will denote by $\nei{i}$ the smallest set of vertices indexes that satisfy this relation and by $d_i = |\nei{i}|$ the number of neighbors of variable $i$.  
% $\nei{i} = \arg\min \{|\cI| : \cI \subseteq \{1,\ldots,p\}, P(x_i|\xx_{[1,\ldots,p] \setminus i}) = P(x_i |\xx_{\cI}) \}$.
Now this dependency relation can be describe by a graph $G(V,E)$ where $V = \{v_1,\ldots, v_p\}$ is the set of vertices and  $E \subseteq V \times V$ is the graph edges.
Each variable $x_i$ will be assign a vertex $v_i$. 
An edge between $v_i$ and $v_j$ exist if  $j \in \nei{i}$\footnote{if $i \in \nei{j}$ than $j \in \nei{i}$ otherwise $P(\xx) = P(\xx_{[1,\ldots,p]\setminus i,j}) P(x_j|\xx_{\nei{j}}) P(x_i|x_j,\xx_{\nei{i}\setminus j})$ and $P(\xx) = P(\xx_{[1,\ldots,p]\setminus i,j}) P(x_i|\xx_{[1,\ldots,p]\setminus i,j}) P(x_j|\xx_{\nei(j)})$ which result in $P(x_i|x_j,\xx_{\nei{i}\setminus j}) =   P(x_i|\xx_{[1,\ldots,p]\setminus i,j})$ in contradiction to $j \in \nei{i}$}.
Now by the Hammersleyâ€“Clifford theorem\cite{hammersley1971markov}\footnote{We defined the probability as positive}, we can write out the probability as
\be
\label{eq:ciluqe_prob}
P(\xx) = \frac{1}{Z} \exp{\sum_{c \in \cC}\theta_{c}(\xx_{c})}
\ee
Where $\theta_{c}(\xx_{c}): \cX^{|c|} \to \reals$  is a function, $\cC$ is the set of all maximal cliques\footnote{a clique is a set of vertices that have an edge to all vertices in the set. A maximal clique mean that no other vertex can be added to the clique and it still be a clique.} in $G$ and $Z = \sum_{\xx \in \cX^p} \exp{\sum_{c \in \cC}\theta_{c}(\xx_{c})}$ is the normalization constant called the partition function.
Even from the ability to describe the probability in this compact form\footnote{the naive description length is $O(|\cX|^p)$ while with this representation it is $O(\sum_{c \in \cC} |\cX|^{|c|})$}the merit of connecting probability and graph is clear, in other words the potential of graphical models \cite{koller2009probabilistic}.


In this thesis we will assume that $|c| = 2$, this in itself does not restrict the probability function but it can increase the cardinality of $\cX$\footnote{Basically each clique is assigned to a variable and an edge exist with cliques sharing some variables. Note that new cardinality of the variable is the product the cardinalities of the clique variables }.
With this restriction the probability can be written as
\be
\label{eq:basic_model}
P(\xx; \thetav) = \frac{1}{Z(\thetav)} \exp{\sum_{ij \in E} \theta_{ij}(x_i,x_j) + \sum_{i \in V} \theta_i(x_i)}
\ee
were we define $\thetav$ as concatenation of $\theta_i(x_i)$ and $\theta_{ij}(x_i,x_j)$ for all $i \in V$ $ij \in E$ and $x_i,x_j \in \cX$ and $Z(\thetav)$ is the normalization constant (with respect to $\xx$)
\be
\label{eq:partition_function}
Z(\thetav) = \sum_{\xx \in\cX}\exp{\sum_{ij \in E} \theta_{ij}(x_i,x_j) + \sum_{i \in V} \theta_i(x_i)}
\ee

\subsubsection{What can we do with it?}
There are two basic questions that are of interest.
The most basic question is which event is the most likely, in other words: what is the assignment that is most probable (MAP - maximum a posteriori).
\be
\label{eq:MAP}
\xx_{MAP} = \arg \max_{\xx \in \cX^p} P(\xx) 
\ee
Now sometimes we are not interested in the whole event but only in small fraction of its variables.
We are looking for the marginal probability of some sub-event $\xx_{\cI}$.
This can be calculated by summing over all the rest of variables:
\be
\label{eq:MAR}
P(\xx_{\cI}) = \sum_{\substack{\zz \in \cX^{P}\\
 s.t.\  \zz_{\cI} = \xx_{\cI} }} P(\zz)
\ee
Note that these two problems can be asked when part of the variables was seen hene exponential number of different queries on the same model. 
Both inference tasks can easily be infer by going over all the possible event.
This however is too expensive since it is exponential in the number of variables.
And indeed in the general case MAP\cite{shimony1994finding} is NP complete problem while MAR\cite{cooper1990computational} is \#P complete (MAR is even hard to approximate \cite{roth1996hardness}).

The situation is not as hopeless as was described.
First the hardness result apply only on the general case, if we restrict the structure of the dependency graph to a tree both problems can be solved in polynomial time \footnote{In fact any the hardness result parameters can be changed from the number of variables in the graph to the graph tree width \cite{robertson1983graph,robertson1994quickly} }.
Other model restriction is known to allow polynomial inference, such as planar graph \cite{jaakkola2007approximate}, fast mixing models\cite{jerrum1993polynomial}.
However in general case approximate algorithm must be used we will return to this point later see . \todo{add reference}
\subsubsection{Pros and cons}
We just claim that inference is hard so why should one use such model.
I will now go over the reasons why one should use graphical models.
One may answer is that it is optimal - in classification for example, if we know the exact conditional distribution and can infer the MAP we will have the optimal solution. This situation is very unrealistic: we usually do not know the true model moreover the inference is NP hard so we usually use some approximation. So we return to the question when should we use it?

In many situations we need to understand our model. In biology, we learn the structure of the model to better understand the learned biology system. If we rule out a user misuse, we may need the ability to explain our decision. In graphical models each variable has meaning hence understanding the learned model is easier then other model - SVM for example.

The model in graphical models usually lack the distinction between features and outcome. In situation that such separation is artificial graphical model give power that is not found else where. We can ask different queries on the same model and have consist answers. Take for example medical situation, we are interested on the probability of several symptoms, each is a different query.

This bring us the next two important features, handling missing values and structured output. In many situations we do not always see all of the variables. Graphical models, handle missing values inherently and such cases need not special treatment ( as is the case in other models). Lastly, the queries result is not necessarily a single variable. Queries on several variables are possible and will take into account the relation between these variables.

To conclude, graphical models is a very powerful toll. This power come with a price,  inferring and learning (as we will see later) are hard and approximation algorithm must be used in most cases.
\subsection{Support Vector Machine}
\subsubsection{Notations and Definitions}
One basic problem in machine learning is classification.
Given features of an instance we want the ability to classify - assign one label out of to small discrete group.
Formally, given $\xx \in \Re^p$, a classifying function (more accurately class of functions $f \in \mathcal{F}$) is defined by $f: \Re^p \times \Re^q \to \cL$ where $|\cL|,q \in \naturalNumbers$  - it take as input the instance to classify and the parameters of $f$  and return a label.
Learning such a function mean to learn its parameters: we want to find parameters  $\ww$ that minimize $\expect{(\xx,y) \sim P}{l(f,\xx, \ww ,y)}$ where $l : \mathcal{F} \times\Re^p\times \Re^q \times \cL \to \Re^{+}$ is the loss function - the cost of returning label $f(\xx,\ww)$ where the true label is $y$.

The above objective is not achievable in most cases, hence some relaxation of the problem must take place.  
First, we usually do not know the real distribution, but only have a sample of it: given a samples $D = \{(\xx_i,y_i)\}_{i=1}^N$  where $\xx_i \in \Re^p, y_i \in \cL$ sampled from some fixed distribution $P$.
We will want to minimize our objective on this sample - minimizing the empirical loss\footnote{Since the data sample is finite we need to regularize the parameters, here we select the euclidean (or Frobenius  norm in the multiclass classification) to be the regularize and $\lambda$ to be its weight's parameter.}.
Moreover we will restrict $f$ to be linear $f(\xx, W) = \argmax_{r \in \cL}\{ \ww_r \cdot \phi(\xx)\}$ on some known feature map $\phi: \Re^p \to \Re^q, q \in \naturalNumbers$ \footnote{We abuse notation, here by giving the instance of group $\cL$ and its index the same notation}.
Finally even though, the most intuitive loss function is the zero one loss $\deltaF{f(\xx) \neq y}$.
This function is very hard to optimize since it is non-convex.
One known surrogate is the hinge loss function $\max\{0,1- y(\ww \cdot \phi(\xx))\}$ where $\cL = \{-1,1\}$ while in the multiclass case we have the extension $ \max_{r \in \cL}\{\deltaF{r \ne y} - \ww_y \cdot \phi(\xx) + \ww_r \cdot \phi(\xx)\}$ \cite{crammer2002algorithmic}. All together the objective is:
\be
\label{eq:svm_obj}
\min_{W \in \Re^{|\cL|,q}} \sum_{i=1}^M \max_{r \in \cL}\{\deltaF{r \ne y} - \ww_{y_i} \cdot \phi(\xx_{i}) + \ww_r \cdot \phi(\xx_i)\} + \lambda \|W\|^2
\ee

Taking the dual and some algebra gives:
\bea
\label{eq:svm_obj_dual}
\max_{A \in \Re^{M,|\cL|}}&& -\sum_{i,j}^M(\phi(\xx_i)\cdot\phi(\xx_j))(\alphav_i\cdot\alphav_j) + \beta\sum_i^M\alphav_i \cdot \bold{1}_{y_i}\\
\text{subject to}:&& \forall i\ \ \alphav_i \leq \bold{1}_i, \text{ and } \alphav_i \cdot \bold{1} = 0
\eea
Where $\lambda$ is the relularization parameter, $\bold{1}_i$ is the all zero vector except the $i$ coordinate and $\bold{1}$ is the all one vector.
Note that in this form it is enough to calculate the dot product between any two samples - no need to calculate the map $\phi$.
This is know as the kernel trick \cite{hofmann2008kernel}.
\subsubsection{Pros and Cons}
Support Vector Machine, optimize objective that is very close to the optimal one. Learning is a convex optimization problem hence efficient and classification time complexity is linear in the number of features. In short its is very efficient method.
But this efficiency come with the price of power of expression since we consider only linear models, to overcome this shortcoming we use kernels.

When using kernel SVM the efficiency depend on the number of samples\footnote{More accurately it depend on the margin of the problem but in practice it is not usually come into effect.} since usually the number of samples that participate in the classifier scales with the number of samples( the number of non-zero $\alphav_i$). Hence both learning and classification can be relatively costing.
Moreover in both linear and kernel SVM, there is no principle way to integrate prior knowledge. This has grate importance in many fields where there is strong dependencies between the features and known structure for example the task of object recognition.

\subsection{Neural Networks}
Neural networks, as SVM, try to solve classification problems.
But instead of restricting  the function class to linear function, it choose a different class of cassification functions that we will now describe.
Remember that a classification function is a function $f: \Re^p \times \Re^q \to \cL$ so a neural networks have the form:\bea
\label{eq:neural_networks}
f(\xx, \wparams) &=& \argmax_{r \in \cL} \zz_{L}(\xx, \wparamsi{L})_r\\
\zz_{k}(\xx, \wparamsi{k}) &=& \sgn{W_k \zz_{k-1}(\xx, \wparamsi{k-1}}\\
\zz_{1}(\xx,W_1) &=&  \sgn{W_1 \xx}
\eea
Where $\sgn{\xx}$ is a function $\Re^q \to \Re^q$, $\wparamsi{k} = W_1,\ldots,W_k$ and $\wparams = \wparamsi{L}$ where $L$ is the depth of the network.
The $\sgn{x}$ function is known as the activation function a simple non linear function.
Examples for such functions are $\sgn{x} = \deltaF{x > 0}$ (zero one), $\sgn{x} = \deltaF{x > 0}x$ (ReLu) or $\sgn{x} = \frac{1}{1+\exp{-x}}$ (sigmoid)\footnote{We abuse notation by using the same symbol for function on scalars and vectors - applying the scalar function independently on each of the vector coordinates (result in a vector).}.

This functions class result in a non convex function hence non convex optimization problem.
But it allow for a quick calculation of the derivative using the chain role and optimization by back propagation\cite{williams1986learning}.

\section{Approximate inference}
\label{sec:approx}
\subsection{Variational Methods}
\subsubsection{Belief Propagation}
\subsubsection{Tree Re-Weighted belief propagation}
\subsection{Sampling}
\section{Learning Graphical Models}
Understanding why you are doing something before doing it, is good practice in life. 
So I will start by presenting two reasons why we learned graphical models.
In many domains the main interest is in understanding the model.
For example, we want to know what is the minimum set of variables that can explain specific variable - which variables are its neighbors.
Or which variable has the greatest effect on another variable etc.
For this purpose we are interested in learning the exact model - a model that is as similar as possible to the ``real'' model \footnote{
In structure learning this sum-down the difference between the two edges sets $ \min_{E} |\exact{E}\setminus (\exact{E} \cap E)| + |E \setminus (\exact{E} \cap E)|$.
When learning the parameters, the functions $\theta_{ij}(x,y)$, defining similarity become more elaborate since two parameters $\thetav \neq \tilde{\thetav}$ while $P(\xx; \thetav) = P(\xx; \tilde{\thetav}), \forall \xx$.
This feature is know as reparameterization and it play important roll in Belief Propagation as was presented \ref{}\todo{FILL REF }}.

On the other hand, in many domains we seek the ability to answer queries.
When asking our model a query we want the result to be similar as if it is the result of the real process.
For example, if we learned a model of digits in an image (the variables are the image pixels and the digit in the picture).
We are looking for a model that given a new image infer the correct digit - the digit that is in the image.
This is regardless of how much the graph structure or the parameters are close to the real model.
In a world with no computations limitation this two objective are the same - we will learned the exact model and preform exact inference.
Unfortunately, exact inference is NP-hard in many cases, hence we will be forced to use approximate inference (see \secref{sec:approx}).
Remember that our objective is to correctly answer queries, taking into account the performance of the inference algorithm is a must.

Most of the research on learning graphical models(GM) seek to reconstruct the real model.
I will give a short survey on learning when the objective is reconstruction of the exact model.
It will be divided into the two main stages of the learning.
First is learning the dependency graph - the edges of the graph $E$.
Than learning the parameters of the model $\thetav$.
\subsection{Structure learning}
First lets define what exactly is our goal.
Having a probability $P(\xx;\exact{\thetav})$ defined as in \eqref{eq:ciluqe_prob}.
We are looking to find the exact edge set $\exact{E}$ given a data set of $N$ identical independent distributed (i.i.d) samples from $P(\xx;\exact{\thetav})$.

I will start by presenting the sample and time complexity of the above problem. 
First if the graph is known to be a tree, the exact structure can be found by maximum spanning tree.  Where the  weights are the empirical information between any two vertices \cite{chowLiu}. 
Later \cite{tan2011learning} prove sample complexity for trees is $\Omega(\log(p))$.
In the global setting, there seem to be four parameters that govern the complexity of structure learning.
The first is  the number of vertices $p$.
The second is the maximum degree in the graph - $\dmax = \max_{i \in V} d_i$.
The third and the forth parameters quantify the strength of the interaction between the variables.
In the Ising model\footnote{ The Ising model is a pairwise binary model where $\cX = \{-1,1\}$ and $P(\xx;\thetav) = \frac{1}{Z(\thetav)} \exp{\sum_{i \in V}\theta_i + \sum_{ij}\theta_{ij}x_ix_j}$} this parameters are easily expressed by (following \cite{santhanam2012information}) $\lambda = \min_{ij} \theta_{ij}$ and $\omega = \max_i \sum_{\nei{i}} \theta_{ij}$.
While in other models (where the cardinality is greater than $|\cX|>2$ or the size of factors greater than $\exists c \in \cC, |c|>2$) it is unknown how to express this quantities - the strength of the interaction between the variables as a function of the model parameters.
Hence lower bounds  on non-binary models ignore these two factors\footnote{In proving upper bounds some conditions on the probability instead of the model is given. In my opinion the above two parameters hide inside the condition see for example \cite{bresler2008reconstruction}}.

The sample complexity bounds are $N > \Omega(\dmax \log p)$ and $N > \Omega(\max\{\dmax^2, \lambda^{-2}\}log p) $ to the general case and the Ising model respectively. This mean that we need at least $N$ samples in order to guarantee recovery of $\exact{E}$ in probability more that $0.5$.
Lower bound on sample complexity of $\Omega(p^{\frac{\dmax}{2}})$ was given by \cite{bresler2014structure} for statistical algorithms \cite{feldman2013statistical}\footnote{Algorithm that use an oracle that return the expectation of any function - no direct access to the data}.

Algorithm for structural graph are abundant, I will give only a short list.
Moreover the algorithms differ what is assumed to be given as input , what is the underline model and other assumption so compression is hard.
I will denote by $a$ parameters (other than $p$, $\dmax$) that depend on the model - usually some bound on interaction strength.
 \begin{itemize}
    \item Gaussian multivariate distributions  is an example where it is clear that the problem is easier since it  is equivalent to the estimation of the non-zeros cell in the inverse of the covariance matrix. An example of algorithm solving this problem are \cite{meinshausen2006high,yuan2007model, friedman2008sparse}. 
    \item As seen before $\dmax$ the maximum degree, play an important role in the lower bounds. Many algorithms \cite{bresler2008reconstruction, abbeel2006learning} assume  $\dmax$ is given as  part of the algorithm input. Knowing this quantity, an exhaustive search over all subsets of size $\dmax$(or a function of it) is preformed in-order to find each vertex neighbors.  In\cite{bresler2008reconstruction} their algorithm's time complexity is $O(p^{2\dmax+1}\log p)$ and sample complexity is $O(\dmax a \log p)$ . While in \cite{abbeel2006learning} their algorithm does not guarantee to reconstruct the model (edge set), but to be as close as needed in a Kullbackâ€“Leibler divergence manner to the true one. Its time complexity bounds are $O(p^{a d})$ and sample complexity $O(\dmax a \log p)$. Another paper in this class is \cite{wu2013learning},  instead of the max degree as an input they require the two sizes. Conditioning on some sets with size smaller or equals to the given quantities insure dependence . In other words $ij \not \in E$ the following take place $\exists S \subset V, |S|< D_1, \forall T \subset V, |T|<D_2    X_i \bot X_j | X_T, X_S$ , and the two quantities are $D_1$ and $D_2$\footnote{In the Ising model $D_1 = \dmax$ and $D_2 = \dmax -1$. When allowing factors with higher cardinality two variables can be independent (and any test will indicate they are so) but given another variable they can be dependent - there exit an edge between the two variables. These two quantities bound the size of such sets. }. 
    \item Focusing on Ising models when a threshold (the minimum difference of the probability when one of its neighbors change its assignment) is given as an input,  \cite{bresler2015efficiently} give an algorithm with time complexity of $O(p^2\log p)$ and sample complexity of $O( a \log p)$. In the paper a more accurate is given -  it take into account the interaction strength . Their algorithm has two stages,  first it collect pseudo-neighborhood for specific vertex - a set that include the correct neighborhood. Than by iterating over all this set, it remove vertices that does not change the conditional probability. Note that this algorithm can not be extended to non pairwise models.
\item In \cite{ravikumar2010high} give another flavor of algorithm. Their algorithm is based on optimization of the conditional likelihood of a vertex given all the other vertices with $l_1$ regularization - $\max_{\thetav} \frac{1}{N}\sum_n^N \log p(x^n_i| \xx^n_{\setminus i};\thetav) - \lambda \sum_{ij} |\theta_{ij}|$. Now the neighborhood set is defined by $\nei{i} = \{\theta_{ij} : |\theta_{ij} > \alpha\}$ where $\alpha$ is a threshold based on the graph parameters. Under certain conditions the algorithm reconstruct $E$ in high probability with time complexity of $O(p^4)$  and sample complexity of $O( d^3 \log p)$. Note that this algorithm is not very sensitive to the selection of $\alpha$ (They suggest how to approximate it) and very practical. Its main draw back is (to my opinion), that the conditions (or parameters) that guarantee its success are not directly connected to the model parameters. Hence it is very hard to compare it to other such algorithms. Another problem is the assumption of pairwise interaction.     
\item Correlation decay is an important concept in structure learning \cite{montanari2009graphical}. The idea is simple, Let denote by $B_{\tau}(i)$ all the vertices that are within distance(a formal definition will be given later) $\tau$ from vertex $i$ and denote by $\partial B_{\tau}(i)$ the boundary of that group. We will require that the assignment of $\partial B_{\tau}(i)$ have negligible effect on the probability of variable $x_i$ for all $i \in V$. It is now clear, if the learned model have this characteristic when deciding what are the neighbors of vertex $i$ only a subset of vertices needed to be considered. By adding this condition \cite{bresler2008reconstruction} approved its time complexity to $O(d^{ad})$. If we further assume that $B_{\tau}(i)$ is a tree (note, the whole graph is not necessarily a tree) we can apply greedy algorithms \cite{netrapalli2010greedy, anandkumar2013learning}. In \cite{anandkumar2013learning} they preform a greedy algorithm with time complexity of $O(d^{2d}p)$ and sample complexity of $O(d^2\log p)$ but focused only on pairwise case.  
\end{itemize}
We can summarize the current state as follows. 
In the general case the maximum degree must be given. Having it as an input, the sample and time complexity are very close to the known lower bounds. 
Hence The big question remain, what needed to be assumed in order to allow efficient learning. 
Correlation decay was considered as an option\cite{montanari2009graphical} but \cite{bresler2014structure} shows that it indeed not a necessary one. 
Another difficulty (which is sometimes ignored) is the jump of difficulty when the factor size is not bounded (or as large as $\dmax$). 
I believe this can be a very int resting research direction, under what conditions  a model with bounded factor size and unbounded degree $\dmax$ can be learned efficiently\footnote{Not exponential in $\dmax$}.
\subsection{Parameters Learning}
As in the previous section I will start with the problem definition. In parameters learning it is assumed that the structure is given (but not necessarily the exact one). The most trivial objective will be to find $\exact{\thetav}$. But this  is well define objective since two parameters can differ but still give the same probability. Hence the objective will be, given a set of i.i.d sample from $p(\xx;\exact{\thetav})$ find $\thetav$ such that for all input $\forall \xx \in \cX^{p}$ we will have the same (or as close as possible) probability $p(\xx;\thetav) = p(\xx;\exact{\thetav})$.
Note that even to check this requirement is exponential of the size of $\xx$.
So other objective, or other way to achieve this goal is required.
I will start with the  most intuitive approach - maximum likelihood(ML).

\subsubsection{Maximum Likelihood}
In ML we are looking for the parameters that will maximize the probability to sample the seen data-set.
Note, that just giving probability of $\frac{1}{N}$ to each of the samples is the maximum likelihood solution, but it suffer from extreme overfitting.
So some regularization is need , in GM it usually done by forcing a structure (forcing Independence assumptions) and the size of the factors.
It is more convenient to maximize the log of the likelihood and to normalize it in the sample size $N$, denote by $X = [\xx^1, \ldots, \xx^N]$ the data-set.
With this the ML objective can be  formally defined  by  using the model in \eqref{eq:basic_model} as, 
\bea
\likelihood{X;\thetav} &=& \frac{1}{N}\log\left(\prod_{n=1}^N p(\xx^n;\thetav)\right) \\
&=& \frac{1}{N}\sum_{n=1}^N\log\left(p(\xx^n;\thetav)\right)\\
&=& \frac{1}{N}\sum_{n=1}^N\log\left(\frac{1}{Z(\thetav)} \exp{\sum_{ij \in E} \theta_{ij}(x^n_i,x^n_j) + \sum_{n \in V} \theta_i(x^n_i)}\right)\\
&=& \frac{1}{N}\sum_{n=1}^N \sum_{ij \in E} \theta_{ij}(x^n_i,x^n_j) + \sum_{i \in V} \theta_i(x^n_i) -\log(Z(\thetav))\\
&=& \sum_{ij \in E} \sum_{x_i,x_j \in \cX}\mub_{ij}(x_i,x_j)\theta_{ij}(x_i,x_j) + \sum_{i \in V} \sum_{x_i \in \cX}\mub_i(x_i)\theta_i(x_i) -\log(Z(\thetav))\\
&=&  \mubv \cdot \thetav - \log(Z(\thetav))
\eea
Where $\mubv$ is the empirical marginals
\be
\mub_i(x_i) = \frac{1}{N}\sum_{n=1}^{N} \deltaF{x_i=x_i^n },\ \
\mub_{ij}(x_i,x_j)  = \frac{1}{N}\sum_{n=1}^{N} \deltaF{x_i=x_i^n \land  x_j=x_j^n}
\ee
ordered in a vector in the same way $\thetav$.

We are looking for $\thetav$ that maximize ML. 
So taking the derivative and compare to zero is the prompted step:
\bea
\frac{ \partial \likelihood{X;\thetav}}{\partial \theta_{i}(x_i,)} &=& \mub_{i}(x_i) - \frac{ \partial \log(Z(\thetav))}{\partial \theta_{i}(x_i)} \\
\frac{ \partial \likelihood{X;\thetav}}{\partial \theta_{ij}(x_i,x_j)} &=& \mub_{ij}(x_i,x_j) - \frac{ \partial \log(Z(\thetav))}{\partial \theta_{ij}(x_i,x_j)} \\
\eea
  The derivative of the partition function is given by\cite{wainwright2008graphical}
\bea
\frac{ \partial \log(Z(\thetav))}{\partial \theta_{k}(x_k)} &=& \sum_{\substack{\xx \in \cX
s.t. \xx_k = x_k}}, \frac{1}{Z(\thetav)}\exp{\sum_{ij \in E} \theta_{ij}(x_i,x_j) + \sum_{n \in V} \theta_i(x_i)} = p(x_i;\thetav)\\
\frac{ \partial \log(Z(\thetav))}{\partial \theta_{ij}(x_i,x_j)} &=& p(x_i,x_j;\thetav)
\eea
We can conclude that the ML parameter $\ml{\thetav}$ must satisfy the moment matching characters - the marginals of the model must equals to the empirical marginals.
\be
\mub_i(x_i) = p(x_i;\thetav), \ \ \ \mub_{ij}(x_i,x_j) = p(x_i,x_j;\thetav)
\ee
