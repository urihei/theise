% !TEX root =../main.tex
\section{Deep Learning and Kernels}
This section relies on two facts.
The first is that kernels allow SVM to produce non-linear classifiers in the input space.
In other words, the use of kernels changes the hypothesis class from linear in the input space to linear in the kernel reproduced Hilbert space.
The second fact is that, the success of deep learning drives to the belief that deep networks describe a good hypothesis class.
Combining the two seems like the logical next step.
Can a kernel be found such that the reproduced Hilbert space defines a hypothesis class that is the same as the hypothesis class deep networks induce?
I will now give a short introduction to some of the ideas in the two fields, which will allow us to discuss relevant work.
\subsection{Support Vector Machine}
\subsubsection{Notations and Definitions}
Classification is one of the most fundamental machine learning problems. The goal of a classifier is simply to assign a label to an object. Assume there are $\card$ possible labels (e.g., when classifying digits $\card=10$). We will consider classifiers specified as follows. Given an input object $\xx \in \Re^\nvars$ and a set of parameters $\ww\in\Re^q$, the classifier is a function $f: \Re^\nvars \times \Re^q \to \Re^\card$. The output label for an input $\xx$ is given simply by:\footnote{Without loss of generality we assume that the labels are $y \in 1,\ldots,\card$}
%%U
%%
\be
y = \arg\max_{r\in 1,\ldots, \card} f_r(\xx,\ww)
\label{eq:argmax_classify}
\ee
Thus $f$ assigns a score $f_r(\xx,\ww)$ to each candidate label $i$, and the highest scoring label is returned.
As an example, the input could be an image and the output the category of the object in the image (e.g., a dog, a cat, a mouse). 

%When $f$ is a linear classifier 	 
% function (more accurately class of functions $f \in \mathcal{F}$) is defined by $f: \Re^p \times \Re^q \to \cL$ where $|\cL|=o,q \in \naturalNumbers$  
 %- it take as input an instance to classify and parameters of $f$  and return a label.

%When learning classifiers it is natural to restrict our attention to a class of functions $f \in \mathcal{F}$ (e.g., all linear classifiers, or all neural nets with one hidden layer). During training, our
%goal is to return a function in $\mathcal{F}$ that performs well on the training data. Formally, consider a loss function $\ell(f(\xx),y)$ that measures how well 

To define the learning problem, one first must define a loss function $\ell(f(\xx,\ww),y)$ which measures the error incurred when using a classifier with parameters $\ww$ to label an object $\xx$ in the case where the true label is $y$.
One loss of interest is the zero-one loss of the classifier in \eqref{eq:argmax_classify}. Namely:
\be
\ell_{zo}(f(\xx,\ww),y) \equiv \deltaF{\arg\max_r{f_r(\xx,\ww)} \neq y} \equiv \begin{cases}
  0 & \arg\max_r{f_r(\xx,\ww)} = y \\
  1 & \arg\max_r{f_r(\xx,\ww_r)} \neq y\\
\end{cases}
\ee
However, since $\ell_{zo}$ is typically hard to optimize, other losses are used in practice. Here we focus on the hinge loss (a convex upper bound on the zero-one loss) defined as follows:
\be
\ell_{h}(f(\xx,\ww),y) \equiv \max_{r}\left\{\deltaF{r \ne y} - f_y(\xx,\ww) + f_r(\xx,\ww)\right\}
\ee
 
The goal of learning is to find a classifier that minimizes the expected loss. Denote the true underlying distribution of $\xx,y$ by $P$. Then we would have liked to 
find $\ww$ such that:
\be
\expect{(\xx,y) \sim P}{\ell(f(\xx, \ww) ,y)}
\label{eq:true_risk}
\ee
is minimized. Of course, we do not know $P$, but assume we have IID samples from it (as in ML. See \secref{sec:max_likelihood}).


%Learning such a function mean to learn its parameters: we want to find parameters  $\ww$ that minimize $\expect{(\xx,y) \sim P}{l(f,\xx, \ww ,y)}$ where $l : \mathcal{F} \times\Re^p\times \Re^q \times \cL \to \Re^{+}$ is the loss function - the cost of returning a label $f(\xx,\ww)$ where the true label is $y$.
Assume we are given $\nsamples$ samples $D = \{(\xx^i,y^i)\}_{i=1}^\nsamples$  where $\xx^i \in \Re^\nvars, y^i \in 1,\ldots,\card$ sampled IID from some fixed distribution $P$.
We will use this sample to approximate the expected value in \eqref{eq:true_risk}.
Namely, we will want to find a $\ww$ that minimizes the empirical loss:
 %\footnote{Since the data sample is finite we need to regularize the parameters, here we select the euclidean (or %Frobenius  norm in the multiclass classification) to be the regularize and $\lambda$ to be its weight's parameter.}.
\be
{1\over \nsamples} \sum_{i=1}^\nsamples \ell(f(\xx^i, \ww) ,y^i)
\ee
To arrive at the classic Support Vector Machine formulation we introduce two further assumptions. First, we consider functions $f$ that are linear in some features $\phiv(\xx)\in\Re^q$ of $\xx$. 
The parameters $\ww$ will be a set of $\card$ vectors $\ww_i \in \Re^q$, and $f_r$ is defined as:
\be
f_r(\xx, \ww) = \ww_r \cdot \phiv(\xx)
\ee  
%Moreover we will restrict $f$ to be linear $f(\xx, W) = \argmax_{r \in \cL}\{ \ww_r \cdot \phi(\xx)\}$ on some known feature map $\phi: \Re^p \to \Re^q, q \in \naturalNumbers$ \footnote{We abuse notation by giving the instance of group $\cL$ and its index the same notation}.
%Finally even though, the most intuitive loss function is the zero one loss $\deltaF{f(\xx) \neq y}$.
%This function is very hard to optimize since it is non-convex.
%One known surrogate is the hinge loss function $\max\{0,1- y(\ww \cdot \phi(\xx))\}$ where $\cL = \{-1,1\}$ while in the multiclass case we have the extension $ \max_{r \in \cL}\{\deltaF{r \ne y} - \ww_y \cdot \phi(\xx) + \ww_r \cdot \phi(\xx)\}$ \cite{crammer2002algorithmic}. All together the objective is:
Finally, we add an quadratic regularization term $\lambda \|\ww\|^2$. This serves several purposes. First, it implies that the $\ww$ will be a linear combination of $\phiv(\xx_i)$ of the training data (this is known as the representer theorem. See \cite{hofmann2008kernel}). Second, it regularizes the weights towards low norms, which results in strong generalization error guarantees \cite{sridharan2009fast}. 
The resulting optimization problem (e.g., see \cite{crammer2002algorithmic}) is:
\be
\label{eq:svm_obj}
\min_{\ww \in \Re^{\card,q}} \sum_{i=1}^\nsamples \max_{r}\{\deltaF{r \ne y} - \ww_{y^i} \cdot \phi(\xx^{i}) + \ww_r \cdot \phi(\xx^i)\} + \lambda \|\ww\|^2
\ee
Since this is a convex optimization problem, it has a convex dual \cite{boyd2004convex} whose solution also yields $\ww$. The dual variables in this case are vectors $\alphav_i\in\Re^\card$ (for $i=1,\ldots,\nsamples$)
and the dual optimization problem is:
\bean
\label{eq:svm_obj_dual}
\max_{\alpha \in \Re^{\nsamples,\card}}&& -\sum_{i,j}^\nsamples(\phi(\xx^i)\cdot\phi(\xx^j))(\alphav_i\cdot\alphav_j) + \lambda\sum_i^\nsamples\alphav_i \cdot \mathbf{1}_{y^i}\nonumber\\
\text{subject to}:&& \forall i\ \ \alphav_i \leq \mathbf{1}_i, \text{ and } \alphav_i \cdot \mathbf{1} = 0
\eean
where $\mathbf{1}_i$ is the all zero vector except the coordinate $i$,  and $\mathbf{1}$ is the all ones vector. The mapping from dual variables $\alphav$ to primal $\ww$ is given by:
\be
\ww_r = \frac{1}{\lambda} \sum_i^\nsamples (\deltaF{y^i=r} - \alpha_{i,r})\phi(\xx^i)
\label{eq:alpha_to_w}
\ee
A nice property of \eqref{eq:svm_obj_dual} is that it only depends on the dot products of $\phiv$ vectors, and not on the vectors themselves. Thus to optimize it, one only needs the dot products 
between training vectors. Furthermore, the structure of $\ww$ in \eqref{eq:alpha_to_w} implies that
calculating $f(\xx';\ww)$ will also only require dot products. Namely:
\be
f(\xx';\ww) = \frac{1}{\lambda} \sum_i^\nsamples (\deltaF{y^i=r} - \alpha_{i,r})\phi(\xx^i) \cdot \phi(\xx')
\ee
The fact that both learning and classification require only dot products is known as the kernel trick (e.g., see \cite{scholkopf2002learning}).
One of its most exciting implications is that we can replace the original feature vector $\xx$ with arbitrary non-linear
features $\phiv(\xx)$, as long as we can efficiently calculate dot products $\phiv(\xx)\cdot\phiv(\xx')$ for any two vectors $\xx$ and $\xx'$. The converse implication is that if we have a kernel function $K(\xx,\xx')$ that can be written as $K(\xx,\xx')=\phiv(\xx)\cdot\phiv(\xx')$ for some $\phiv(\xx)$, then $K$ can be used in both learning and inference as above. Our focus will be on the case where $\phiv$ is given. 

\subsubsection{Extending Hypothesis Classes}
Consider a binary classifier of the form $\sign\left(f(\xx;\ww)\right)$, where $f$ is a function of $\xx$  parameterized by $\ww$.  Given some function of $\alpha(\ww)$ from $\ww$ to the reals, we can 
use $\alpha$ to define a new classifier:
\be
\sign\left(\int f(\xx;\ww)\alpha(\ww) d\ww \right)
\ee
It is clear that these classifiers are a superset of those defined via $f$, since we can always choose $\alpha(\bar{\ww}) = \delta(\bar{\ww}-\ww)$  and recover the classifier $f(\xx;\ww)$. 

The advantage of the above formulation is that it is linear in $\alpha(\ww)$, and we can thus employ the powerful machinery and generlization theory of linear classifiers. This idea was explored in the past, and in particular in \cite{NIPS2008_3495}. In \cite{heinemann2016improper} we use this as a starting point for learning extensions of neural network models.   

\ignore{
\subsubsection{Kernels}
I will present the idea of kernels with an example.
Consider, a two dimension input $p=2$ and a feature map $\phi([x_1, x_2]) = [x_1^2, x_2^2, x_1x_2, x_1, x_2, 1]$ ($q =6$).
Note that a linear classifier in $\Re^6$ induce non-linear classifier in $\Re^2$, in our example a classifier $f = \deltaF{x_1x_2 < 1}$ is clearly linear in the range of $\phi([x_1, x_2])$ while a circle in $\Re^2$.
Define the polynomial kernel as $K_{c,d}(\xx,\xx') = (\xx \cdot \xx'+c)^d$.
It is easy to see that the dot product between any two feature maps (as defined above) equals to the polynomial kernel $K_{c,2}(\xx, \xx') \approx \phi([x_1, x_2]) \cdot \phi([x'_1, x'_2])$\footnote{For equality $\phi([x_1, x_2]) = [x_1^2, x_2^2, \sqrt{2}x_1x_2, \sqrt{2c}x_1, \sqrt{2c}x_2, \sqrt{c}]$}.
As mention above, in SVM only the result of the inner product can be used - instead of explicitly calculating $\phi(\xx)$ for each sample only the kernel can be calculated. 

If the kernel is used while learning (and not the explicit feature maps) the resulted classifier will have the form of $\deltaF{f(\xx) >0}$ where $f(\xx) = \sum_{i=1}^N \alpha_{i}K(\xx,\xx^i)$ \footnote{One big draw back of the kernel method is the dependence of the classification time complexity in the sample size $N$. 

Moreover, all samples with non-zero $\alpha_i$ needed to be saved as a part of the classifier.
This degrade the learning time complexity to $O(N^3)$ and the classification time complexity to $O(Np)$. Many paper try to solve this shortcoming for example \cite{williams2001using,rahimi2007random}}.
Even for this kernel, for a big $p$ using only the kernel instead of full calculation of $\phi(\xx)$ may be of great value.

The importance of kernel though, came from feature maps where $\phi(\xx)$ map to an infinite space $q=\infty$ (Note that its is not necessarily even countable).
For example, the feature map of the Radial Basis Function (RBF) $K_{\sigma}(\xx,\xx') = \exp{\frac{-\|\xx-\xx'\|^2}{2\sigma}}$  has infinite dimension see \cite{shashuaNotes}.
It turn out, that whenever the kernel is positive definite\footnote{A kernel is positive definite if for all $\xx^1, \ldots, \xx^M \in \Re^p$ the matrix $K_{i,j} = K(\xx^i,\xx^j)$ is positive definite see \cite{hofmann2008kernel}} it induce a Hilbert space - kernel reproduced Hilbert space.
The elements of the Hilbert space are any functions that can be written as $f\left(\xx;\{\alpha_i,{\xx'}^i\}_{i=1}^n\right) = \sum_{i=1}^n \alpha_i K(\xx,{\xx'}^i)$ for some $\alpha_i \in \Re$ and ${\xx'}^i \in \Re^p$, and the inner product is defined as $f \cdot g = \sum_{i=1}^n\sum_{j=1}^{n'} \alpha_i\alpha_i' K(\xx^i,{\xx'}^j)$.
So by using positive definite kernel in SVM, one find linear classifier in the reproduced Hilbert space the kernel induce.

Using kernels allow us to change the hypothesis class from linear function in the original space $\Re^p$ to linear classifier in the reproduced Hilbert space.
But a question remain, having a linear classifier in this space, can it be learned using only the kernel and the sample.
The Representer theorem\cite{scholkopf2002learning} answer this question with affirmative.
The function $f \in \mathcal{H}_K$ in the kernel reproduced Hilbert space which minimize the regularized empirical loss can be expressed as a linear combination  of the samples maps $f(\xx) = \sum_i^M \alpha_i K(\xx,x_i)$.
In the SVM case it translate to the fact that the resulted classifier $f\left(\xx;\{\alpha_i, \xx^i\}_{i=1}^N\right)$ minimize the empirical hinge loss with $l_2$ regularization for a given $\lambda$.
}

\ignore{
One big draw back of the kernel method is the dependence of the classification time complexity in the sample size $N$. 
Moreover, all samples with non-zero $\alpha_i$ needed to be saved as a part of the classifier.
This degrade the learning time complexity to $O(N^3)$ and the classification time complexity to $O(Np)$.
There where many papers trying to solve this problem, I will mention only two.
The first \cite{williams2001using} show that the kernel matrix could be approximated by 
}
\ignore{
\subsubsection{Extending Hypothesis Classes using Kernels}
\label{sec:elad_trick}
%The above implies that using kernels enable learning of non linear function.
%Having the above the following question is of importance.
Consider a set of classification 

Giving a hypothesis class or more specifically function that their parameters needed to be learned, could a kernel be defined such a linear classifier in its reproduced Hilbert space will represent the same hypothesis class.
I will now describe a method that by enlarging the hypothesis class defined such a kernel.

Learning a function from hypothesis class translate most of the times to learning its parameters.
This can be translated to finding a delta function for the following integral $ f(\xx;\thetav^*) = \int{f(\xx;\ww)\delta(\ww-\bar{\ww}) d\ww}$.
Where $\delta(\thetav)$ is the Dirac delta function and the learned parameter is $\bar{\ww}$.
Lets relax the above and allow any function - not only the Dirac delta function $ f(\xx;\ww^*) = \int{f(\xx;\ww)\alpha(\ww) d\ww}$
where $\alpha \cX^q \to \Re^{+}$ is some function\footnote{Note, that by that the hypothesis class was enlarged to include any combination of classifiers.}.
But how can such function can be learned, or in our context can a kernel be defined such SVM learned such a function.

Define the map $\Phi(\xx) = f(\xx,\ww)$\footnote{Note that here we treat $\ww$ as variable and not a parameter} it take as an input a sample and return a function of the parameters $g : \Re^q \to \Re$.
Lets define the following kernel $K(\Phi(\xx),\Phi(\xx')) = \int{f(\xx,\ww) f(\xx',\ww) d \ww}$\footnote{This is the basic integral for $L^2$.}.
An element of the reproduced Hilbert space is $g\left(\Phi(\xx);\{\alpha_i,{\xx'}^i\}_{i=1}^n\right) = \sum_{i=1}^n \alpha_i \int{f(\xx,\ww) f(\xx',\ww) d \ww}$\footnote{Note that the Dirac delta function is not in the reproduced Hilbert space since $\int \delta(\ww)\delta(\ww) d\ww \nless \infty$. However it could be approximate by functions inside the Hilbert space to any requested accuracy}.
Having this, using this kernel in SVM result with a classifier that by the Representer theorem\cite{scholkopf2002learning} it guaranteed to minimize \eqref{eq:svm_obj_dual} (no other element of this Hilbert space results with a lower value of \eqref{eq:svm_obj_dual}).

Summarizing, giving any function $f(\xx;\ww)$ the kernel $K(\xx,\xx')=\int{f(\xx,\ww) f(\xx',\ww) d \ww}$ is defined.
Using this kernel in SVM will result in a convex optimization of the learning problem.
This however came with the price, first by extending the hypothesis class the sample complexity increase as well.
Second, the function can only be approximated - the sample complexity depend on the needed quality of the approximations.
 
I will now present basic notation for deep learning.
}
\ignore{Support Vector Machine, optimize objective that is very close to the optimal one. Learning is a convex optimization problem hence efficient and classification time complexity is linear in the number of features. In short its is very efficient method.
But this efficiency come with the price of power of expression since we consider only linear models, to overcome this shortcoming we use kernels.

When using kernel SVM the efficiency depend on the number of samples\footnote{More accurately it depend on the margin of the problem but in practice it is not usually come into effect.} since usually the number of samples that participate in the classifier scales with the number of samples( the number of non-zero $\alphav_i$). Hence both learning and classification can be relatively costing.
Moreover in both linear and kernel SVM, there is no principle way to integrate prior knowledge. This has grate importance in many fields where there is strong dependencies between the features and known structure for example the task of object recognition.
}
\subsection{Neural Network Models}
Neural network (NN) models have recently revolutionized machine learning, by demonstrating significant performance improvements in applications like image categorization \cite{krizhevsky2012imagenet} and speech recognition \cite{hinton2012deep}. A typical NN involves a sequence of linear projections and non-linear activation functions.
In our context, we define a NN model as a function $f: \Re^\nvars \times \Re^q \to 1,\ldots,\card$ defined recursively as:
\bea
\label{eq:neural_networks}
f(\xx, \wparams) &=& \argmax_{r} \zz_{L}(\xx, \wparamsi{L})_r\\
\zz_{k}(\xx, \wparamsi{k}) &=& \sgn{W_k \zz_{k-1}(\xx, \wparamsi{k-1}}\\
\zz_{1}(\xx,W_1) &=&  \sgn{W_1 \xx}
\eea
The $\sgn{x}$ function is the activation function a simple non linear function.
Examples for such functions are $\sgn{x} = \deltaF{x > 0}$ (zero one), $\sgn{x} = \deltaF{x > 0}x$ (ReLu) or $\sgn{x} = \frac{1}{1+\exp{-x}}$ (sigmoid).
We abuse notation and when $\sgn{\xx}$ is applied to a vector,  it worked element-wise $\sgn{\xx} = [\sgn{\xx_1},\ldots,\sgn{\xx_q}]$.
%Where $\sgn{\xx}$ is a function $\Re^q \to \Re^q$,\atodo{define $\sgn{\xx}$ first as a scalar function and then explain that it operates elemenwise}
Further, $\wparamsi{k} = W_1,\ldots,W_k$ is a collection of matrices $W_1$ to $W_k$, each matrix $W_i \in \Re^{l_i,l_{i-1}}$ where $l_i$ is the size of layer $i$ with $l_0 = \nvars$.
The parameters of the whole network will be denoted by $\wparams = \wparamsi{L}$ where $L$ is the depth of the network.

The resulting function $f$ is non-convex, and hence the standard loss minimization used in machine learning will correspond to a non-convex optimization problem. However, optimization algorithms like stochastic gradient descent can still be applied to the problem (since gradient can be calculated using back-propagation \cite{williams1986learning}), and empirically often converge to models that perform well. 

\subsection{Deep Learning and Kernels}
Deep learning and kernel based learning are both highly influential approaches in the machine learning community. Each has its advantages, both theoretical and empirical. For example, kernel 
methods offer the advantage of globally optimal convex optimization, while deep learning offers excellent empirical performance and a highly expressive hypothesis class that seems to be a good fit for practical domains.

A natural and intriguing question is whether these two approaches can be combined. Indeed, in recent years, several works have addressed this issue, and our paper \cite{heinemann2016improper} also contributes to this effort.

One nice observation made by several researchers \cite{le2007continuous,cho2009kernel} is that networks with a continuum of hidden units can be translated into a kernel based learning problem. Our paper  \cite{heinemann2016improper} uses some technical insights from these papers, but considers finite networks. Another relevant line of work is Bayesian neural networks \cite{neal1995bayesian} which also integrate over a continuum of neural net models.

Another line of work linking deep learning and kernels considers random features, or equivalently random initializations of neural net models. Several authors have shown relations between this setup and kernels \cite{rahimi2007random,giryes2015deep,daniely2016toward}.

Finally, we note that kernels can be used to capture certain invariants of the input space, a goal shared by neural architectures. For such approaches see e.g., \cite{bruna2013invariant}.


\ignore{
\subsection{Deep Networks Kernels}

Above it was argued that a kernel that induce the same hypothesis class as deep networks is of value.
In \cite{le2007continuous} a one layer kernel was introduced with activation function sign - if $x>0$ $\sgn{x} = 1$ else $\sgn{x}= -1$.
They show that the kernel for one layer with infinite number of hidden units is $K(\xx,\xx') = 1-\frac{\sqrt{2}}{\delta \sqrt{(d-1)\pi}} \|\xx-\xx'\|$ where $\|\xx'\| \leq \delta \in \Re$. 
But the first work that consider deep networks was of  \cite{cho2009kernel}.
Our work \cite{heinemann2016improper}is based on this work\footnote{In fact we started with the general direction of \secref{sec:elad_trick} with activation function of zero one and ReLu- this result with exactly the same kernels as \cite{cho2009kernel}. As I will show later for deeper networks there is a difference between the methods.} hence it will be presented in more details.
Lets return with the simple case of one layer network. 
Kernel define an inner product in some space.
So the intuitive definition for the wanted kernel will be a dot product of the network output,
\be
\label{eq:saul_ker_motivation}
K(\xx, \xx';W_1) = \frac{1}{l_1}\sgn{W_1\xx} \cdot \sgn{W_1\xx'} = \frac{1}{l_1}\sum_{i=0}^{l_1} \sgn{\ww_1^i \xx}\sgn{\ww_1^i \xx')}
\ee
Where $\ww_1^i$ is the $i$ row of the matrix $W_1$, $\sgn{x}$ is some activation function  and $l_1$ is the number of neurons in the first hidden layer.

But this kernel depend on the an known parameter $W_1$.
If it is assumed that the parameters $\ww_1^i \sim N(0,1)$ are distributed normally and the number of nodes goes to infinity $l_1 \to \infty$ the kernel can be written as
\be
K(\xx, \xx') = \int{\frac{\exp{\frac{\|\ww\|^2}{2}}}{(2\pi)^p} \sgn{\ww \cdot \xx} \sgn{\ww\cdot \xx'}}
\ee

This integral can be solved for a family of activation functions \footnote{ The activation functions are defined as $\sgn{x}_n = \deltaF{x >0}x^n $  a close form equation is given for the integral for all $n$}.
Define $\theta = \cos^{-1}\left( \frac{\xx \cdot \xx'}{\|\xx\|\|\xx'\|}\right)$ if the activation function is the zero one the kernel is defined as $K_0(\xx,\xx') = 1 - \frac{1}{\pi}\theta$.
While the ReLu activation function induce the following $K(\xx,\xx') = \frac{1}{\pi}\|\xx\|\|\xx'\|\left(\sin \theta +(\pi-\theta)\cos \theta\right)$

So far the kernel describe one layer networks (with infinite number of nodes).
Can a kernel be found to deeper networks.
Returning to \eqref{eq:saul_ker_motivation} a kernel for network with depth $L$ will look like (using \eqref{eq:neural_networks} notations)
\bea
K_L(\xx,\xx') &=& \zz_{L}(\xx, \wparamsi{L}) \cdot \zz_{L}(\xx', \wparamsi{L})\\
 &=& K\left(\zz_{L-1}(\xx, \wparamsi{L-1}), \zz_{L-1}(\xx', \wparamsi{L-1})\right)
\eea
remember that for $l_i = \infty$ for all $1 \leq i \leq L$.
Using the fact the kernels itself is defined only by inner products this equation have a closed form formulation.
For example the deep kernel with activation function zero one can be written as $K_{L+1}(\xx,\xx') = 1 - \frac{1}{\pi}\cos^{-1}\left(K_{L}(\xx,\xx')\right)$
While with the ReLu activation function, it can be written as 
\be
K_{L+1}(\xx,\xx') = \frac{1}{\pi}\|\xx\|\|\xx'\|\left(\sin \theta_L +(\pi-\theta_L)\cos \theta_L\right)
\ee
where $\theta_L = \cos^{-1}\left(K_L(\xx,\xx')\right)$\footnote{General equation is given for all $n$}.

The main draw back of this method is that it deal only with infinite networks - each layer has an infinite number of variables.
Another point to note that it assume the parameters are normally distributed.
In our paper \secref{sec:impnet} we deal with these two drawbacks.
First, we present a kernel for networks with finite number of nodes in each level.
Second, we assume the parameters are uniformly sampled from the unit ball a less restricted distribution.
}
