\subsection{Learning Graphical Models}
\label{sec:learning}
Understanding why you are doing something before doing it, is good practice in life. 
So I will start by presenting two reasons why we learned graphical models.
In many domains the main interest is in understanding the model.
For example, we want to know what is the minimum set of variables that can explain specific variable - which variables are its neighbors.
Or which variable has the greatest effect on another variable etc.
For this purpose we are interested in learning the exact model - a model that is as similar as possible to the ``real'' model \footnote{
In structure learning this sum-down the difference between the two edges sets $ \min_{E} |\exact{E}\setminus (\exact{E} \cap E)| + |E \setminus (\exact{E} \cap E)|$.
When learning the parameters, the functions $\theta_{ij}(x,y)$, defining similarity become more elaborate since two parameters $\thetav \neq \tilde{\thetav}$ while $P(\xx; \thetav) = P(\xx; \tilde{\thetav}), \forall \xx$.
This feature is know as reparameterization and it play important roll in Belief Propagation as was presented in \secref{sec:belief}}.

On the other hand, in many domains we seek the ability to answer queries.
When asking our model a query we want the result to be similar as if it is the result of the real process.
For example, if we learned a model of digits in an image (the variables are the image pixels and the digit in the picture).
We are looking for a model that given a new image infer the correct digit - the digit that is in the image.
This is regardless of how much the graph structure or the parameters are close to the real model.
In a world with no computations limitation this two objective are the same - we will learned the exact model and preform exact inference.
Unfortunately, exact inference is NP-hard in many cases, hence we will be forced to use approximate inference (see \secref{sec:approx}).
Remember that our objective is to correctly answer queries, taking into account the performance of the inference algorithm is a must.

Most of the research on learning graphical models(GM) seek to reconstruct the real model.
I will give a short survey on learning when the objective is reconstruction of the exact model.
It will be divided into the two main stages of the learning.
First is learning the dependency graph - the edges of the graph $E$.
Than learning the parameters of the model $\thetav$.
\subsection{Structure learning}
First lets define what exactly is our goal.
Having a probability $P(\xx;\exact{\thetav})$ defined as in \eqref{eq:ciluqe_prob}.
We are looking to find the exact edge set $\exact{E}$ given a data set of $M$ identical independent distributed (i.i.d) samples from $P(\xx;\exact{\thetav})$.

I will start by presenting the sample and time complexity of the above problem. 
First if the graph is known to be a tree, the exact structure can be found by maximum spanning tree.  Where the  weights are the empirical information between any two vertices \cite{chowLiu}. 
Later \cite{tan2011learning} prove sample complexity for trees is $\Omega(\log(p))$.
In the global setting, there seem to be four parameters that govern the complexity of structure learning.
The first is  the number of vertices $p$.
The second is the maximum degree in the graph - $\dmax = \max_{i \in V} d_i$.
The third and the forth parameters quantify the strength of the interaction between the variables.
In the Ising model\footnote{ The Ising model is a pairwise binary model where $\cX = \{-1,1\}$ and $P(\xx;\thetav) = \frac{1}{Z(\thetav)} \exp{\sum_{i \in V}\theta_i + \sum_{ij}\theta_{ij}x_ix_j}$} this parameters are easily expressed by (following \cite{santhanam2012information}) $\lambda = \min_{ij} \theta_{ij}$ and $\omega = \max_i \sum_{\nei{i}} \theta_{ij}$.
While in other models (where the cardinality is greater than $|\cX|>2$ or the size of factors greater than $\exists c \in \cC, |c|>2$) it is unknown how to express this quantities - the strength of the interaction between the variables as a function of the model parameters.
Hence lower bounds  on non-binary models ignore these two factors\footnote{In proving upper bounds some conditions on the probability instead of the model is given. In my opinion the above two parameters hide inside the condition see for example \cite{bresler2008reconstruction}}.

The sample complexity bounds are $M > \Omega(\dmax \log p)$ to the general case and $N > \Omega(\max\{\dmax^2, \lambda^{-2}\}log p) $ for the Ising model.
This mean that we need at least $M$ samples in order to guarantee recovery of $\exact{E}$ in probability more that $0.5$.
Lower bound on sample complexity of $\Omega(p^{\frac{\dmax}{2}})$ was given by \cite{bresler2014structure} for statistical algorithms \cite{feldman2013statistical}\footnote{Algorithm that use an oracle that return the expectation of any function - no direct access to the data}.

Algorithm for structural graph are abundant, I will give only a short list.
Moreover the algorithms differ what is assumed to be given as input , what is the underline model and other assumption so compression is hard.
I will denote by $a$ parameters (other than $p$, $\dmax$) that depend on the model - usually some bound on interaction strength.

Gaussian multivariate distributions  is an example where it is clear that the problem is easier since it  is equivalent to the estimation of the non-zeros cell in the inverse of the covariance matrix. An example of algorithm solving this problem are \cite{meinshausen2006high,yuan2007model, friedman2008sparse}. 

As seen before $\dmax$ the maximum degree, play an important role in the lower bounds. Many algorithms \cite{bresler2008reconstruction, abbeel2006learning} assume  $\dmax$ is given as  part of the algorithm input. Knowing this quantity, an exhaustive search over all subsets of size $\dmax$(or a function of it) is preformed in-order to find each vertex neighbors.  In\cite{bresler2008reconstruction} their algorithm's time complexity is $O(p^{2\dmax+1}\log p)$ and sample complexity is $O(\dmax a \log p)$ . While in \cite{abbeel2006learning} their algorithm does not guarantee to reconstruct the model (edge set), but to be as close as needed in a Kullbackâ€“Leibler divergence manner to the true one. Its time complexity bounds are $O(p^{a d})$ and sample complexity $O(\dmax a \log p)$. Another paper in this class is \cite{wu2013learning},  instead of the max degree as an input they require the two sizes. Conditioning on some sets with size smaller or equals to the given quantities insure dependence . In other words $ij \not \in E$ the following take place $\exists S \subset V, |S|< D_1, \forall T \subset V, |T|<D_2    X_i \bot X_j | X_T, X_S$ , and the two quantities are $D_1$ and $D_2$\footnote{In the Ising model $D_1 = \dmax$ and $D_2 = \dmax -1$. When allowing factors with higher cardinality two variables can be independent (and any test will indicate they are so) but given another variable they can be dependent - there exit an edge between the two variables. These two quantities bound the size of such sets. }. 

Focusing on Ising models when a threshold (the minimum difference of the probability when one of its neighbors change its assignment) is given as an input,  \cite{bresler2015efficiently} give an algorithm with time complexity of $O(p^2\log p)$ and sample complexity of $O( a \log p)$. In the paper a more accurate bound is given  which take into account the interaction strength . Their algorithm has two stages,  first it collect pseudo-neighborhood for specific vertex - a set that include the correct neighborhood. Than by iterating over all this set, it remove vertices that does not change the conditional probability. Note that this algorithm can not be extended to non pairwise models.

In \cite{ravikumar2010high} give another flavor of algorithm. Their algorithm is based on optimization of the conditional likelihood of a vertex given all the other vertices with $l_1$ regularization - $\max_{\thetav} \frac{1}{N}\sum_n^N \log p(x^n_i| \xx^n_{\setminus i};\thetav) - \lambda \sum_{ij} |\theta_{ij}|$. Now the neighborhood set is defined by $\nei{i} = \{\theta_{ij} : |\theta_{ij} > \alpha\}$ where $\alpha$ is a threshold based on the graph parameters. Under certain conditions the algorithm reconstruct $E$ in high probability with time complexity of $O(p^4)$  and sample complexity of $O( d^3 \log p)$. Note that this algorithm is not very sensitive to the selection of $\alpha$ (They suggest how to approximate it) and very practical. Its main draw back is (to my opinion), that the conditions (or parameters) that guarantee its success are not directly connected to the model parameters. Hence it is very hard to compare it to other such algorithms. Another problem is the assumption of pairwise interaction.     

Correlation decay is an important concept in structure learning \cite{montanari2009graphical}. The idea is simple, Let denote by $B_{\tau}(i)$ all the vertices that are within distance(a formal definition will be given later) $\tau$ from vertex $i$ and denote by $\partial B_{\tau}(i)$ the boundary of that group. We will require that the assignment of $\partial B_{\tau}(i)$ have negligible effect on the probability of variable $x_i$ for all $i \in V$. It is now clear, if the learned model have this characteristic when deciding what are the neighbors of vertex $i$ only a subset of vertices needed to be considered. By adding this condition \cite{bresler2008reconstruction} approved its time complexity to $O(d^{ad})$. If we further assume that $B_{\tau}(i)$ is a tree (note, the whole graph is not necessarily a tree) we can apply greedy algorithms \cite{netrapalli2010greedy, anandkumar2013learning}. In \cite{anandkumar2013learning} they preform a greedy algorithm with time complexity of $O(d^{2d}p)$ and sample complexity of $O(d^2\log p)$ but focused only on pairwise case.  

We can summarize the current state as follows. 
In the general case the maximum degree must be given. Having it as an input, the sample and time complexity are very close to the known lower bounds. 
Hence The big question remain, what needed to be assumed in order to allow efficient learning. 
Correlation decay was considered as an option\cite{montanari2009graphical} but \cite{bresler2014structure} shows that it indeed not a necessary one. 
Another difficulty (which is sometimes ignored) is the jump of difficulty when the factor size is not bounded (or as large as $\dmax$). 
I believe this can be a very interesting research direction, under what conditions  a model with bounded factor size and unbounded degree $\dmax$ can be learned efficiently\footnote{Not exponential in $\dmax$}.
\subsection{Parameters Learning}
As in the previous section I will start with the problem definition. 
In parameters learning it is assumed that the structure is given (but not necessarily the exact one). The most trivial objective will be to find $\exact{\thetav}$.  
This  objective is not well define,  since two parameters can differ $\hat{\thetav} \neq \tilde{\thetav}$ but still give the same probability $\forall\ \xx \in \cX^p\ p(\xx;\hat{\thetav}) = p(\xx;\tilde{\thetav})$.
Hence the objective will be to find the parameters that will define the same probability: given a set of i.i.d observations, sampled from $p(\xx;\exact{\thetav})$ find $\thetav$ such that for all input $\forall \xx \in \cX^{p}$ we will have the same (or as close as possible) probability $p(\xx;\thetav) = p(\xx;\exact{\thetav})$.
Note that even to check this requirement is exponential of the size of $\xx$.
So other objective, or other way to achieve this goal is required.
I will start with the  most intuitive approach - maximum likelihood(ML).

\subsubsection{Maximum Likelihood}
\label{sec:max_likelihood}
In ML we are looking for the parameters that will maximize the probability to sample the seen data-set.
Note, that just giving probability of $\frac{1}{N}$ to each of the samples is the maximum likelihood solution, but it suffer from extreme overfitting.
So some regularization is need , in GM it usually done by forcing a structure (forcing Independence assumptions) and the size of the factors.
I will now define the maximum likelihood objective and than discuss its merits and drawbacks.

It is more convenient to maximize the log of the likelihood and to normalize it by the sample size $N$, this changes do not change the parameters that maximize the  function $\arg \max_x\frac{1}{N}\log(f(x)) = \arg\max_x f(x)$. 
Denote by $X = [\xx^1, \ldots, \xx^N]$ the data-set.
With this the ML objective can be  formally defined  by  using the model in \eqref{eq:basic_model} as, 
\bean
\likelihood{X;\thetav} &=& \frac{1}{N}\log\left(\prod_{n=1}^N p(\xx^n;\thetav)\right) \nonumber \\
&=& \frac{1}{N}\sum_{n=1}^N\log\left(p(\xx^n;\thetav)\right)\nonumber\\
&=& \frac{1}{N}\sum_{n=1}^N\log\left(\frac{1}{Z(\thetav)} \exp{\sum_{ij \in E} \theta_{ij}(x^n_i,x^n_j) + \sum_{n \in V} \theta_i(x^n_i)}\right)\nonumber\\
&=& \frac{1}{N}\sum_{n=1}^N \sum_{ij \in E} \theta_{ij}(x^n_i,x^n_j) + \sum_{i \in V} \theta_i(x^n_i) - A(\thetav)\nonumber\\
&=& \sum_{ij \in E} \sum_{x_i,x_j \in \cX}\mub_{ij}(x_i,x_j)\theta_{ij}(x_i,x_j) + \sum_{i \in V} \sum_{x_i \in \cX}\mub_i(x_i)\theta_i(x_i) -A(\thetav)\nonumber\\
&=&  \mubv \cdot \thetav - A(\thetav) \label{eq:ML}
\eean
Where $\mubv$ is the empirical marginals
\be
\mub_i(x_i) = \frac{1}{N}\sum_{n=1}^{N} \deltaF{x_i=x_i^n },\ \
\mub_{ij}(x_i,x_j)  = \frac{1}{N}\sum_{n=1}^{N} \deltaF{x_i=x_i^n \land  x_j=x_j^n}
\ee
ordered in a vector in the same way as $\thetav$.

We are looking for $\thetav$ that maximize ML. 
So taking the derivative and compare to zero is the prompted step:
\bean
\label{eq:ml_derv_single}
\frac{ \partial \likelihood{X;\thetav}}{\partial \theta_{i}(x_i)} &= \mub_{i}(x_i) - \frac{ \partial \log(Z(\thetav))}{\partial \theta_{i}(x_i)} =& \mub_{i}(x_i) - p(x_i;\thetav) \\
\label{eq:ml_derv_pairs}
\frac{ \partial \likelihood{X;\thetav}}{\partial \theta_{ij}(x_i,x_j)} &= \mub_{ij}(x_i,x_j) - \frac{ \partial \log(Z(\thetav))}{\partial \theta_{ij}(x_i,x_j)} =& \mub_{ij}(x_i,x_j) - p(x_i,x_j;\thetav)
\eean
 Where the derivative of the partition function is given by \eqref{eq:pratition_derivative}.
We can conclude that the ML parameter $\ml{\thetav}$ must satisfy the moment matching characters - the marginals of the model must equals to the empirical marginals.
\be
\label{eq:moment_matching}
\mub_i(x_i) = p(x_i;\thetav), \ \ \ \mub_{ij}(x_i,x_j) = p(x_i,x_j;\thetav)
\ee
This feature is very intuitive, we want the resulted probability to be able to re-generate the sample's marginals.
ML has other desirable qualities (under some conditions): 
It is consist - when $N \to \infty$, we will recover the exact parameters $\lim_{N \to \infty} \arg \max_{\thetav} p(X^N;\theta) =  \exact{\thetav}$\footnote{This does not contradict the above discussion - part of the conditions is uniqueness $\exact{\thetav}$.}. 
It is efficient - in the limit no other consist estimator has lower mean square error than ML.   
Asymptotic Normality: with sufficient data, the ML estimator mean will be the true parameter with added Gaussian noise, the probability to get very wrong parameter go down exponentially fast.

It seem that ML is indeed all we need, this is however not the whole picture.
ML suffer from several drawback.
The first and most important it is not time efficient.
Calculating the ML is $\#P$ since in involve calculating the partition function.
Moreover, even calculating its derivative is $\#P$ since it involve inferring the marginals of the model.
%So direct gradient descent is not possible since it involve calculating the current model marginals.
Indeed \ignore{Even if we tackle the time complexity of finding the marginals}, a result by \cite{bresler2014hardness,montanari2015computational} show that if $NP \neq RP$ no algorithm that use only the model marginals ( marginals corresponding to the model parameters) can learn the model parameters. 
Hence if a time efficient algorithm that find the ML parameter is to be found, it must use higher order of marginals.

In practice, since inference is hard, approximate inference is used.
Due to BP success, it was used to approximate the marginal in gradient descent.
The next section discuss the meaning of doing do.
\subsubsection{Maximum Likelihood with BP as the approximate inference}
Optimizing ML is usually done by gradient descent.
In order to calculate the derivative, the marginals of the current model are needed.
BP is a successful algorithm for finding the marginals.
so using it to find the marginals seem like a good choice.
But what is optimize exactly?

using the variation stand point \eqref{eq:ML} cab be written as
\be
\sup_{\thetav \in \Omega^G} \likelihood{X;\thetav} = \sup_{\thetav \in \Omega^G}\left\{ \mubv \cdot \thetav - \sup_{\muv \in \margpoly} \left\{\muv \cdot \thetav - A^*(\muv)\right\}\right\}
\ee
Since BP is used and remembering the result of \cite{yedidia2000generalized} the optimization can be written as ,
\be
\bethelikelihood{X;\thetav}  = \sup_{\thetav \in \Omega^G} \left\{\mubv \cdot \thetav - \sup_{\tauv \in \lclmargpoly} \left\{\tauv \cdot \thetav + H_B(\tauv)\right\}\right\}
\ee
This will be called the Bethe maximum likelihood.
It is a convex function of $\thetav$ since $A_B(\thetav)$ is a convex function of $\thetav$ (but not smooth).
Moreover it can be proving that $\bethelikelihood{X; \thetav}$ is strictly convex (see \appref{})  
Taking the derivative and comparing to zero result in,
\be
\mu_{ij}(x_i,x_j) = \convexhull{ \arg \sup_{\tauv \in \lclmargpoly} \left\{\tauv \cdot \thetav + H_B(\tauv)\right\}}
\ee
It is easy to see that if the Bethe energy has only one maximum the moment matching condition \eqref{eq:moment_matching} is satisfied.
While in the case of multiple maximum no moment matching is guaranteed.
Hence, the learned parameters will not reconstruct the empirical marginals.
A question remain, is such empirical marginals exist such that no parameters exist that will guaranteed moment matching?

In the paper \cite{heinemann2012cannot} we prove that such empirical marginals exist, and show that indeed there are marginals in the marginal polytope that will \textbf{never} (for any parameter) be the result of BP.

We will now consider an approximation to the ML Pseudo-Likelihood.
\subsubsection{Pseudo-Likelihood}
Pseudo-Likelihood (PL)\cite{besag1975statistical} is a way to approximate the likelihood.
It is the first of larger family of likelihood approximation called composite likelihood \cite{lindsay1988composite}.
Specifically, if we will note that the likelihood can be written as $ p(\xx;\thetav) = \prod_{i=1}^p p(x_i |x_1,\ldots x_{i-1})$
The PL approximation is
\be
pl(\xx;\thetav) = \prod_{i=1}^p p(x_i |\xx_{\setminus i};\thetav) = \prod_{i=1}^p p(x_i |\xx_{\nei{i}};\thetav)
\ee
Note that with using model \eqref{eq:basic_model}, we can write:
\bea
p(x_k |\xx_{\setminus k};\thetav) 
%&=& \frac{p(\xx;\thetav)}{p(\xx_{\setminus k};\thetav)}\\
&=& \frac{p(\xx;\thetav)}{\sum_{x_k \in \cX }p(\xx;\thetav)}\\
%&=&\frac{\exp{\sum_{ij \in E} \theta_{ij}(x_i,x_j) + \sum_{ i \in V} \theta_i(x_i)}}{\sum_{ \tilde{x}_{k} \in \cX }\exp{\sum_{ij \in E} \theta_{ij}(x_i,x_j) + \sum_{i \in V} \theta_i(x_i)}}\\
&=&\frac{\exp{\sum_{j \in \nei{k}} \theta_{kj}(x_k,x_j) + \theta_k(x_k)}}{\sum_{\tilde{x}_k \in \cX}\exp{\sum_{j \in \nei{k}} \theta_{kj}(\tilde{x}_k,x_j) + \theta_k(\tilde{x}_k)}}\\
\eea
%Now if all we want is to find the parameters we can just maximize the conditional likelihood namely:
Now we can write PL as,
\bea
&&\frac{1}{N} \log \left(\prod_{n=1}^N \prod_{i \in V} p(x^n_i | \xx^n_{\nei{i};\thetav})\right) \\ 
%&=&\sum_{i \in V}\frac{1}{N}\sum_{n=1}^N \sum_{j \in \nei{i}} \theta_{ij}(x^n_i,x^n_j) + \theta_i(x^n_i)\\
%&&-\log\left(\sum_{\tilde{x}_i \in \cX}\exp{\sum_{j \in \nei{i}} \theta_{ij}(\tilde{x}_i,x^n_j) + \theta_i(\tilde{x}_i)}\right) \\
&=& \sum_{i \in V} \sum_{j \in \nei{i}}\sum_{x_i,x_j \in \cX} \mub_{ij}(x_i,x_j)\theta_{ij}(x_i,x_j) + \sum_{x_i \in \cX}\mub_i(x_i)\theta_i(x_i)\\
&&-\sum_{\xx_{\nei{i}} \in \cX^{|\nei{i}|}} \mub_{\nei{i}}(\xx_{\nei{i}})\log\left(\sum_{\tilde{x}_i \in \cX}\exp{\sum_{j \in \nei{i}} \theta_{ij}(\tilde{x}_i,x_j) + \theta_i(\tilde{x}_i)}\right) \\
\eea
And the derivatives are,
\bea
\frac{\partial pl(X^n;\thetav)}{\partial \theta_{ij}(x_i,x_j)} &=&
2 \mub_{ij}(x_i,x_j) \\
&&- \sum_{\xx_{\nei{i}\setminus j} \in \cX^{|\nei{i}-1|}} \mub_{\nei{i}}(x_j,  \xx_{\nei{i}\setminus j}) p(x_i|x_j, \xx_{\nei{i}\setminus j};\thetav)\\
&& - \sum_{\xx_{\nei{j}\setminus i} \in \cX^{|\nei{j}-1|}} \mub_{\nei{j}}(x_i,  \xx_{\nei{j}\setminus i}) p(x_j|x_i, \xx_{\nei{i}\setminus j};\thetav)\\
\frac{\partial pl(X^n;\thetav)}{\partial \theta_{i}(x_i)} &=& 
\mub_i(x_i) - \sum_{\xx_{\nei{i}} \in \cX^{|\nei{i}|}} \mub_{\nei{i}}(\xx_{\nei{i}}) p(x_i|\xx_{\nei{i}};\thetav)
\eea
Note first that it is easy to calculate $ p(x_i|\xx_{\nei{i}};\thetav)$ since the partition function is summation only on the values of $x_i$.
Hence optimizing by gradient descent is plausible.  
Second, this condition is similar to the moment matching condition of \eqref{eq:moment_matching}.
Unfortunately, for each derivative we not only need to a approximate the pairwise marginals but the marginals of the whole neighborhood.
But if we consider the result by \cite{bresler2014hardness,montanari2015computational} to have any time efficient  algorithm we will need moments with higher dimensions hence pay in sample complexity. 
Other characteristic of PL, such as $\thetav$ is asymptotically normally distributed,  can be derived from the more general family of composite likelihood see \cite{varin2011overview}.

Another view point of PL is, that instead of approximating the likelihood, it just preforming ML on the conditional marginals. 
We are looking for the parameters that maximize the conditional probability for each vertex separately.
The derivative of the singleton parameter is the same.
While the pairwise change, so it include only one of the summation - hence a more moment matching like equation.


The first reason to use PL and not ML is time complexity.
While PL parameters can easily be computed via gradient descent.
No efficient way is known to recover ML parameters.
The question whether there are cases when PL give better results than ML is still open.
In the well specified case (the model structure and factors are exact) it was shown that ML is more efficient (sample wise) than PL \cite{liang2008asymptotic}
While in the miss specified case (the more realistic case) it is not clear which method is better(see \cite{varin2011overview} for discussion).
An interesting direction for research is to design an inference algorithm for PL.
Meaning, design an efficient inference algorithm such that it is promised to return the empirical marginals when the model parameters was learned using PL.
   
\subsubsection{Other Methods}
Contrastive Divergance (CD) \cite{hinton2002training}  was designed for models where part of the variables are hidden ( variables that there are no samples include them), but it could be used to general models.
The idea is simple, re-call from \eqref{eq:ml_derv_single} and \eqref{eq:ml_derv_pairs}, that the reason gradient-descent is impractical,  is  the hardness of evaluating the marginals according to the parameters $p(x_i,x_j;\thetav)$. 
One way to approximate these marginals  is by MCMC \secref{sec:sampling}.
If we only preform small number of steps (usually one) to evaluate $p(x_i,x_j;\thetav)$ we result with CD.
Justification to this method was given by \cite{bengio2009justifying} and \cite{carreira2005contrastive} but it success in learning Restricted Boltzmann Machine (RBM) gain her its importance.

In contrast to the previous methods   \cite{abbeel2006learning} give a closed form equation for parameters estimation.
Using the ideas from the proof of the Hammersley-Clifford theorem,  it estimate the canonical factors  - hence learned the graph parameters.
More specifically, any distribution can be written by(using the model in \eqref{eq:ciluqe_prob}) 
\be
p(\xx;\exact{\thetav}) = \frac{1}{Z(\thetav)} \exp{\sum_{c \in \cC}\thetav_c(\xx_c)} = p(\tilde{\xx})\exp{\sum_{c\in\cC} \thetav^{\dagger}_c(\xx_c)}
\ee
Now the canonical factors $\thetav^{\dagger}$ are defined by,
\be
\thetav^{\dagger}_{c}(\xx_c) = \sum_{b \subseteq c} -1^{|c \setminus b|} \log{p(f(b,\xx_c,\tilde{\xx}))}
\ee
where  $f(b,\xx_c,\tilde{\xx}) : \cI \times \cX^{|c|} \times \cX^{|d|}\to \cX^{|d|}$  such that $b \subseteq c\subseteq d$ and for all $i \in d$ if $i \in b$ than $x_i = [\xx_c]_i$ otherwise $x_i = \tilde{\xx_i}$ for the fixed assignment $\tilde{\xx}$.
This can be father simplify by using independence 
\be
\label{eq:abbeel}
\thetav^{\dagger}_{c}(\xx_c) = \sum_{b \subseteq c} -1^{|c \setminus b|} \log{p(f(b,\xx_c,\tilde{\xx}_c) |\tilde{\xx}_{\nei{c}})}
\ee
So a factor involve only the clique and the clique's neighbors.
Hence to estimate the parameters we just need to change $p(f(b,\xx_c,\tilde{\xx}_c) |\tilde{\xx}_{\nei{c}})$ by the corespondent empirical marginals.
In continues  paper \cite{roy2009learning} farther simplify the above equation but with no  implication to parameters learning.
Note the resemblance of  \eqref{eq:abbeel} to the PL equation.
Indeed in \cite{bradley2012sample} it is proved that the two methods give the same results.
They include a compression of the sample complexity of this method, PL(more accurately composite likelihoods)  and ML.

Other methods goes in the same lines - avoiding the partition function. 
For example \cite{hyvarinen2007some} use the ratio between the probability of two assignments, hence the partition function canceled.
Note that using the ratio is not very different than using the conditional probability and indeed \cite{marlin2010inductive} and \cite{marlin2012asymptotic} compare the methods to pseudo-likelihood and explain the differences.
Other methods like \cite{mizrahi2014icml} just cut off each clique with its neighbors and learn each factor independently\footnote{It assume independence of the clique neighbors from their other neighbors. But it is not clear how it effect the sample complexity a non is given in the paper. }.

To conclude, ML is provably the most sample complexity efficient,   but no time efficient algorithm is known.
Moreover, it was proven by \cite{bresler2014hardness,montanari2015computational} that no algorithm that use only the sufficient statistics in the clique order can optimize ML.
On the other hand there is PL that is time efficient but its sample complexity is worst that that of PL.
Methods like composite likelihood can give better sample complexity results while still be time efficient.

\subsection{Inferning}
In a world without computation limitation learning the exact model is optimal.
I our world however it is not always the case.
Since when using the model approximation algorithm will be used, the quality of the learned model should be with respect to the results of the approximation algorithm, not how close the model to the real one.
In \cite{wainwright2006estimating} it   
