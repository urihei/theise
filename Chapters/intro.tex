The world around us is digitized.
What we read (newspapers, letters, chats, articles, etc), what we see (pictures, movies), what we hear (music, radio, Skype, etc) have been, for several years, saved in the format of bytes.
Factories, medical records and financial markets have all been digitized, too.
But this is only the tip of the iceberg; gradually most of our surrounding will be monitored and be transformed into bytes.
These bytes are sent to central units, where the data is stored and intended to be utilized.
This quantity of data is almost useless to humans - no human can even observe this amount of data, let alone say anything intelligible on it.
Moreover, even manual design of tools to use this amount of data, is not possible.
The need for automated ways to create tools that will handle the growing amount of data is immanent.   

Machine learning is the discipline that tries to bridge over this gap.
Its objective is to find algorithms that use past data as input, and return tools that can monitor, classify, recommend etc -  based on the current data.
%The challenge is huge but results of this field merit are already exist.
Graphical Models (GM), Support Vector Machine (SVM) and deep learning are the leading methods in machine learning.
In this work I will touch upon each of these methods, and try to contribute to their understanding and success.

Graphical models describe the world as a stochastic process.
%Each feature is described by a vertex which is connected to all features that directly effect it.
Given an event, it returns the probability to see this event.
This gives us a great deal of power; for example, the most probable event can be found, or the probability of each feature assignment can be calculated.
Moreover, these inference queries could be asked when part of the assignment is unseen.
Unfortunately, there is another side to the story. Inference in GM is usually hard - all known algorithms are impractical\footnote{More formally MAP is NP-complete and MAR is \#P is the general case; see \secref{sec:def}}.
Hence, approximate inference plays an important role in GM research.

One of the most commonly-used algorithms for approximate inference is Belief Propagation \cite{pearl1986fusion}.
This algorithm is exact on trees (the dependencies graph is a tree), but is also used with great success on models with cycles \cite{willsky2002multiresolution,loeliger2004introduction,kschischang2003codes}.
Running BP on models with cycles brings up several problems.
There are cases where this algorithm does not converge; the final results depend on the initial state which is arbitrary; and the quality of the resulted pseudo-marginals varies  (to note, it will be shown that they are not always true marginals).

After the publication of \cite{yedidia2000generalized}, in which BP results are connected to the Bethe free energy, came a surge of publications of related papers, trying to improve on it \cite{elidan2012residual}, change it \cite{welling2001belief, wainwright2003tree, meshi2009convexifying}, analyze its convergence \cite{tatikonda2002loopy, mooij2007sufficient, roosta2008convergence} and understand its results \cite{heskes2002stable, yedidia2005constructing, AlanNips2007, YusukeNips2009, RuozziNips2012}.

As stated above, the required ability is to automatically create the tools; this translates into learning graphical models.
Most of the research on learning GM focuses on learning the exact model - a model as close to reality as possible.
If the objective is to learn a model that will be used for inference, this may not be the correct goal.
In a world without computation restriction, learning the exact model is optimal, since exact inference will be used as an inference algorithm.
Unfortunately, as mentioned above, this usually is not the case, hence only approximate inference is practical.
However, when an approximation algorithm is used as the inference algorithm, its characteristics must be taken into account when learning.

Maximum Likelihood (ML) is the common objective for learning the model parameters.
Optimization is usually done by gradient descent, which requires inference for calculating the derivative (see \secref{sec:max_likelihood}).
In \cite{wainwright2006estimating} it is argued that using the same inference algorithm in learning as when the learned model is used improves the results outcome.
In our paper \cite{heinemann2014inferning} we take another direction. Our goal is to guarantee the quality of the resulted marginals - the marginals that are the output of the inference algorithm when run on the learned model.
BP is known to be exact on trees, a result which can be extended by using parameter that measure how locally tree the model is.
In our paper, an algorithm for learning models (structure and parameters) is given, such that the size of this parameter is bound - hence bounding the error of the results.

As written above, when maximizing ML with gradient descent, an inference algorithm must be used.
BP, as a good approximation inference algorithm, is a plausible choice.
In \cite{heinemann2012cannot} we show that when maximizing the ML in that way, one actually maximizes the Bethe ML (see \secref{sec:Bethe_ML}).
Another characteristic of BP was found - not all the marginals can be a result of BP.
In other words, there are marginals within the marginal polytope that, for any given parameter, will never yield a stable result of BP\footnote{ The paper \cite{pitkow2011learning} basically repeats our result.}.
Combining these two facts, it may be concluded that for some empirical marginals (see definition \eqref{eq:empirical_mar}), no parameter will result with moment matching \footnote{ A property of the learned model is insuring equivalence between the empirical marginals and the model marginals, see \secref{sec:max_likelihood}}, hence gradient descent will never converge \footnote{This explains the failure of BP as an inference algorithm in gradient descent as in \cite{wainwright2006estimating} and others}.
Moreover, since Bethe ML is the real objective, the canonical parameters (\eqref{eq:canonical}) are proved to be optimal when moment matching is achievable.

In recent years, the success of deep networks has been overwhelming\footnote{not all think it to be for the \href{https://www.linkedin.com/pulse/computer-vision-research-my-deep-depression-nikos-paragios?trk=hp-feed-article-title-like}{best}}.
In machine vision \cite{krizhevsky2012imagenet}, natural language processing \cite{mikolov2013efficient} and speech recognition \cite{mohamed2009deep, hinton2012deep}, state of the art results come from deep learning.
In this method, a complex non-linear function is generated by stacking simple non-linear functions.
Gradient descent is used for optimization, but some parameters need to be fine-tuned, along with a number of ``tricks'' that are necessary in order to achieve top results.
Thus, it could be concluded that deep networks is a good hypothesis class, while their optimization techniques are still more an art than solid science.
%This, however, is not yet backed up by a theoretical understanding of their success.
%Moreover, the optimization process requires fine-tuning, with only a collection of rules of thumb of how to set the parameters,  given a new problem.
On the other hand, support vector machine - the former machine learning community trend, is a well-understood technique.
Using kernels, the non-linear classifier may be learned by quadratic programming\cite{scholkopf2002learning}.
Hence, a kernel that will induce the deep networks hypothesis class may enjoy both worlds.

In \cite{cho2009kernel} the authors present a kernel that is related to infinite networks.
It has been shown that the \textit{arc-cosine} function is related to a one-layer network with an infinite number of units \cite{williams1998computation}.
While the \textit{arc-cosine} is related to the zero-one activation function, the authors generalized it to a series of activation functions one of which is the known ReLu.
Using a nice trick, the infinite networks could be stacked one on top of the other, with the result of deep infinite networks.

In our work \cite{heinemann2016improper} we present a finite network kernel.
By using the natural dot product between functions, the kernel is defined as the dot product between the result of a network for the two inputs.
For one layer networks our kernel is similar to that of \cite{cho2009kernel}.
For multi-layer networks, however, a new kernel is defined for a finite number of nodes in each layer.

To summarize, in the following work I will present $3$ papers.
The first, \cite{heinemann2012cannot}, defines a set of marginals that are suitable to be learned by Bethe maximum likelihood \eqref{}.
We give inner and outer bounds for this set.
Marginals outside this set are the ones that for all parameters will not be a sole fix point for BP.
By giving an outer prove, we prove the existence of these marginals.
On the other hand, if the marginals belong to this set, the learning sums up to a closed form equation - the canonical parameters (\eqref{eq:canonical}).
The second, \cite{heinemann2014inferning}, presents an algorithm for learning graphical models (both structure and parameters) that takes into account the used inference algorithm.
The algorithm returns a model in a family of models where the quality of inference by BP could be bound.
While the first two papers focus on graphical models, the third, \cite{heinemann2016improper}, connects SVM and deep networks.
We present a kernel that induces a hypothesis class of combinations of networks with a fixed structure.

In the following paragraphs, I will give a short introduction that covers the background necessary to understanding the context of these papers.
