As more and more digital information becomes available, there is a growing need for automating its analysis, and designing algorithms for learning from it. This is precisely the goal 
of the field of machine learning, which has seen rapid growth in recent years. 
Most machine learning methods can be viewed as fitting a model to data, and then using this model to make predictions. However, both these steps (learning and prediction) often
involve computationally hard problems, leading to use of approximate methods. In this thesis we study several such forms of computational hardness, propose methods
for approximations, and analyze the resulting trade-off.

We first consider the problem of learning probabilistic models for large sets of variables, and using these models to make predictions. Graphical models are one of the most powerful approaches
to this problem, and have been used successfully in a wide range of domains. However, most graphical models of interest are in fact both hard to learn and to perform inference with. As a result
researchers resort to approximating both the learning and inference components. This leads to heuristics whose performance is hard to analyze.

In the first part of the thesis we study learning in graphical models where belief propagation (BP) is the algorithm used for approximate inference. The learning approach we study uses
an approximate maximum likelihood objective that is motivated by the BP approximation. Our analysis of this approach highlights the cases in which it is expected to succeed or fail. Moreover,
the analysis sheds new light on the characterization of BP fixed points. 

In the second part of the thesis we study learning and inference in graphical models whose structure is ``tree like''. We show that in this case there exists an algorithm which can guarantee recovery of model structured and parameters, as well as guarantee accurate inference results on novel probabilistic queries. Our algorithm can be viewed as an extension of the classic Chow Liu method to the ``tree like'' case. 

In the third part we turn to the problem of learning neural net models. Despite the striking success of deep learning models, the learning problem is non-convex and thus may get stuck in local minima. It is thus a challenge to understand how this optimization hurdle is avoided in practice, and develop methods that can be optimized globally provably. Here we propose an approach that
extends the class of neural nets, and results in a convex optimization problem that can be globally optimized. Our approach is to integrate over all possible network models, which turns out to be equivalent to learning linear classifiers with a certain kernel, which we calculate in closed form for some architectures. 

In summary, the thesis studies hardness of learning and inference in several machine learning problems, with an emphasis on graphical models and deep learning. Our results demonstrate that appropriate relaxations of the learning and inference steps, coupled with assumptions on the underlying model, can result in methods that are empirically effective and enjoy theoretical guarantees.



\ignore{
The learning process can be described as follows: First select an hypothesis class, then select a loss function,  and finally find a model within the hypothesis class that minimizes the loss function.
Having the model, it can now be utilized to answer queries - preform inference. 
In this thesis we examine the implication of the time complexity of minimizing the loss function and inference, on the choices of the hypothesis class and loss function.

Choosing the hypothesis class is a crucial step. A good hypothesis class is one that integrates prior knowledge of the problem, and is complex enough to express the richness of the data, while not too complex such that it will easily over-fit.
%The above is usually the main factors in choosing an hypothesis class.
Lately, deep networks proved to be a good hypothesis class.
It easily integrates prior knowledge in deep networks  (for example convolution and pooling \cite{lecun1995convolutional}), it is very expressive, and several techniques are used to avoid over-fitting (such as dropout \cite{srivastava2014dropout}).
A crucial problem still remains: the optimization problem is non-convex, hence the quality of the result can vary.
In this work, we address this difficulty by turning to ``improper'' learning of deep networks.
We extend the deep networks hypothesis class, with a result of a kernel based method.
This allows convex optimization, with a price of increase in sample complexity.

Another interesting hypothesis class is graphical models (GM).
For this hypothesis class, the natural loss function is minus maximum likelihood.
Optimization of ML is usually achieved by gradient decent, which involves calculating the derivative - the model marginals.
Inferring the marginals in the general case is hard, hence an approximation algorithm is used.
Belief propagation (BP) is an approximate inference algorithm that proved to give good results \cite{murphy1999loopy}.
In this work, we show that using BP as the inference algorithm implies optimizing a different objective - the Bethe maximum likelihood.
Arising from this, we show that there are empirical marginals that cannot be learned by this objective.
More accurately, learning these marginals results in a model that cannot reconstruct these marginals - no moment matching.
On the the other hand, for marginals that can be learned, we prove the optimality of a closed form equation.
We give inner and outer bounds to these sets.
Another interesting result of this work is the fact that there are marginals that are \textit{never} the result of BP.

In many circumstances, learning is preformed in order to use the learned model for answering queries.
In GM answering queries - inference is generally hard, hence approximate inference is used.
%When ML is the optimization objective, the use of the approximate algorithm that will be used to answer queries is not taken into consideration.
The quality of the learning process is measured by the quality of the quarry's result.
Since approximate inference is used to answer the quarries, even if the exact model was learned, no guarantee on the quality of the result can be made.
In this work, we present an algorithm that returns a model belonging to a group where the quality of the result of BP is guaranteed.
If the data is sampled from a model in this group, we derive finite sample results, guaranteeing that beyond a certain sample size, the resulting models will answer probabilistic queries with a high level of accuracy.

In summary, this thesis starts to explore the implication of the time complexity of the algorithms that are used in the learning process.
We show that in GM, choosing the approximate inference both for learning and answering queries effects the result, and in deep networks we present a kernel that extends the deep network hypothesis class. 

}
