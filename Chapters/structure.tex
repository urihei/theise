\subsection{Structure learning}
\label{sec:structure}
Given a distribution $P(\xx;\exact{\thetav})$ as in \eqref{eq:basic_model},
the structure learning problem is to find the exact edge set $\exact{E}$, given a data set of $N$ IID samples from $P(\xx;\exact{\thetav})$.

I will start by presenting the sample and time complexity of the problem stated above. 
If the graph is known to be {\em some} tree, the exact structure can be found by maximum spanning tree, where the weights are the empirical mutual information between any two vertices. This is the classic algorithm of Chow and Liu (see \cite{chowLiu}). Later, \cite{tan2011learning} proved that sample complexity for this method is $\Omega(\log({\nvars}))$.\atodo{Explain in footnote what sample complexity is in this case} In the general case, there seem to be four parameters governing the complexity of structure learning.
The first is the number of vertices $\nvars$.\atodo{I changed number of nodes to macro. Make sure it's consistent throughout.} The second is the maximum degree of the graph, which we denote by $\dmax = \max_{i \in V} d_i$.
The third and the fourth parameters quantify the strength of the interaction between the variables.

In the Ising model\footnote{The Ising model is a pairwise binary model where $\cX = \{-1,1\}$ and $P(\xx;\thetav) = \frac{1}{Z(\thetav)} \exp{\sum_{i \in V}\theta_i + \sum_{ij}\theta_{ij}x_ix_j}$}, these last two parameters are easily expressed; for example, in \cite{santhanam2012information} the authors define $\lambda =
\min_{ij} |\theta_{ij}|$ and $\omega = \max_i \sum_{j \in \nei{i}} |\theta_{ij}|$.  However, in other models (where the cardinality of each variable is greater than two, or for non pairwise models), it is unknown how to express these quantities - the strength of the interaction between the variables as a function of the model
parameters, and hence lower bounds on non-binary models ignore these two factors\footnote{In proving upper bounds, some conditions on the probability, instead of the model, is given. In my opinion, the above two parameters hide inside the condition; see for example \cite{bresler2008reconstruction}.}.\atodo{I don't understand this last sentence. Rephrase.}

Given the above parameters, the sample complexity bounds \atodo{Give refs} \atodo{Also, this looks like a lower bound, but it's actually an upper bound so clarify} are $N > \Omega(\dmax \log \nvars)$ in the general case, and for the Ising model $N > \Omega(\max\{\dmax^2, \lambda^{-2}\}\log \nvars)$\footnote{Note that the bound is looser in the general case. This is a result of dropping some model dependent factors - these factors may depend on $d$. See \cite{bresler2008reconstruction} for details}.
This means that at least $N$ samples are necessary in order to guarantee recovery of $\exact{E}$ with probability more than $0.5$.
Lower bound on time complexity of $\Omega(p^{\frac{\dmax}{2}})$ were given by \cite{bresler2014structure} for statistical algorithms \cite{feldman2013statistical}\footnote{Algorithms using an oracle that returns the expectation of any function with no direct access to the data.}.

There are many variants of structure learning algorithms, and I will give only a partial list. Moreover, the algorithms differ in what is assumed to be given as input, what is the underlying model, and other assumptions, so direct comparison is not always possible. In the following, $a$ will denote parameters (other than $\nvars$, $\dmax$) that depend on the model, usually expressing bounds on interaction strength.

When the MRF involves continuous variables, and corresponds to Gaussian multivariate distributions, the problem becomes easier, since it is equivalent to the estimation of the non-zeros cell in the inverse of the covariance matrix. Examples of algorithms solving this problem are \cite{meinshausen2006high, yuan2007model, friedman2008sparse}. 

The maximum degree $\dmax$ plays an important role in the lower bounds.\atodo{Why just lower bounds. Not upper as well?} Many algorithms \cite{bresler2008reconstruction, abbeel2006learning} assume $\dmax$ is provided as input to the algorithm. Knowing this quantity, an exhaustive search over all subsets of size $\dmax$ (or a function of it) is preformed, in order to find each vertex's neighbors.  In\cite{bresler2008reconstruction} the time complexity of the algorithm is $O({\nvars}^{2\dmax+1}\log \nvars)$ and sample complexity is $O(\dmax a \log \nvars)$. In contrast, in \cite{abbeel2006learning} the algorithm does not guarantee reconstructing $E$, but rather similarity in Kullback- Leibler divergence to the true distribution. The time complexity in this case is $O({\nvars}^{a d})$ \atodo{What is $d$ here?} and sample complexity $O(\dmax a \log {\nvars})$. Another paper in this class is \cite{wu2013learning} in which, instead of the max degree as an input, they require two quantities that bound the cardinalities of sets which guarantee detection of dependencies. 
%The quantities are the maximum cardinality of two groups. The first $D_1$ insure that for all pairs of vertices that there is no edge between them there exist a set of cardinality less than $D_1$ such given this set the information between the two variables are less than $\epsilon$. The other quantity $D_2$ say exactly the apposite, for all edges there exist a set of cardinality less that $D_2$ such that the information     Conditioning on some sets with size smaller or equals to the given quantities insure dependence . 

\atodo{What is the goal of this paragraph?}
The quantities $D_1$ and $D_2$ are defined as follows: for all $ij \not \in E$ the following take place: $\exists S \subset V, |S|< D_1, \forall T \subset V, |T|<D_2$ $I(X_i;X_j | X_T, X_S)<\epsilon$ and for all $ij \in E$ $\exists T \subset V, |T|< D_2, \forall S \subset V, |S|<D_1$ $I(X_i;X_j | X_T, X_S)>4\epsilon$\footnote{In the Ising model $D_1 = \dmax$ and $D_2 = \dmax -1$. When allowing factors with higher cardinality, two variables may be independent (and any test will indicate they are so), but given another variable they may be dependent - there exists an edge between the two variables. These two quantities bound the size of such sets.}. 

Focusing on Ising models, when a threshold (the minimum difference of the probability when one of its neighbors changes its assignment)\atodo{The defintion in brackets is completely unclear, and also unclear how it relates to previous methods you discussed} is given as an input, \cite{bresler2015efficiently} give an algorithm with time complexity of $O(p^2\log p)$ and sample complexity of $O( a \log p)$. In the paper, the authors give a more accurate bound\atodo{More accurate than what?}, which takes into account the interaction strength. Their algorithm has two stages: First it collects a pseudo-neighborhood for a specific vertex - a set that includes the correct neighborhood. Then by iterating over all this set, it removes vertices that do not change the conditional probability. Note that this algorithm cannot be extended to non-pairwise models.

In \cite{ravikumar2010high} a different approach is considered. Their algorithm is based on the optimization of the conditional likelihood of a vertex, given all the other vertices with $l_1$ regularization - $\max_{\thetav} \frac{1}{N}\sum_n^N \log p(x^n_i| \xx^n_{\setminus i};\thetav) - \lambda \sum_{ij} |\theta_{ij}|$. Given the optimal $\thetav$, the neighborhood set is defined by $\nei{i} = \{\theta_{ij} : |\theta_{ij} > \alpha\}$, where $\alpha$ is a threshold based on the graph parameters. Under certain conditions, the algorithm reconstructs $E$ with high probability, in time complexity of $O({\nvars}^4)$ and sample complexity of $O( d^3 \log {\nvars})$. Note that this algorithm is not very sensitive to the selection of $\alpha$ - and furthermore, the authors suggest how to approximate it. The algorithm is also very practical. Its main drawback, is that the conditions (or parameters) that guarantee its success are not directly related to the model parameters, and hence it is very hard to compare it to other such algorithms. Another problem is the assumption of pairwise interaction.     

Correlation decay is an important concept in structure learning. There are several definitions of this property, and we focus on the one in \cite{montanari2009graphical}. Let $B_{\tau}(i)$ denote all the vertices that are within distance (for which a formal definition will be given later) $\tau$ from vertex $i$, and denote by $\partial B_{\tau}(i)$ the boundary of that group. The assignment of $\partial B_{\tau}(i)$ is required to have negligible effect on the probability of variable $x_i$ for all $i \in V$. It is now clear, that if the learned model has this characteristic, when deciding what the neighbors of vertex $i$ are, only a subset of vertices need to be considered. By adding this condition, \cite{bresler2008reconstruction} improved its\atodo{what is its here?} time complexity to $O(d^{ad})$. If it is further assumed that $B_{\tau}(i)$ is a tree (note, the whole graph is not necessarily a tree), then greedy algorithms such as \cite{netrapalli2010greedy, anandkumar2013learning} may be applied. In \cite{anandkumar2013learning}, the authors preform a greedy algorithm with time complexity of $O(d^{2d}{\nvars})$ and sample complexity of $O(d^2\log \nvars)$, but focus only on the pairwise case.  

To summarize: in most of the above approaches, the maximum degree must be provided as input, and plays a key role in sample and algorithmic complexity. In most such cases, the sample and time complexity are very close to the known lower bounds. Hence, the next challenge is to define the necessary assumptions in order to allow efficient structure learning.\atodo{Not sure what this last sentence means in this context.}

Correlation decay was considered as an option\atodo{Option for what?} in \cite{montanari2009graphical}, but \cite{bresler2014structure} shows that it, in fact, is not a necessary one. 
Another problem (which is sometimes ignored) is the additional step-up in difficulty when the factor size is not bounded (or is as large as $\dmax$). 
I believe that the conditions under which a model with a bounded factor size and an unbounded degree $\dmax$ can be learned efficiently\footnote{Not exponential in $\dmax$}, may constitute a very interesting research direction.
