\subsection{Sampling Approaches}
\label{sec:sampling}
The marginals of a distribution can be viewed as expected values of indicator random variables. For example $\mu_i(x_i)$ is the expected value of a binary
random variable $Z(X_i)$ that takes value $1$ whenever $X_i=x_i$. Given this observation, a natural approach to estimating marginals is to approximate the expected value via an empirical average over samples drawn from $P(\xx;\thetav)$. 

More formally, suppose that $\xx^{1},\ldots,\xx^{\nsamples}$ are $\nsamples$ samples (each of all the $\nvars$ variables) drawn IID from $P(\xx;\thetav)$. Then we can estimate the singleton marginal $\mu^{\thetav}_i(x_i)$ via:
\be
\hat{\mu}_i(x_i) = \frac{1}{\nsamples}\sum_{m=1}^{\nsamples} \deltaF{\xx_i^m=x_i}
\ee
As $\nsamples \to \infty$ this will converge to the true marginal. Furthermore, we can use concentration bounds like Hoeffding's inequality to characterize the rate of convergence. For the above case this will state that for all $\epsilon > 0$:
%\atodo{Why does the cardinality of $X$ appear here? If you have a single marginal it shouldn't matter what $\cX$ is. Check this - you changed the notation it should be the cardinality of x_i}
\be
P\left[|\mu^{\thetav}_i(x_i) - \hat{\mu}_i(x_i)| > \epsilon\right] < 2\exp{-\frac{\epsilon^2M}{|\cX_i|^2}}
\ee
This in turn implies that with probability greater than $1-\delta$ we will need $O(\frac{\log(\delta) |\cX_i|^2}{\epsilon^2})$ samples to reach accuracy $\epsilon$.

There is, however, one major difficulty with the above approach.
Namely, that sampling IID from $P(\xx;\theta)$ is hard \footnote{This can be easily seen from redction to q-coloring, for example \cite{levin2009markov, bordewich2016mixing}}.
How, then, can a model be sampled, even approximately?

A key idea in the sampling literature is to use a different distribution $Q(\xx)$ from which we {\em can} sample efficiently. 
Markov Chain Monte Carlo (MCMC) is perhaps the most popular and general among these approaches\cite{andrieu2003introduction,levin2009markov}.
%\atodo{cite nando jordan review}
%In this, a Markov chain with two main features is constructed: first, it must be easy to move from one state to another; secondly, the stationary distribution of the Markov chain is $P(\xx)$.

%An MCMC sampler defines a markov chain, where at any given point in the sampling process a new sample $\xx$ is generated based on the previous sample.
To define a Markov chain, one needs to define the transition matrix between any two states.
%%U
In our case, each state is an assignment $\xx$, and the transition matrix describes the probability to move from state $\xx$ to state $\xx'$.
Hence, the matrix rows should sum to one $\sum_{\xx'} T_{\xx,\xx'} = 1$, and the matrix dimensions are $T \in [0,1]^{|\cX|,|\cX|}$.
%%
%The matrix rows should sum to one $\sum_{\xx'} T_{\xx,\xx'} = 1$, the coordinate $A_{\xx,j}$ defining the probability to move from state $i$ to state $j$.
%In MCMC, this matrix of dimensions is $T \in [0,1]^{|\cX|,|\cX|}$, meaning that each state is a full assignment of the model variables. 
The stationary distribution $\piv$ is a distribution over the states (over different assignments) that satisfies $\piv=\piv T$.
In MCMC methods, this goal is to define $T $ such that the stationary distribution is $P(\xx)$ - the distribution we wish to sample for.
When this is the case, sampling from $T$ for sufficiently long times will yield samples from $P(\xx)$.

The Metropolis-Hastings algorithm is an example of how the above Markov chain can be constructed. Assume we have 
some conditional distribution $Q(\xx|\xx')$ from which it is easy to sample. Consider the transition matrix is defined as follows:
\be
T_{\xx,\xx'} = \left\{
\begin{array}{lr}
Q(\xx'|\xx)\min\{1,\frac{Q(\xx|\xx')P(\xx')}{Q(\xx'|\xx)P(\xx)}\} & \xx' \neq \xx\\
1 - \sum_{\hat{\xx} \neq \xx} Q(\hat{\xx}|\xx)\min\{1,\frac{Q(\xx|\hat{\xx})P(\hat{\xx})}{Q(\hat{\xx}|\xx)P(\xx)}\} & \text{else}
\end{array} \right.
\ee
%Note that $Q(\xx|\xx')$ must only be irreducible
%%U
%\red{
%\footnote{Irreducible defined: for all $\xx,\xx_0 \in \cX$ there exists a $k \in \naturalNumbers$ and $\xx^1, \ldots, \xx^k=\xx$, such that $\prod_{i=1}^k Q(\xx_i|\xx_{i-1}) >0$ - There is a positive probability to move from every $\xx_0$ to any $\xx$ in finite number of steps}.
%It does not necessarily have to be symmetric; hence, it could be the distribution over a tree or any easy-to-sample distribution.
Sampling from the above Markov chain is easy. Given that we are in state $\xx$, a candidate next state $\xx'$ is chosen by the distribution $Q(\xx' |\xx)$, and is accepted with probability  $\frac{Q(\xx|\xx')P(\xx')}{Q(\xx'|\xx)P(\xx)}$. The key observation is that the rejection probability only depends on $\frac{P(\xx)}{P(\xx')}$, and therefore the partition function cancels out. This ratio is indeed easy to calculate for pairwise MRFs as we consider here. 

Glauber chains, or Gibbs sampler, is another method to define the Markov transition matrix.
The idea is simple, in each iteration only a small number of variables will change, this group denoted by $\cI \subseteq [1,\ldots,\nvars]$, $|\cI| = k$.
Then, the possible next states are the ones that differ from the current state $\xx$ only in the indices in $\cI$, this set denoted by $\mathcal{D}(\xx,\cI) = \left\{\xx' \in \cX | \forall i \notin \cI,\ \xx_i = \xx'_i \right\}$.
The set $\mathcal{D}^k(\xx)= \cup_{\cI, |\cI|=k} \mathcal{D}(\xx,\cI)$ denotes the set of all $\xx'$ that differ from $\xx$ only in $k$ entries.
Now the transition matrix is defined as,
\be
T_{\xx,\xx'} = \left\{
\begin{array}{lr}
0 & \xx' \notin \mathcal{D}^k(\xx)\\
\frac{(\nvars-k)!k!}{\nvars!}\frac{P(\xx')}{\sum_{\hat{\xx} \in \mathcal{D}(\xx,\cI)}P(\hat{\xx})} & \xx' \in \mathcal{D}^k(\xx), \xx \neq \xx'\\
\frac{(\nvars-k)!k!}{\nvars!}\sum_{\cI, |\cI|=k}\frac{P(\xx)}{\sum_{\hat{\xx} \in \mathcal{D}(\xx,\cI)}P(\hat{\xx})}& \xx = \xx'
\end{array} \right.
\ee
Note that as in the case above, only the ratio of $P(\xx)$ needs to be computed, hence the partition function is canceled.

The remaining question is, how fast are we guaranteed to sample from the stationary distribution when starting from an arbitrary assignment $\xx$.
This constant is called the \textit{mixing time}.
The mixing time is defined as the maximum number of moves (transitions between states) necessary until the visited states distribution is $\epsilon$ close to the stationary distribution while starting from any state.
\be
\tau_{\epsilon}  = \min_{\tau \in \naturalNumbers}\sup_{\vv \in \Omega} \left\{\tau | d\left(\vv T^{\tau},P(\xx)\right) < \epsilon\right\}
\ee
where $\Omega = \{ \vv \in [0,1]^{|\cX|} | \sum_{\xx \in \cX} \vv_{\xx} = 1\}$ is the group of all distributions over $\xx$ (All $\vv$ such that each possible $\xx \in \cX$ has a probability $\vv_{\xx} \in [0,1]$, with the constrain that $\sum_{\xx \in \cX} \vv_{\xx} = 1$), and $d(\vv,\vv')$ is any distance function between probabilities.\footnote{ From the above condition it is easy to see the connection to the spectral gap (the distance between the largest to the second eigenvalue) and the mixing time. For more information see\cite{levin2009markov}}  .
The mixing time depends on the model features, such as interaction strength, dependency structure etc.
Hence, models with a small mixing time can be easily sampled, and inferring the marginals is doable.
This fact will be important when talking about inferning \secref{sec:inferning}.

The next section covers learning graphical models.

