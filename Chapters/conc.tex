% !TEX root =../main.tex
\chapter{Conclusion and Discussion } % Main chapter title
\label{con} % For referencing the chapter elsewhere, use \ref{con} 
Learning using an approximate inference try to cover two aspect of the learning process.
The first, in the process of learning one use an approximation inference.
The second after learning the model will be used with approximate inference and the quality of learning will be with regard to the quality of the inference.
The first paper \secref{sec:lwbethe} we were clarifying the meaning of using BP as the approximate inference in the learning process.
It turn out that the optimized objective is the Bethe ML \eqref{eq:bethe_like} and not ML \eqref{eq:ML}.
We showed that although Bethe ML is a convex function (but not smooth) learning must be done with care.

More specifically, we define a set of marginals that could be learned by Bethe ML.
Moreover, for empirical marginal in this set, we showed that the canonical parameters \eqref{eq:canonical}  maximize the Bethe ML - no gradient descent is needed.
While for marginals outside this set, there are no parameter that will guarantee moment-matching \eqref{eq:moment_matching}  which translate parameters with debatable use.
We that gave an inner and outer bound for this set.
The existence of outer bounds inside the marginal polytope prove the existence of marginals that will never be a sole fix of BP (or stable fix point at all).

The second paper address the second aspect, how one should learn when knowing the approximate inference that will be used on the model.
%Remembering the fact that the quality of the learning process is asses by the quality of the results of the approximate inference and not by how close the model is to the real one.
The quality of the learning process will be measured by the quality of the result of the approximate algorithm. 
Hence the inference quality needed to be assured  - the learned  model must belong to a family of models where the inference is known to give good approximation.
In \secref{sec:lg} we present an algorithm that guarantee to return a model in a model family that BP return good approximations, the family of High Girth Correlation Decay (HGCD).
HGCD models are models that for every vertex its surrounding is a tree, where a vertex belongs to other surrounding if a change in its assignment effect it marginals in non-negligible way.

In the paper we present an algorithm to learn the structure of the model.
The algorithm is a natural extension to \cite{chowLiu} where we replace the tree constrain with the constrain of the girth\footnote{The girth of a graph is the length of its shortest cycle. The girth of a tree is infinite.}.
Parameters are the canonical parameters but to insure correlation decay the input marginals strength may be needed to be clamp down\footnote{We know how to do it only in the binary pairwise case. In the general case I am not aware on a good way to measure the interaction strength between the variables.}.
We prove that if the sample are generated from a model from this family.
Under some conditions, the models can be reconstruct.
More importantly, we showed that if the number of samples are big enough the error of the inference algorithm can be bounded.

In the third paper \secref{sec:impnet} I go in another direction.
We present a kernel that its hypothesis class is combination of deep networks for a given network structure.
This is the first time that a kernel is defined of a finite deep networks.
This came with a price of high sample complexity but the sample complexity of 



 
\ignore{
\section{Results summery}
\subsection{What cannot be learned with Bethe approximations}
A common  practice is to use BP as the approximate inference algorithm when optimizing likelihood by gradient descent .
%In the paper \cite{heinemann2012cannot} we were looking at the implication of the optimization of ML objective while the inference is BP.
As was explain before \secref{sec:Bethe_ML}, it turn out that the optimized objective is the Bethe ML \eqref{eq:bethe_like} and not ML \eqref{eq:ML}.
Bethe ML is a convex function of the optimized parameter $\thetav$, although not a smooth one, so optimization should be feasible.
Writing the equation for the optimality of the parameters \eqref{eq:bethe_opt} gave similar condition to the moment matching equations \eqref{eq:moment_matching}.
There is, however, one big different - were there are more than one pseudo-marginals that maximize the Bethe likelihood there is no guarantees that moment matching exist.

The importance of moment matching can be stressed by loss of information.
Running inference on a model learned by ML result in the empirical marginals the model was learned from. 
Hence in learning no information was lost\footnote{This is only exact with infinite data - where the learned parameters are exact. In the finite case the parameters are inexact and higher moments of the learned model may differ from the empirical ones.}.
On the other hand, where there is no moment matching the empirical marginals can not be inferred from the model.
 This reveal a loss of information by the learning process.
Hence moment matching is a crucial characteristic of learning GM.
But is there such empirical marginals - marginals that for all parameters will never be the only pseudo-marginal that minimize the Bethe free energy?
Can such marginals be characterize?

In the paper \cite{heinemann2012cannot} we give the surprising answer is of yes.
There are marginals inside the marginal polytope(\eqref{eq:margpoly}) that for all parameters they are not the sole minimizer of the Bethe free energy.
This imply, that if the empirical marginals are such, the parameters that maximize the Bethe likelihood can not reconstruct the empirical marginals - no moment matching.
Note, that in this case optimizing by gradient descent will have difficulty to converge since BP may return marginals that are far a way from each other in each iteration.
This encourage the definition of a new set of marginals - Bethe learn-able.
A marginal is Bethe learn-able if there exist a parameter that this marginal is the only pseudo-marginal that minimize the Bethe free energy for this parameter.
I our paper bounds to this set is given.

Lets start from inner bounds.
As mentioned above, convergence of BP is a topic of active research moreover the found conditions not only guarantee convergence but convergence to a sole marginals.
This is very close to what is needed for inner bound, but the conditions are the other way around - they constrains the \textit{parameters} such a sole minimizer is guaranteed.
We close this gap using the canonical parameters \eqref{eq:canonical} which can be thought as a map between empirical marginals to parameters.
Using the fact that the marginals are always a fix point of BP, not necessarily stable, for their canonical marginals.
The inner bound is define as all the marginals that their canonical parameters satisfy one of the convergence conditions.

Discovery of the outer bounds is more of a challenge since their very existence being proof to the existence of the unbelievable marginals \cite{pitkow2011learning}\footnote{This paper was published after to publication of our paper.}.
The canonical parameters can be used to define such bounds as well.
Any marginals that running BP on its canonical parameters do not always return the same marginals are not Bethe learn-able.
This indeed define an outer bound but is such marginals exists? can an explicit bound be found.
In the paper we define a closed form bounds using the Hessian of the Bethe likelihood.
Close look at the Hessian will reveal that it is a function of the marginals a lone.
Hence if the Hessian of the marginals is not negative semi-definite these marginals are not Bethe learn-able.
This, though a clear function of the marginals, is not closed form equation.
For this end, we restrict our model to be binary and homogeneous. 
A closed form equation is given that give a condition for the pairwise marginals given the singleton marginals and the number of edges in the graph.
Note that this is the first proof (to my knowledge) that there are marginals that will never be a stable point of BP - which mean, in some way, that BP is biased.

To conclude, we defined a set of marginals that could be learned with Bethe ML and achieve moment matching.
We gave inner and outer bounds for this set and prove that there are marginals outside this set.
Note that for marginals in this set the canonical marginals are optimal.  

\subsection{Inferning with high girth graphical models}
Learning models when our goal is to use the model for inference is different than trying to reconstruct the exact model.
The quality of the learning process is asses by the quality of the inference and not by how close the model is to the real one.
To this end, the inference quality needed to be assured  - the learned  model must belong to a family of models where the inference is known to give good approximation.
Trees is such a family - and indeed \cite{chowLiu} give present an algorithm learned the tree models where the parameters are the canonical parameters \eqref{eq:canonical}
But tree are very limited family of models, a broader family is the locally tree or High Girth Correlation Decay (HGCD) models where inference by BP is guaranteed to give good approximation.
HGCD models are models that for every vertex its surrounding is a tree, where a vertex belongs to other surrounding if a change in its assignment effect it marginals in non-negligible way.
In \cite{heinemann2014inferning} we present an algorithm that guarantee to return a model in HGCD.

In the paper we present an algorithm to learn the structure of the model.
The algorithm is a natural extension to \cite{chowLiu} where we replace the tree constrain with the constrain of the girth\footnote{The girth of a graph is the length of its shortest cycle. The girth of a tree is infinite.}.
Parameters are the canonical parameters but to insure correlation decay the input marginals strength may be needed to clamp down\footnote{We know how to do it only in the binary pairwise case. In the general case I am not aware on a good way to measure the interaction strength between the variables.}.
We prove that if the sample are generated from a model from this family.
Under some conditions, the models can be reconstruct.
More importantly, we showed that if the number of samples are big enough the error of the inference algorithm can be bounded.
}
