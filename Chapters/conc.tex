% !TEX root =../main.tex
\chapter{Conclusion and Discussion } % Main chapter title
\label{con} % For referencing the chapter elsewhere, use \ref{con} 
The name ``learning using an approximate inference''  try to cover two aspect of the learning process.
The first, in the process of learning one use an approximation inference.
The second after learning the model will be used with approximate inference and the quality of learning will be measured by the quality of the inference.
The first paper \secref{sec:lwbethe} clarify the implication of using BP as the approximate inference in the learning process.
It turn out that the optimized objective is the Bethe ML \eqref{eq:bethe_like} and not ML \eqref{eq:ML}.
We showed that although Bethe ML is a convex function (but not smooth) learning must be done with care.

More specifically, we define a set of marginals that could safely be learned by Bethe ML.
Moreover, for empirical marginal in this set, we showed that the canonical parameters \eqref{eq:canonical}  maximize the Bethe ML - no gradient descent is needed.
While for marginals outside this set, there are no parameter that will guarantee moment-matching \eqref{eq:moment_matching}  which translate to parameters with debatable use.
We gave an inner and outer bound for this set.
The existence of outer bounds to this set which cross the marginal polytope prove the existence of marginals that will never be a sole fix point of BP (or even a stable fix point at all).

The second paper address the second aspect, how should one learn when knowing that the model will be used for inference by a known approximate inference.
%Remembering the fact that the quality of the learning process is asses by the quality of the results of the approximate inference and not by how close the model is to the real one.
%In cases where the model will be used for inference after learning the quality of the learning process should be measured by the quality of the result of the approximate inference algorithm.
In such cases the quality of the learning process will be measured by the quality of the results of the approximate algorithm. 
Hence inference quality needed to be assured - the learned  model must belong to a family of models where the inference is known to give good approximation.
In \secref{sec:lg} we present an algorithm that guarantee to return a model that belong to the family of High Girth Correlation Decay (HGCD)  a model family that BP is guaranteed to return good approximations .
HGCD models are models that for every vertex its surrounding is a tree, where a vertex belongs to other vertex surrounding if a change in its assignment effect the other vertex marginals in non-negligible way.

In the paper we present an algorithm to learn the structure and parameters of the model.
The structure learning algorithm is a natural extension to \cite{chowLiu} where we replace the tree constrain with the constrain on the girth\footnote{The girth of a graph is the length of its shortest cycle. The girth of a tree is infinite.}.
Parameters are the canonical parameters but to insure correlation decay the empirical marginals strength may be necessary to clam them down\footnote{We know how to do it only in the binary pairwise case. In the general case I am not aware of a good way to measure the interaction strength between the variables.}.
We prove that if the sample are generated from a model from this family, under some conditions, the models can be reconstruct.
More importantly, we showed that if the number of samples are big enough the error of the inference algorithm can be bounded.

In the third paper \secref{sec:impnet} we present a kernel that its hypothesis class is a combination of deep networks with a given network structure. This work follow the work of \cite{cho2009kernel} but while their kernel relate to inifinite networks ours realte to finite number of nodes.

\subsection{Future Work}
I will now suggest interesting direction for future work.

\claimref{thm:bp_bethe} give meaning to the fix point of BP - they are the pseudo-marginals that minimize the Bethe free energy.  
The variational point of view shows that the argument that minimize the free energy are indeed the marginal of the model.
From here it is concluded that the pseudo-marginals that minimize the Bethe free energy are approximations to the true marginal.
This conclution is not clear in case of multiple minimum - the resulted marginals sould not be thought as approximation of the true marginals (they are usually very bad ones)
So how can the marginals can be estimated in these cases?
A simple way to calculate a single marginal is to note that $\mu_i^{\thetav}(x_i) = \frac{Z(x_i;\thetav)}{\sum_{\hat{x}_i}Z(\hat{x}_i;\thetav)}$  where $Z(x_i;\thetav)$% = \sum_{\hat{\xx}, \hat{\xx}_i = x_i}  
is the partition function where the $i$ variable is set to $x_i$.
Now, the approximation will be $\tau_i^{\thetav} = \frac{\exp{A_B(x_i;\thetav)}}{\sum_{\hat{x}_i}\exp{A_B(\hat{x}_i;\thetav)}}$.
The problem with this approximation is that for each variable assignment one should run BP.
But can the previous runs be used to accelerate BP convergence so BP will run for only few iterations.
Is this method could be effieceint enough for use.

The hardness results of learning and infernce of GM imply that maybe a more direct approch is in order.
The desired model should be able to answer different queries, for structure output.
The richness of models with correlation decay suggest that even when only a small number of variabels effect  each other reach models can be desgin.
Another related fact is that in many cases running BP for only small number of iterations results in good approximation to the marginals.
Can a model be defined that mimic the behavior of running BP of small number of iteration.
One way to implemnte it is by a model where for each variable build its computation tree of BP of depth $l$.
While the parameters of between any two variables are tied through out the trees.
The inference will be defined as recalculating the computation tree for any assignment (when a variable is observed all its instances should be assigned).


Pseudo-Likelihood (PL) has many appealing features.
Learning is feasible, though it is less sample efficient ML it is still consist.
This work argue, that the inference algorithm that will be used on the learned model should  be take into account when learning.
Can an algorithm be found that insure PL inference quality.
Such algorithm is unlikely to be found since PL is consist.
So, can an efficient algorithm be found that will guarantee moment matching.
Note that a moment in PL mean not only the factor itself but it include its neighbourhood as well.

PL is an example of a larger set of methods composite likelihood.
While PL break down the probability to each vertex and its neighbours composite breaking it down to larger sets.
As long as each set is easy to calculation the time complexity does not change much - tough sample complexity depended on the larger set and its neighbours.
An interesting direction will be to optimize, during learning, how the model should be break down - what are the best sets.

Using approximate infernce in learning is a practice that will not stop in the near future.
Hence, its implication needed a farther research for both aspects: using the approximate algorithm in the learning process and the use of the model will be with specific approximate algorithm.
I hope that when GM will come back to fashion these aspect will not be over looked.

\ignore{
The research of structure learning algorithm is now at a turning point.
In the general case the time complexity lower bounds and known algorithm are very close but both impractical (exponential in the maximum degree).
So, in my opinion, the interesting direction is to defined a class that structure can be learned efficiently.
The challenge is to go beyond the pairwise case finding other criteria that will allow structure learning.

\section{Results summery}
\subsection{What cannot be learned with Bethe approximations}
A common  practice is to use BP as the approximate inference algorithm when optimizing likelihood by gradient descent .
%In the paper \cite{heinemann2012cannot} we were looking at the implication of the optimization of ML objective while the inference is BP.
As was explain before \secref{sec:Bethe_ML}, it turn out that the optimized objective is the Bethe ML \eqref{eq:bethe_like} and not ML \eqref{eq:ML}.
Bethe ML is a convex function of the optimized parameter $\thetav$, although not a smooth one, so optimization should be feasible.
Writing the equation for the optimality of the parameters \eqref{eq:bethe_opt} gave similar condition to the moment matching equations \eqref{eq:moment_matching}.
There is, however, one big different - were there are more than one pseudo-marginals that maximize the Bethe likelihood there is no guarantees that moment matching exist.

The importance of moment matching can be stressed by loss of information.
Running inference on a model learned by ML result in the empirical marginals the model was learned from. 
Hence in learning no information was lost\footnote{This is only exact with infinite data - where the learned parameters are exact. In the finite case the parameters are inexact and higher moments of the learned model may differ from the empirical ones.}.
On the other hand, where there is no moment matching the empirical marginals can not be inferred from the model.
 This reveal a loss of information by the learning process.
Hence moment matching is a crucial characteristic of learning GM.
But is there such empirical marginals - marginals that for all parameters will never be the only pseudo-marginal that minimize the Bethe free energy?
Can such marginals be characterize?

In the paper \cite{heinemann2012cannot} we give the surprising answer is of yes.
There are marginals inside the marginal polytope(\eqref{eq:margpoly}) that for all parameters they are not the sole minimizer of the Bethe free energy.
This imply, that if the empirical marginals are such, the parameters that maximize the Bethe likelihood can not reconstruct the empirical marginals - no moment matching.
Note, that in this case optimizing by gradient descent will have difficulty to converge since BP may return marginals that are far a way from each other in each iteration.
This encourage the definition of a new set of marginals - Bethe learn-able.
A marginal is Bethe learn-able if there exist a parameter that this marginal is the only pseudo-marginal that minimize the Bethe free energy for this parameter.
I our paper bounds to this set is given.

Lets start from inner bounds.
As mentioned above, convergence of BP is a topic of active research moreover the found conditions not only guarantee convergence but convergence to a sole marginals.
This is very close to what is needed for inner bound, but the conditions are the other way around - they constrains the \textit{parameters} such a sole minimizer is guaranteed.
We close this gap using the canonical parameters \eqref{eq:canonical} which can be thought as a map between empirical marginals to parameters.
Using the fact that the marginals are always a fix point of BP, not necessarily stable, for their canonical marginals.
The inner bound is define as all the marginals that their canonical parameters satisfy one of the convergence conditions.

Discovery of the outer bounds is more of a challenge since their very existence being proof to the existence of the unbelievable marginals \cite{pitkow2011learning}\footnote{This paper was published after to publication of our paper.}.
The canonical parameters can be used to define such bounds as well.
Any marginals that running BP on its canonical parameters do not always return the same marginals are not Bethe learn-able.
This indeed define an outer bound but is such marginals exists? can an explicit bound be found.
In the paper we define a closed form bounds using the Hessian of the Bethe likelihood.
Close look at the Hessian will reveal that it is a function of the marginals a lone.
Hence if the Hessian of the marginals is not negative semi-definite these marginals are not Bethe learn-able.
This, though a clear function of the marginals, is not closed form equation.
For this end, we restrict our model to be binary and homogeneous. 
A closed form equation is given that give a condition for the pairwise marginals given the singleton marginals and the number of edges in the graph.
Note that this is the first proof (to my knowledge) that there are marginals that will never be a stable point of BP - which mean, in some way, that BP is biased.

To conclude, we defined a set of marginals that could be learned with Bethe ML and achieve moment matching.
We gave inner and outer bounds for this set and prove that there are marginals outside this set.
Note that for marginals in this set the canonical marginals are optimal.  

\subsection{Inferning with high girth graphical models}
Learning models when our goal is to use the model for inference is different than trying to reconstruct the exact model.
The quality of the learning process is asses by the quality of the inference and not by how close the model is to the real one.
To this end, the inference quality needed to be assured  - the learned  model must belong to a family of models where the inference is known to give good approximation.
Trees is such a family - and indeed \cite{chowLiu} give present an algorithm learned the tree models where the parameters are the canonical parameters \eqref{eq:canonical}
But tree are very limited family of models, a broader family is the locally tree or High Girth Correlation Decay (HGCD) models where inference by BP is guaranteed to give good approximation.
HGCD models are models that for every vertex its surrounding is a tree, where a vertex belongs to other surrounding if a change in its assignment effect it marginals in non-negligible way.
In \cite{heinemann2014inferning} we present an algorithm that guarantee to return a model in HGCD.

In the paper we present an algorithm to learn the structure of the model.
The algorithm is a natural extension to \cite{chowLiu} where we replace the tree constrain with the constrain of the girth\footnote{The girth of a graph is the length of its shortest cycle. The girth of a tree is infinite.}.
Parameters are the canonical parameters but to insure correlation decay the input marginals strength may be needed to clamp down\footnote{We know how to do it only in the binary pairwise case. In the general case I am not aware on a good way to measure the interaction strength between the variables.}.
We prove that if the sample are generated from a model from this family.
Under some conditions, the models can be reconstruct.
More importantly, we showed that if the number of samples are big enough the error of the inference algorithm can be bounded.
}
