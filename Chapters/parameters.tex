\subsection{Parameter Estimation}
%As in the previous section, I will start with the problem definition. 
In the previous section we considered the problem of learning the MRF graph structure. We now turn to the problem of learning the model parameters $\thetav$, assuming the graph $G$ is known. This task is known as parameter learning or parameter estimation.

In what follows we will sometime assume that there is a {\em true} underlying MRF generating the training data, and denote its parameter by $\exact{\thetav}$ and the corresponding MRF by $P(\xx; \exact{\thetav})$. As mentioned earlier, it is usually not a good idea to try and recover $\exact{\thetav}$, since multiple $\thetav$ vectors may correspond to the same distribution (in other words, the parameters are not identifiable). A better-defined goal is to find {\em any} parameter $\thetav$ such that $P(\xx; \exact{\thetav})$ is as close as possible to $P(\xx; \exact{\thetav})$. This of course cannot be achieved in general, since we do not know $\exact{\thetav}$,  but rather have IID samples from it. Furthermore, there are difficulties arising from hardness of inference, which we discuss later.

We begin by reviewing the maximum likelihood method, which offers a natural approach to parameter estimation from samples.


% given a set of i.i.d observations, sampled from $P(\xx;\exact{\thetav})$, find $\thetav$ such that for all input $\forall \xx \in \cX^{p}$ the two return the same (or as close as possible) probability $P(\xx;\thetav) = P(\xx;\exact{\thetav})$.
%This, however, is not practical.
%First, $P(\xx;\exact{\thetav})$ is not given
%Moreover, not all realization of $\xx$ could be expected to be estimated by the sample - and to even check this requirement is exponential to the size of $\xx$.
%So another objective, or another way to achieve this goal, is required.


\subsubsection{Maximum Likelihood}
\label{sec:max_likelihood}
Maximum Likelihood (ML) has a simple and intuitive goal. Given a IID training sample  $X^\nsamples = [\xx^1, \ldots, \xx^{\nsamples}]$, find the parameter $\thetav$ under which 
this sample is the most likely. 


%In ML,(e.g., see \atodo{cite}) one is looking for the parameters that will maximize the probability to sample the observed data-set.
%Note, that simply giving probability of $\frac{1}{N}$ to each of the samples is the maximum likelihood solution, but it suffers from extreme overfitting.
%Therefore some regularization is needed, and in graphical models this is usually done by forcing a certain structure (thereby forcing independence assumptions) and a bound on the size of the factors.
%I will now define the maximum likelihood objective and then discuss its merits and drawbacks.

To present the maximum likelihood objective, we consider the log of the likelihood and normalize it by the sample size $\nsamples$.\footnote{This does not change the parameters that maximize the function, since $\arg \max_x\frac{1}{\nsamples}\log(f(x)) = \arg\max_x f(x)$} The ML objective for the model $P(\xx;\thetav)$ in \eqref{eq:basic_model} turns out to have a very simple form: 
\bean
\likelihood{X^\nsamples;\thetav} &=& \frac{1}{\nsamples}\log\left(\prod_{n=1}^\nsamples P(\xx^n;\thetav)\right) \nonumber \\
&=& \frac{1}{\nsamples}\sum_{n=1}^\nsamples\log\left(P(\xx^n;\thetav)\right)\nonumber\\
&=& \frac{1}{\nsamples}\sum_{n=1}^\nsamples\log\left(\frac{1}{Z(\thetav)} \exp{\sum_{ij \in E} \theta_{ij}(x^n_i,x^n_j) + \sum_{n \in V} \theta_i(x^n_i)}\right)\nonumber\\
&=& \frac{1}{\nsamples}\sum_{n=1}^\nsamples \sum_{ij \in E} \theta_{ij}(x^n_i,x^n_j) + \sum_{i \in V} \theta_i(x^n_i) - A(\thetav)\nonumber\\
&=& \sum_{ij \in E} \sum_{x_i,x_j}\mub_{ij}(x_i,x_j)\theta_{ij}(x_i,x_j) + \sum_{i \in V} \sum_{x_i}\mub_i(x_i)\theta_i(x_i) -A(\thetav)\nonumber\\
&=&  \mubv \cdot \thetav - A(\thetav) \label{eq:ML}
\eean
Here $\mubv$ are the empirical pairwise and singleton marginals, ordered in the same way as $\thetav$.
\be
\label{eq:empirical_mar}
\mub_i(x_i) = \frac{1}{\nsamples}\sum_{n=1}^{\nsamples} \deltaF{x_i=x_i^n },\ \
\mub_{ij}(x_i,x_j)  = \frac{1}{\nsamples}\sum_{n=1}^{\nsamples} \deltaF{x_i=x_i^n \land  x_j=x_j^n}
\ee
Note the close similarity between \eqref{eq:ML} and \eqref{eq:conjugate_partition}.


Since the $\thetav$ that maximizes $\ell(\thetav)$ is sought,
we set the gradient to zero to characterize the optimal parameters.
\bean
\label{eq:ml_derv_single}
\frac{ \partial \likelihood{X;\thetav}}{\partial \theta_{i}(x_i)} &= \mub_{i}(x_i) - \frac{ \partial \log(Z(\thetav))}{\partial \theta_{i}(x_i)} =& \mub_{i}(x_i) - p(x_i;\thetav) \\
\label{eq:ml_derv_pairs}
\frac{ \partial \likelihood{X;\thetav}}{\partial \theta_{ij}(x_i,x_j)} &= \mub_{ij}(x_i,x_j) - \frac{ \partial \log(Z(\thetav))}{\partial \theta_{ij}(x_i,x_j)} =& \mub_{ij}(x_i,x_j) - p(x_i,x_j;\thetav)
\eean
where we used the expression for the gradient of $A(\thetav)$ in \eqref{eq:pratition_derivative}.
Setting the gradient to zero, we arrive at the following simple characterization, known as moment matching:
\be
\label{eq:moment_matching}
\mub_i(x_i) = P(x_i;\thetav), \ \ \ \mub_{ij}(x_i,x_j) = P(x_i,x_j;\thetav)
\ee
In other words, the marginals of the learned model should be identical
to the empirical ones. This condition is very intuitive, and can be viewed
as a constraint on the learned distribution being able to re-generate the empirical marginals.

The ML method has other desirable qualities:\footnote{These hold under fairly general regularity conditions.}
\begin{itemize}
\item Consistency: When $\nsamples \to \infty$, the learned parameters $\thetav^\nsamples$ satisfy $P(\xx;\theta^M) =  P(\xx;\exact{\thetav})$
  %the exact parameters $\lim_{\nsamples \to \infty} \arg \max_{\thetav} p(X^\nsamples;\theta) =  \exact{\thetav}$ will be recovered
  %%U
%  \footnote{This does not contradict the above discussion - part of the conditions is uniqueness $\exact{\thetav}$. But this condition mat be dropped. Denote the maximum likelihood parameter by $\thetabv(\nsamples)$. Consistency could be define by $\lim_{\nsamples \to \infty} \expect{\exact{\thetav}}{\frac{P(\xx;\thetabv(M))}{P(\xx;\exact{\thetav})}} = 0$ with no uniqueness condition}.\todo{You can express consistency in terms of distribution and then there's no issue of uniqueness.}
  %%
\item Efficiency \cite{cramer2016mathematical}: At the limit  $\nsamples \to \infty$, no other consistent estimator has a lower mean squared error than ML. 
\item Asymptotic Normality: At the limit  $\nsamples \to \infty$, the ML estimator will have a Gaussian distribution.
\end{itemize}

Despite these desirable facts, it should be noted that ML does not dominate all other estimators for finite data.
For estimating MRFs, ML suffers from several \ignore{additional} drawbacks. The first and most important is that finding the ML estimator is computationally hard. This is intuitively sensible since the ML objective and gradient involve partition functions and marginals, both of which are hard. This fact on its own does not rule out the existence of an efficient algorithm that sidesteps partitions and marginals.
However, the intuition of hardness is indeed confirmed by recent results from \cite{bresler2014hardness} and \cite{montanari2015computational}.
%%U
They show that by using only the pairwise empirical marginals corresponding to the MRF (and not the whole data) no algorithm can efficiently learn the model parameters unless  $NP = RP$.\footnote{RP - randomized polynomial, an polynomial algorithm that use randomness}
%if $NP \neq RP$ then no algorithm using only the model marginals (marginals corresponding to the model parameters) can learn the model parameters.
Hence, if a time- efficient algorithm that finds the ML parameter is to be found, it must use marginals with higher cardinality.
For example, PL uses marginals with cardinalities that are higher than the model factors.
%\todo{Also not clear.}
%}%%
Thus, exact ML estimation is impossible for general MRFs.
It is natural to turn to approximate inference methods to obtain an approximate ML estimator.
In the next section we describe such an approach, focusing on BP as the approximate inference method.

\subsubsection{Maximum Likelihood with BP as the approximate inference}
\label{sec:Bethe_ML}
%% Optimizing ML is usually achieved by gradient descent.
%% In order to calculate the derivative, the marginals of the current model are needed.
%% BP is a successful algorithm for finding the marginals,
%% so using it to find the marginals seems like a good choice.
%% But what does it optimize exactly?
The likelihood function $\likelihood{X;\thetav}$ in \eqref{eq:ML} is hard to evaluate due to the the log partition function $A(\thetav)$, and its gradient is hard to calculate since it involves marginals.
Both these difficulties can be avoided if one replaces the $A(\thetav)$ with its Bethe approximation $A_B(\thetav)$ in \eqref{eq:bethe_approximation}.
%% Using the variation stand point, \eqref{eq:ML} can be written as
%% \be
%% \sup_{\thetav \in \Omega^G} \likelihood{X;\thetav} = \sup_{\thetav \in \Omega^G}\left\{ \mubv \cdot \thetav - \sup_{\muv \in \margpoly} \left\{\muv \cdot \thetav - A^*(\muv)\right\}\right\}
%% \ee
%Since BP is used, and remembering the result of \cite{yedidia2000generalized}(\claimref{thm:bp_bethe}), the optimizing ML with BP can be written as ,
The resulting objective, which we call the Bethe likelihood, is:
\be
\label{eq:bethe_like}
\bethelikelihood{X;\thetav}  = \sup_{\thetav \in \Omega^G} \left\{\mubv \cdot \thetav - \sup_{\tauv \in \lclmargpoly} \left\{\tauv \cdot \thetav + H_B(\tauv)\right\}\right\}
\ee
The function $\bethelikelihood{X;\thetav}$ is a convex function in $\thetav$, since $A_B(\thetav)$ is a convex function of $\thetav$ (note that it is indeed convex, but not smooth). 
Moreover, it can be shown that $\bethelikelihood{X; \thetav}$ is \textbf{strictly} convex.

When the graph is a tree, the Bethe likelihood is exact, and in fact the optimal parameters can be found in closed form.
These are sometimes known as canonical parameters, and are a simple function of the empirical marginals:
%To summarize, optimizing ML by using BP as the inference algorithm in gradient descent, will in fact optimize the Bethe likelihood and not the exact one.

%Bethe and BP are exact for models with a tree dependency graph, and structure learning is easy for such models.
%It is clear that optimizing ML is feasible, but cannot better be done?  
%The canonical parameters are a closed form set of equations, that learn the exact parameters for tree models.
They are defined as,
\be
\label{eq:canonical}
\theta_{ij}(x_i,x_j) = \log{\frac{\mub_{ij}(x_i,x_j)}{\mub_i(x_i)\mub_j(x_j)}}, \ \ \ \theta_i(x_i) = \log{\mub_i(x_i)}
\ee
%So if the model graph is a tree, then the canonical parameters could be used, and no gradient descent is necessary.

When the graph is not a tree, a natural approach is to perform gradient descent on the Bethe likelihood.
The gradient of $\bethelikelihood{X;\thetav}$ turns out to be the empirical marginals minus the BP marginals, where the  latter are the argmax of the optimization problem  \eqref{eq:bethe_approximation}.
However, as mentioned earlier, these are also hard to calculate exactly, due to non-convexity of Bethe free energy.
A compromise is to run BP on $\thetav$ and (assuming convergence) using the resulting marginals in the gradient.
This is indeed a common heuristic, but not much can be guaranteed about its results.

The Bethe ML approximation is also implicit in other approximate learning methods.
In \cite{ganapathi2012constrained}, for example, ML is optimized via its dual - maximizing the entropy with moment matching constraints.
Since this problem is hard, the problem is relaxed in the same way as the Bethe - using the local marginal polytope instead of the marginal polytope, and using the Bethe entropy instead of the exact one.\footnote{Note that if no other constraints are found, the moment matching condition in the Bethe entropy maximization fully specifies the solution}
%so, in fact, trying to optimize an objective that is similar to the Bethe likelihood 
Then, to optimize the relaxed objective, the Bethe marginals must be found - which is done using the CCCP method of \cite{yuille2002cccp}.


In \cite{heinemann2012cannot} we ask a basic question regarding the above approximation. Assume that BP marginals {\em can} be calculated exactly. What can one deduce about the resulting ML estimate? We provide a set of rather surprising results that specify when such a procedure will succeed and when it will not.

\ignore{
But let us return to the optimization of the Bethe ML.
The Bethe likelihood $\bethelikelihood{X;\thetav}$ is a non-smooth function, hence the optimal conditions sum down to,
\be
\label{eq:bethe_opt}
\mu_{ij}(x_i,x_j) = \convexhull{ \arg \sup_{\tauv \in \lclmargpoly} \left\{\tauv \cdot \thetav + H_B(\tauv)\right\}}
\ee
where $\convexhull{[\tauv_1,\ldots,\tauv_k]}$ is the convex hull of $[\tauv_1,\ldots,\tauv_k]$.
It is easy to see that if the Bethe energy has only one maximum, the moment matching condition \eqref{eq:moment_matching} is satisfied.
Moreover, running ML with BP will converge to this result.
In contrast, in the case of multiple maximum, no moment matching is guaranteed - the learned parameters will not reconstruct the empirical marginals.
As a result, running ML with BP will converge slowly but will keep ``jumping'' between the different maximums. 
%As a result, running ML with BP will keep ``jumping'' between the different maximums, and converge only extremely slowly. 
A question remains, do such empirical marginals exist?

In the paper \cite{heinemann2012cannot} we prove that such empirical marginals do exist, and show that there are indeed marginals in the marginal polytope that will \textbf{never} (for any parameter) be the result of BP.
Note that this result also has relevance to the paper of \cite{ganapathi2012constrained} - there are marginals for which the results of the algorithms depend on the initialization of the gradient descent.   
}

%We next review another p onsider an approximation to the ML Pseudo-Likelihood.
\subsubsection{Pseudo-Likelihood}
The Pseudo-Likelihood (PL) method, first introduced by \citet{besag1975statistical}, is an elegant parameter estimation method that is both consistent and a computationally- efficient approach. It is the first of a larger family of likelihood approximations called composite likelihood \cite{lindsay1988composite}.

PL begins by noting that the likelihood may be written as $P(\xx;\thetav) = \prod_{i=1}^{\nvars} P(x_i |x_1,\ldots x_{i-1})$.
It then proceeds by conditioning on all variables instead of the first $i-1$ ones. Namely, it considers the following {\em approximate} likelihood:
\be
pl(\xx;\thetav) = \prod_{i=1}^{\nvars} P(x_i |\xx_{\setminus i};\thetav) = \prod_{i=1}^{\nvars} P(x_i |\xx_{\nei{i}};\thetav)
\ee
where the last equality follows from the conditional independence property of the MRF.
\ignore{
Using model \eqref{eq:basic_model}, the probability of a vertex given its neighbors can be written as,
\be
\label{eq:pl_condition}
p(x_k |\xx_{\setminus k};\thetav) 
%&=& \frac{p(\xx;\thetav)}{p(\xx_{\setminus k};\thetav)}\\
%&=& \frac{p(\xx;\thetav)}{\sum_{x_k \in \cX }p(\xx;\thetav)}\\
%&=&\frac{\exp{\sum_{ij \in E} \theta_{ij}(x_i,x_j) + \sum_{ i \in V} \theta_i(x_i)}}{\sum_{ \tilde{x}_{k} \in \cX }\exp{\sum_{ij \in E} \theta_{ij}(x_i,x_j) + \sum_{i \in V} \theta_i(x_i)}}\\
%&=&\frac{\exp{\sum_{j \in \nei{k}} \theta_{kj}(x_k,x_j) + \theta_k(x_k)}}{\sum_{\tilde{x}_k \in \cX}\exp{\sum_{j \in \nei{k}} \theta_{kj}(\tilde{x}_k,x_j) + \theta_k(\tilde{x}_k)}}\\
=\frac{\exp{\sum_{j \in \nei{k}} \theta_{kj}(x_k,x_j) + \theta_k(x_k)}}{\sum_{\tilde{x}_k \in \cX}\exp{\sum_{j \in \nei{k}} \theta_{kj}(\tilde{x}_k,x_j) + \theta_k(\tilde{x}_k)}}\\
\ee
}
Normalizing and taking the log, the optimization of PL can be written as,
\bean
\label{eq:pl_maximization}
\max_{\thetav} \frac{1}{\nsamples} \log{\left(\prod_{i=1}^\nsamples pl(\xx^i;\thetav)\right)}
%&&\frac{1}{N} \log \left(\prod_{n=1}^N \prod_{i \in V} p(x^n_i | \xx^n_{\nei{i};\thetav})\right) \\ 
%&=&\sum_{i \in V}\frac{1}{N}\sum_{n=1}^N \sum_{j \in \nei{i}} \theta_{ij}(x^n_i,x^n_j) + \theta_i(x^n_i)\\
%&&-\log\left(\sum_{\tilde{x}_i \in \cX}\exp{\sum_{j \in \nei{i}} \theta_{ij}(\tilde{x}_i,x^n_j) + \theta_i(\tilde{x}_i)}\right) \\
&=& \max_{\thetav} \sum_{i \in V} \sum_{j \in \nei{i}}\sum_{x_i,x_j} \mub_{ij}(x_i,x_j)\theta_{ij}(x_i,x_j)\\
&&+ \sum_{x_i}\mub_i(x_i)\theta_i(x_i)\nonumber\\
&&-\sum_{\xx_{\nei{i}}} \mub_{\nei{i}}(\xx_{\nei{i}})\log\left(\sum_{\tilde{x}_i}\exp{\sum_{j \in \nei{i}} \theta_{ij}(\tilde{x}_i,x_j) + \theta_i(\tilde{x}_i)}\right)\nonumber
\eean
And the derivatives \footnote{The pairwise derivatives are similar.} are,
\be
\label{eq:pl_derivative}
%\frac{\partial pl(X^n;\thetav)}{\partial \theta_{ij}(x_i,x_j)} &=& 2 \mub_{ij}(x_i,x_j) \\
%&&- \sum_{\xx_{\nei{i}\setminus j} \in \cX^{|\nei{i}|-1}} \mub_{\nei{i}}(x_j,  \xx_{\nei{i}\setminus j}) p(x_i|x_j, \xx_{\nei{i}\setminus j};\thetav)\\
%&& - \sum_{\xx_{\nei{j}\setminus i} \in \cX^{|\nei{j}|-1}} \mub_{\nei{j}}(x_i,  \xx_{\nei{j}\setminus i}) p(x_j|x_i, \xx_{\nei{i}\setminus j};\thetav)\\
%\frac{\partial pl(X^n;\thetav)}{\partial \theta_{i}(x_i)} &=& 
%\mub_i(x_i) - \sum_{\xx_{\nei{i}} \in \cX^{|\nei{i}|}} \mub_{\nei{i}}(\xx_{\nei{i}}) p(x_i|\xx_{\nei{i}};\thetav)
\frac{\partial pl(X^n;\thetav)}{\partial \theta_{i}(x_i)} =  \mub_i(x_i) - \sum_{\xx_{\nei{i}}} \mub_{\nei{i}}(\xx_{\nei{i}}) p(x_i|\xx_{\nei{i}};\thetav)
\ee

The first thing to note is that \eqref{eq:pl_maximization} is easy to optimize, since both it and its derivative (\eqref{eq:pl_derivative}) involve only conditional probabilities of indvidual variables. Also, the sum over $\xx_{\nei{i}}$ doesn't involve all these assignments,\footnote{which would have been exponential in the degree.} but rather only ones which had non-zero empirical probability, and thus is limited to $\nsamples$ elements. 
%sum over the neighbor variables, not the whole models.
As a consequence, optimizing by gradient descent is plausible.
Second, note that \eqref{eq:pl_derivative} gives us a similar condition to the moment matching condition \eqref{eq:moment_matching} of ML.
The difference arises from the fact that only the conditional probability is estimated, hence the need to sum over all neighbor values.
This gives intuition to why PL is less sample-efficient. Namely, it needs to approximate not only the factor's marginals but marginals of the factor and its neighbors.
%Unfortunately, for each derivative we not only need to a approximate the pairwise marginals but the marginals of the whole neighborhood.
If one considers the result by \cite{bresler2014hardness} and \cite{montanari2015computational}, this is not surprising - any efficient estimation algorithm, is bound to use higher order marginals.

\ignore{
The most important property of PL is that it is consistent \cite{besag1975statistical}. Namely, if data is generated by $P(\xx;\thetav^*)$ for an MRF with the same structure as the learned one, then as $N\to\infty$ PL will find parameters that correspond to the distribution $P(\xx;\thetav^*)$.}

The most important property of PL is that it is in fact consistent. In other words, assuming the true underlyling model is a pairwise MRF with the correct structure, PL will find its true parameters (up to reparameterizatoin) given enough training data. Other characteristics of PL, such as $\thetav$ being asymptotically normally distributed, can be derived from the more general family of composite likelihood, see \cite{varin2011overview}.
\ignore{
Another view point of PL is, that instead of approximating the likelihood, it just preforming ML on the conditional marginals. 
We are looking for the parameters that maximize the conditional probability for each vertex separately.
The derivative of the singleton parameter is the same.
While the pairwise change, so it include only one of the summation - hence a more moment matching like equation.
}

The first reason to use PL and not ML is time complexity.
While PL parameters can easily be computed via gradient descent, no efficient way is known to recover ML parameters.
The question whether there are cases when PL gives better results than ML, remains open.
When the true underlying model has the same structure of the learned MRF, it was shown that ML is more efficient than PL \cite{liang2008asymptotic}.
In cases where the underlying model does not have this structure (i.e., the more realistic case), it is not clear which method is better (see \cite{varin2011overview} for discussion).
An interesting direction for research is to design an inference algorithm tailored for PL.
Namely, design an efficient inference algorithm such that it is guaranteed to return the empirical marginals when the model parameter were learned using PL.
This may have important implications, as discussed in \secref{sec:inferning}.

\subsubsection{Other Methods}
In the brief review above, we covered ML and PL approaches.
There are, of course, many variants of these, as well as methods based on different principles. Of these, a key one is methods based on sampling, which were discussed in the context of approximate inference. A natural sampling-based approach to learning is to estimate marginals via sampling, and then use those in the gradient of the likelihood in \eqref{eq:ml_derv_single}. We briefly mention some other methods below.
 
Contrastive Divergance (CD) \cite{hinton2002training} was designed for models where some of the variables are unobserved, but it could be used on the fully observed case. Recall from \eqref{eq:ml_derv_single} and \eqref{eq:ml_derv_pairs}, that the reason gradient-descent is impractical,  is the hardness of evaluating the marginals according to the parameters $p(x_i,x_j;\thetav)$. 
One way to approximate these marginals is by MCMC (see \secref{sec:sampling}).
Preforming only a small number of steps - usually one - to evaluate $p(x_i,x_j;\thetav)$, results in CD. Justification to this method was given by \cite{bengio2009justifying} and \cite{carreira2005contrastive}, but its success in learning Restricted Boltzmann Machine (RBM) in highlights its importance.

In contrast to the previous methods,  \cite{abbeel2006learning} give a closed form equation for parameter estimation.
Using the ideas from the proof of the Hammersley Clifford theorem,  they estimate the canonical factors, which can be translsated to model parameters.
%%U
More specifically, to simplify presentation I will focus on the pairwise case. Considering a fixed assignment $\tilde{\xx}$, the pairwise canonical factor is :
\bean
\label{eq:HC_param}
\theta^{\dagger}_{ij}(x_i,x_j) &=& \log P(x_i,x_j| \tilde{\xx}_{\nei{i}}\tilde{\xx}_{\nei{j}}) -\log P(x_i,\tilde{x}_j| \tilde{\xx}_{\nei{i}}\tilde{\xx}_{\nei{j}})\nonumber\\
&& - \log P(\tilde{x}_i,x_j| \tilde{\xx}_{\nei{i}}\tilde{\xx}_{\nei{j}})+ \log P(\tilde{x}_i,\tilde{x}_j| \tilde{\xx}_{\nei{i}}\tilde{\xx}_{\nei{j}})
\eean
while $\theta^{\dagger}_i(x_i)$ is defined in the same way. 
As in the Hammersley-Clifford theorem, it can be shown that any positive Gibbs distribution with pairwise interaction can be expressed as $P(\xx;\thetav) = P(\tilde{\xx}) \exp{\sum_{ij} \theta^{\dagger}_{ij}(x_i,x_j) + \sum_{i}\theta^{\dagger}_i(x_i)}$.
Using this fact for learning, the parameters can be estimated by using the empirical distribution in \eqref{eq:HC_param}.
\ignore{
Note the resemblance of  \eqref{eq:HC_param} to the PL equation\gtodo{Which? Give reference.}.
Indeed, in \cite{bradley2012sample} it is proven that the two methods\gtodo{Which two methods} give the same result.
}
\cite{bradley2012sample} provide a comparison of the sample complexity of this method, composite likelihood, and ML.


In another approach to avoiding partition function estimation, \cite{hyvarinen2007some} use the ratio between the probability of two assignments, which eliminates the partition function.
Note that using the ratio is basically similar to using the conditional probability, and indeed \cite{marlin2010inductive} and \cite{marlin2012asymptotic} compare the ratio-based  methods to pseudo-likelihood and explain the differences.
Other methods, like \cite{mizrahi2014icml}, simply disconnect each clique from its neighbors, and learn each factor independently.
%\footnote{It assumes independence of the clique neighbors from their other neighbors. However it is not clear how it effects the sample complexity, and non is given in the paper.\atodo{Not sure what this comment means.}}.

To conclude, ML has many desirable statistical properties, but no time efficient algorithm is known. Moreover, it was proven by \cite{bresler2014hardness} and \cite{montanari2015computational} that under reasonable assumptions, no algorithm can optimize ML efficiently. On the other hand, PL is time efficient but its sample complexity is worse than that of ML. Methods like composite likelihood can give better sample complexity results, while still being time efficient.

\subsection{Inferning}
\label{sec:inferning}
In a world without computation limitation, learning the exact model,
and using exact inference, is always optimal. In our world, however, it
is not always the case.  If the quality of a model is measured by the accuracy
of its inference predictions, then the approximate inference algorithm used
should also be taken into account during learning. This point was formulated
elegantly in \cite{wainwright2006estimating}, where it shown both theoretically and empirically that learning and inference should be matched (e.g., exact learning with approximate inference is worse than approximate learning with approximate inference). Note that in that paper, the structure of the model was given.  An interesting question remains, how learning should be preformed when
the structure is not given, and the goal is preforming inference on
the learned model.
The above interplay between learning and inference has recently been coined the term ``inferning'' \cite{rohanimanesh2009training}

The complexity of inference for a given model is exponential in its tree-width \cite{robertson1983graph,robertson1994quickly}.
Therefore it would be desirable to learn bounded tree-width models.
This, however, turns out to be a hard problem in itself \cite{srebro2001maximum}. There are multiple approximation approaches of this problem. See for example \cite{karger2001learning,bach2001thin,elidan2009learning}.
%

Another class of interesting models is where the inference algorithm is not exact, but still guaranteed to return a good approximation. 
As mentioned earlier (see here \ref{sec:belief} ), running BP on models that are tree-like can be shown to return a good approximation \cite{dembo2010ising}.
Tree-like models are models that are locally trees, but allow long cycles.
Formally, a ``long'' cycle corresponds to negligible correlation between vertices or a vertex and a group. This property is also known as correlation decay (see \secref{sec:structure} for further discussion).
\ignore{ $\epsilon$ influence of the assignment of two vertices on each other (or on a group of vertices to a vertex).}
%In other words, in models where in the influential surrounding are trees, inference is plausible.
In our paper \cite{heinemann2014inferning}, the structure and parameters are learned such that the resulting model has the above tree-like properties.
%

Fast mixing models are another model family,  known to have good approximation inference algorithms.
As explained above in \secref{sec:sampling}, if mixing is rapid, then sampling from the distribution is feasible, resulting in a theoretical guarantee for approximate inference. 
In the paper \cite{domke2015maximum}, a learning algorithm is presented, that guarantees returning a model which belongs to the fast mixing model family,
thereby guaranteeing the quality of inference approximations.
