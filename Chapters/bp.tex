\subsubsection{Belief Propagation}
\label{sec:belief}
Belief-Propagation (BP) is one of the most widely used approximate inference algorithms. One of the reasons for this popularity is its excellent performance
on tasks such as decoding error correcting codes \cite{richardson2001design} and machine vision applications \cite{felzenszwalb2006efficient}.
 
Like mean field, BP is an iterative algorithm, which updates a set of variables at each iteration. However, whereas in mean field the variables are single 
node marginals, in BP they are messages between nodes. Specifically, BP involves
updating messages $m_{i \to j}(x_j)$ that are functions of $x_j$ and are {\em sent} from node $i$ to node $j$.
%The message reflects the source vertex belief on the destination vertex probability.

%In each cycle\footnote{The order of the messages can effect convergence rate, see for example  \cite{elidan2012residual}.} 
%a message is passed from a vertex to all its neighbors. It is a function of the factor connecting the two vertices and the messages from all neighbors except the destination neighbor.
%BP continues to cycle until the difference in the messages is very small, or after a fixed number of cycles.
There are many schedules for updating messages (sequentially, in parallel, or
%U via some
dynamically chosen as in  \cite{elidan2012residual}). In all of these,
the new value for $m_{i \to j}^{t}(x_j)$ is a function of all messages $m^{t-1}_{k\to i}(x_i)$ as follows:
\be
\label{eq:belief_propagation}
m_{i \to j}^{t}(x_j) \propto \sum_{x_i \in\cX} \exp{\theta_{i,j}(x_i,x_j)+\theta_{i}(x_i)}\prod_{k \in \nei{i} \setminus j } m_{k \to i}^{t-1} (x_i)
\ee 

The resulting approximation for the marginals are given by:
\bean
\tau_i(x_i) &\propto& \exp{\theta_i(x_i)} \prod_{k \in \nei{i}} m^T_{k \to i}(x_i) \label{eq:bp_single_marginal}\\
\tau_{ij}(x_i,x_j) &\propto& \exp{\theta_{ij}(x_i,x_j)+\theta_i(x_i)+\theta_j(x_j)} \prod_{k \in \nei{i}\setminus j} m_{k \to i}^{T} (x_i) \prod_{k \in \nei{j}\setminus i}m_{k \to j}^{T} (x_j)\label{eq:bp_pairwise_marginal}
\eean
Note that $\tau_i,\tau_{ij}$ need not correspond to marginals of any distribution $P(\xx)$. In other words, they may be outside the marginal polytope.
%\atodo{cite yedidia for an example of this - i did not see it at yedidia so i cite wainright} 
Thus, they are often referred to as ``pseudo-marginals'' \cite{wainwright2008graphical}, a term we will also use here.


The empirical success of BP has prompted researchers to better understand its theoretical properties, leading to a wide range of elegant results \cite{tatikonda2002loopy,wainwright2003tree,ihler05b}.
%\atodo{cite some}
We describe some of these below, with a focus on the variational perspective.

First, when the graph $G$ is a tree BP is exact, since it essentially implements
dynamic programming on the tree. There are other model families  where the error of BP marginals can be bounded (e.g., ``tree-like'' graphs with long cycles as in  \cite{dembo2010ising}. See also  \cite{heinemann2014inferning} in this thesis).
However, in the general case the quality of the BP approximation is unknown. 
Moreover, there are cases where BP does not converge, and even when it does, the resulting pseudo-marginals may depend on the initial messages.
 Despite these points, BP often yields  good results in practice  \cite{willsky2002multiresolution,kschischang2003codes,loeliger2004introduction}.

We next turn to a variational formulation of BP, as derived in the seminal work of  \citet{yedidia2000generalized, yedidia2003understanding}. Briefly, these results
show that the fixed points of BP are local optima of a certain variational approximation,\footnote{Later,  \cite{heskes2002stable} refined this result, and found that \textbf{stable} fixed point of BP are local minima of the variational approximation.} which starts
at \eqref{eq:variation_A} and approximates both the marginal polytope and entropy term.

% where the authors found that the fix points of BP are local minima of the Bethe free energy of the system\footnote{Later,  \cite{heskes2002stable} refined this result to \textbf{stable} fix point of BP are local minima of the Bethe approximation.}.
%This result not only gave meaning to BP fix points, it also allowed theoretical analysis of BP. 
%I will now present this result; prior to this, some definitions are necessary.

{
We begin by considering an outer bound on the marginal polytope, which will be used to approximate
the marginal polytope in the BP variational approach. Note that any set of marginals in $\margpoly^G$ needs to be consistent, in the sense that $\mu_{ij}(x_i,x_j)$ needs to marginalize to $\mu_i(x_i)$.
This implies that requiring a set of pseudo-marginals to be pairwise consistent yield a set $\lclmargpoly$ such that $\lclmargpoly \supseteq \margpoly^G$.
The formal definition of $\lclmargpoly$ (also known as the {\em local marginal polytope}) is as follows:
%\atodo{Should add $G$ to $\lclmargpoly$, or remove it from $\margpoly^G$.} 
\be
\label{eq:local_polytope}
\lclmargpoly^G = \left\{\tauv \in \Re^d\left| 
\begin{array}{lr}
\forall i \in V & \sum_{x_i} \tau_i(x_i) = 1\\
\forall i \in V, \forall x_i \in \cX,\ \forall j \in \nei{j}& \sum_{x_j}\tau_{ij}(x_i,x_j) = \tau_i(x_i)\\
\forall i \in V,\ \forall ij \in E,\ x_i,x_j \in \cX &\tau_{ij}(x_i,x_j) \geq 0% \tau_i(x_i) \geq 0,\ 
\end{array}\right.\right\}
\ee 
When $G$ is a tree we in fact have $\margpoly^G = \lclmargpoly$.
}

{
%As mentioned earlier, the approximation of \eqref{eq:variation_A} includes two parts.
Next we define the Bethe entropy, which will approximate the entropy term in \eqref{eq:variation_A}. The Bethe entropy $H_B(\tauv) $ is a function of given singleton and pairwise pseudo-marginals $\tauv \in \lclmargpoly$ defined as follows:
\bean
H_B(\tauv) &=& -\sum_{i} (1-d_i)\sum_{x_i}\tau_i(x_i)\log\tau_i(x_i) -\sum_{ij}\sum_{x_i,x_j}\tau_{ij}(x_i,x_j)\log\tau_{ij}(x_i,x_j)\label{eq:bethe_entropy}\\
&=&-\sum_{i}\sum_{x_i}\tau_i(x_i)\log\tau_i(x_i) -\sum_{ij}\sum_{x_i,x_j}\tau_{ij}(x_i,x_j)\log\frac{\tau_{ij}(x_i,x_j)}{\tau_i(x_i)\tau_j(x_j)} \label{eq:bethe_entorpy_information}
\eean
For a tree shaped MRF, with marginals $\tauv$, we in fact have $H_B(\tauv) = H(\thetav(\tauv))$ (see  \cite{yedidia2003understanding})
}


Using the above two definitions, we can now write the so called Bethe approximation to the partition function as:
\be
\label{eq:bethe_approximation}
A_B(\thetav) = \sup_{\tauv \in \lclmargpoly} \left\{\thetav \cdot \tauv + H_B(\tauv)\right\}
\ee
A related function used in the literature is the Bethe energy, which corresponds to $-A_B(\thetav)$.\\

The advantage of the above optimization is that the objective is simple to calculate, and the constraints are simple to check and express via
a polynomial number of inequalities. 

There are however two remaining difficulties. First, the maximizer of $\eqref{eq:bethe_approximation}$ is not necessarily in the marginal-polytope, and hence not a ``real'' marginal. Second, the resulting optimization problem is no longer convex, as the Bethe entropy is a non-convex function.
In fact,  as far as we know, there is no result analyzing the general hardness of the Bethe approximation problem.\footnote{In  \cite{weller2012bethe} a polynomial algorithm is proposed for approximating the Bethe free energy up to arbitrary precision in binary attractive models.}

We are now ready to state the result relating the BP algorithm to the Bethe variational approximation in \eqref{eq:bethe_approximation}.
%The claim by  \cite{yedidia2000generalized} can now be quoted:
\begin{claim}  \cite{yedidia2000generalized}
\label{thm:bp_bethe}
Let  $\mm$ be a set of messages as in \eqref{eq:belief_propagation}, and let $\tauv$ be the calculated pseudo-marginal as in \eqref{eq:bp_pairwise_marginal}.
Then the pseudo-marginals are a fix-point of BP, if and only if they are zero gradient points of the Bethe Free energy \eqref{eq:bethe_approximation}.
\end{claim}
%So the importance of \eqref{eq:bethe_approximation} to inference, is its connection to BP.

The proof is as follows. Writing the Lagrangian of this optimization problem,
\bea
\mathcal{L}(\thetav,\tauv,\lambdav) &=& -\thetav \cdot \tauv - H_B(\tauv) \\
&+& \sum_i \lambda_i \left(1-\sum_{x_i} \tau_i(x_i)\right) + \sum_{i} \sum_{j \in \nei{i}}\sum_{x_i}\lambda_{j \to i, x_i}\left(\tau_i(x_i)-\sum_{x_j} \tau_{ij}(x_i,x_j)\right)
\eea
Using \eqref{eq:bethe_entorpy_information} for the Bethe entropy and remembering that the derivative of $x\log\frac{x}{a}$ is $\log\frac{x}{a}+1$, comparing the derivative to zero gives,\footnote{And using the constraint $\sum_{x_i}\tau_{ij}(x_i,x_j) = \tau_j(x_j)$},
\bea
\log{\tau_i(x_i)} &=& \theta_i(x_i)+ \sum_{j \in \nei{i}} \lambda_{j \to i,x_i}-(d_i-1)+\lambda_i\\
\log{\tau_{ij}(x_i,x_j)} &=&  \theta_{ij}(x_i,x_j) - \lambda_{j \to i,x_i} -  \lambda_{i \to j,x_j} +\log \tau_{i}(x_i) +\log \tau_{j}(x_j) -1
%\frac{\partial \mathcal{L}(\thetav,\tauv,\lambdav)}{\partial \tau_i(x_i)} &=& \theta_i(x_i) - \log{\tau_i(x_i)}-1 - \sum_{j \in \nei{i}} \lambda_{i \to j,x_i}\\
%\frac{\partial \mathcal{L}(\thetav,\tauv,\lambdav)}{\partial \tau_{ij}(x_i,x_j)} &=& \theta_{ij}(x_i,x_j) + \log{\tau_{ij}(x_i,x_j)} + 1 + \lambda_{i \to j,x_i} + \lambda_{j \to i,x_j}
\eea
Taking the exponent and rearranging we have,\footnote{$\lambda_i$ are part of the normalization along with the constants.}
\bea
\tau_i(x_i) &\propto& \exp{\theta_i(x_i)}\prod_{j \in \nei{i}} \exp{\lambda_{j \to i,x_i}}\\
\tau_{ij}(x_i,x_j) &\propto&  \exp{\theta_{ij}(x_i,x_j)+\theta_i(x_i)+\theta_j(x_j)} \prod_{k \in \nei{i}\setminus j} \exp{\lambda_{k \to i,x_i}} \prod_{k \in \nei{j}\setminus i} \exp{\lambda_{k \to j,x_j}}
\eea
which is exactly as in \eqref{eq:bp_single_marginal} and \eqref{eq:bp_pairwise_marginal} when BP converges.
Hence, BP fixed points are local minima of the Bethe energy.

The result in \claimref{thm:bp_bethe} prompted much followup research. These followup works can be roughly divided into three categories:
Extensions of BP, convergence of BP and bounding the error of BP.
 We begin by stating some implications of  \claimref{thm:bp_bethe}, followed by a short review of each category.

In the general case,  BP may have more than one fixed point.
%This empirical fact now has theoretical reasoning.
\claimref{thm:bp_bethe} provides a theoretical characterization of these fixed points.
It states that the local optima of the Bethe free energy are fixed points of BP.
%\atodo{If you want to say  local minimum then cite Heskes, or leave like this - I deleted stable} 
In addition, the Bethe free energy is a non-convex function, therefore multiple minima may exist, which is in agreement
with the empirical observation of multiple fixed points.
%Taken together, this  that BP may have more than one fixed point.

Note that not all BP fixed points are equally good in terms of the optimization problem \eqref{eq:bethe_approximation}.
Give two fixed points, the one with the better objective value may be a solution to \eqref{eq:bethe_approximation} while the other may not.
This goes against the common practice of initializing the messages $\boldsymbol{m}^0$ to a uniform distribution and running BP only once.
Multiple restarts of BP with random initialization may improve the quality of BP results, at least when calculating the partition function.\footnote{How to use the resulting pseudo-marginals of BP when multiple maxima exists can be an interesting research direction.}

The first extension of \claimref{thm:bp_bethe} was the Generalized Belief Propagation algorithm  \cite{yedidia2000generalized}. 
Instead of maximizing the Bethe energy, it maximizes the Kikuchi free energy - an extension from only pairwise interactions to larger subsets. This 
indeed resulted in improved marginal approximation in many cases.


One of the drawbacks of the Bethe approximation is the fact that the objective \eqref{eq:bethe_approximation} is not concave.
Tree Re-Weighted Belief Propagation (TRW)  \cite{wainwright2003tree} proposes an alternative entropy approximation which is concave.
This is achieved by proper weighting of the sum over edges in \eqref{eq:bethe_entorpy_information}.
%Its name comes from the interpretation of these weights.
\ignore{
The model parameter can be decomposed: $\thetav=\sum_{\mathfrak{t}} t_{\mathfrak{t}}\thetav^{\mathfrak{t}}$ such that each $\thetav^{\mathfrak{t}}$ is a sub-model\footnote{$\theta^{\mathfrak{t}}_{ij}$ is either $0$ or $\theta_{ij}$, and $\theta^{\mathfrak{t}}_i =\theta_i$} the structure of which is a tree, and $t_{\mathfrak{t}}\geq 0$, $\sum_{\mathfrak{t}} t_{\mathfrak{t}} = 1$.
The probability on trees induces a probability for each edge to appear in any one of the trees;
these probabilities are weights of the information part.
}
Another nice property of TRW is that the resulting log partition approximation is an upper bound on the true partition function. 
%This can easily be seen by using Jensen inequality\footnote{Remember that the partition function is a convex function of $\thetav$.} $Z(\thetav) = Z\left(\sum_{\mathfrak{t}} t_{\mathfrak{t}}\thetav^{\mathfrak{t}}\right) \leq \sum_{\mathfrak{t}} t_{\mathfrak{t}} Z(\thetav^{\mathfrak{t}})$ which is exactly the TRW approximation to the partition function.
% Both algorithms suggest a variation of BP messages to fit the change in energy.
A unified view of entropy based approximation algorithms are found in \cite{meshi2009convexifying}.

Another line of inference algorithms attempts to optimize the Bethe free energy directly.
For the binary case,  \cite{welling2001belief} and later  \cite{shin2012complexity} present two similar algorithms, but the latter gives time complexity guarantees. A general algorithm was given by  \cite{yuille2002cccp}, which uses a convex concave procedure (CCCP, see  \cite{yuille2002concave}) to optimize the Bethe or Kikuchi energies.

One of the basic desiderata of any algorithm is convergence in finite time. Unfortunately, it is easy to construct cases where the
BP messages do not converge to any limit. This has prompted much research on understanding conditions for guaranteeing BP convergence.
%So the question remain, can convergence be guaranteed under some model's constrains.
The first work to do so was  \cite{tatikonda2002loopy}, where a bound on the pairwise interaction was given to guarantee convergence.
For proving the bound, the computation tree of BP was used.
The computation tree describes the running of BP where each iteration is a level in the tree.
The root value is the pseudo-marginal of a specific vertex at iteration $t$, its direct descendants are the neighbors that send it a messages from iteration $t-1$, and their values are the pseudo-marginals at that iteration.
It continues to go back, until arriving at the leaves which are the initial messages at iteration $0$.
Taking the number of iterations to infinity, if the marginals at iteration $t$ are independent of the initial messages at iteration $0$, BP will converge.
This method has roots in statistical physics, where it is related to the questions of uniqueness of the Gibbs measure, or independence of boundary conditions.\footnote{These notions are important to the understanding of our second paper  \cite{heinemann2014inferning}, where each marginal should be independent of nodes which are at distance $l$.}

\claimref{thm:bp_bethe} is used in  \cite{heskes2004uniqueness} for finding convergence guarantees.
Since insuring the convexity of Bethe free energy implies convergence of BP, they define a set of conditions that guarantee this convexity.
Another method to guarantee BP convergence is the ability to bound the speed at which the messages get closer to each other.
In other words, if the distance (under any distance measure) between any two vectors of messages at time $t+1$ $\mm^{t+1}$,$\hat{\mm}^{t+1}$ is smaller than the distance at time $t$ by a rate that is always strictly smaller than one, then convergence of BP can be guaranteed: $ K d(\mm^{t+1}, \hat{\mm}^{t+1}) \leq d(\mm^t, \hat{\mm}^t)$ where $0\leq K<1$ and $d(\xx,\yy)$ is some distance function.
In both papers  \cite{mooij2007sufficient} and  \cite{roosta2008convergence} this intuition is used to provide bounds for BP convergence.
Note that all the above methods do not find conditions for BP convergence, but rather that BP will have a single fixed point.
This fact will be important in \cite{heinemann2012cannot}, presented in this thesis.

The relation between the Bethe partition function \eqref{eq:bethe_approximation} and the true partition function in \eqref{eq:variation_A} is still an open question.
For example, the Bethe partition function neither upper bounds nor lower bounds $A(\thetav)$ in the general case, but such a bound might exist in specific cases.
One of the first results characterizing BP approximations is \cite{AlanNips2007}, where it was shown that if all the pseudo-marginals have the same orientation (all larger or smaller than $0.5$), then the Bethe partition function lower bounds the exact partition function.
Later \cite{RuozziNips2012} proved a stronger result, showing that if the pairwise interactions are log sub-modular, then the Bethe partition function lower bounds the exact partition function (the case of attractive binary Ising models is an instance of this class).
In their proof, they used the important work of  \cite{vontobel2013counting}, in which the Bethe entropy was given a combinatorial interpretation.

To conclude, the connection between BP and the Bethe variational formulation contributes to the understanding of BP and can result in practical improvements.
The biggest shortcoming of this connection is that it says nothing with respect to BP messages before convergence. Specifically, when BP does not converge,
 the resulting pairwise beliefs are not in the local polytope and thus cannot be directly related to the Bethe free energy. Hence, the Bethe interpretation applies only
 at convergence. This has prompted researchers to develop algorithms that directly minimize the Bethe free energy. See \cite{welling2001belief,yuille2002cccp}.
%%U
%\red{
%So, if BP does not converge, the result is not in the local polytope, and hence cannot be related to the Bethe free energy.
%In other words, BP does not minimize the Bethe free energy, but rather seeks after another objective, one that when reached, minimizes the Bethe free energy.}
%%
Finally, we mention another interpretation as a re-parametrization algorithm. In this view, BP changes the parametrization of the MRF parameters $\thetav$ in order to optimize a bound on the partition function. For more details please refer to  \citet{wainwright2001tree,wainwright2002stochastic}.
\ignore{
\be
\mu_k(x_k;\thetav) = \frac{1}{Z(\thetav)}\sum_{\substack{\xx \\
s.t.\  \xx_k=x_k}}e^{\theta_k(x_k) + \sum_{j \in \nei{k}}\theta_{k,j}(x_k,x_j)}e^{\sum_{i \in V \setminus k}\theta_{i}(x_i) +\sum_{\substack{ij \in E\\
 s.t.\  i,j \ne k}}\theta_{ij}(x_i,x_j)}
%} {\sum_{\hat{x}_k}e^{\theta_k(\hat{x}_k) + \sum_{j \in \nei{k}}\theta_{k,j}(\hat{x}_k,x_j)}e^{\sum_{i \in V \setminus k}\theta_{i}(x_i) +\sum_{\substack{ij \in E\\
% s.t.\  i,j \ne k}}\theta_{ij}(x_i,x_j)}}
\ee
Denote by $\thetav^{\setminus k}$ the model where we remove all factors involve the vertex $k$.
Now the marginal of the neighbors of $k$ in  that model is
\be
\muv_{\nei{k}}(\xx_{\nei{k}}; \thetav^{\setminus k}) \approx \sum_{\substack{\hat{\xx}\\
s.t. \hat{\xx}_{\nei{k}} = \xx_{\nei{k}}}}  e^{\sum_{i \in V \setminus k}\theta_{i}(\hat{x}_i) +\sum_{\substack{ij \in E\\
 s.t.\  i,j \ne k}}\theta_{ij}(\hat{x}_i,\hat{x}_j)}
\ee
 With this we can write
\bea
\mu_k(x_k;\thetav)  &\approx& \sum_{\xx_{\nei{k}}} e^{\theta_k(x_k) + \sum_{j \in \nei{k}}\theta_{k,j}(x_k,x_j)} \muv_{\nei{k}}(\xx_{\nei{k}}; \thetav^{\setminus k})\\
 &\approx& e^{\theta_k(x_k)}  \prod_{j \in \nei{k}} \sum_{ x_j } e^{\theta_{k,j}(x_k,x_j)} \muv_{j}(x_j; \thetav^{\setminus k})\\
\eea
}
In the next section, we describe a different approximate inference approach, based on sampling.

