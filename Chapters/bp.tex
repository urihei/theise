\subsubsection{Belief Propagation}
\label{sec:belief}
Belief-Propagation (BP) is a message-passing algorithm.
In each cycle\footnote{The order of the messages can effect convergence rate, see for example \cite{elidan2012residual}.} a message is passed from a vertex to all its neighbors.
The message reflects the source vertex belief on the destination vertex probability.
It is a function of the factor connecting the two vertices and the messages from all neighbors except the destination neighbor.
BP continues to cycle until the difference in the messages is very small, or after a fixed number of cycles.
The messages may be written as, 
\be
\label{eq:belief_propagation}
m_{i \to j}^{t}(x_j) \propto \sum_{x_i \in\cX} \exp{\theta_{i,j}(x_i,x_j)+\theta_{i}(x_i)}\prod_{k \in \nei{i} \setminus j } m_{k \to i}^{t-1} (x_i)
\ee 
while resulted pseudo-marginals are calculated by,
\bean
\tau_i(x_i) &\propto& \exp{\theta_i(x_i)} \prod_{k \in \nei{i}} m^T_{k \to i}(x_i) \label{eq:bp_single_marginal}\\
\tau_{ij}(x_i,x_j) &\propto& \exp{\theta_{ij}(x_i,x_j)+\theta_i(x_i)+\theta_j(x_j)} \prod_{k \in \nei{i}\setminus j} m_{k \to i}^{T} (x_i) \prod_{k \in \nei{j}\setminus i}m_{k \to j}^{T} (x_j)\label{eq:bp_pairwise_marginal}
\eean

BP is exact when the dependency graph is a tree - the pseudo-marginals are the exact marginals $\tauv^{\thetav} = \muv^{\thetav}$.
There are other model families  where the error of BP marginals can be bound\footnote{Tree-like model is an example of such a family\cite{dembo2010ising}; this fact is utilized in my second paper\cite{heinemann2014inferning}.}. 
However, in the general case the quality of the pseudo-marginals is unknown. 
Moreover, there are cases where BP does not converge, and even when it does, the resulted pseudo-marginals may depend on the initial messages $\boldsymbol{m}^0$.
Despite these points, BP gives good results in practice \cite{willsky2002multiresolution,loeliger2004introduction,kschischang2003codes}.

The first step in understanding BP is to give meaning to the resulted pseudo-marginals.
This was acheived by \cite{yedidia2000generalized, yedidia2003understanding}, where the authors found that the fix points of BP are local minima of the Bethe free energy of the system\footnote{Later, \cite{heskes2002stable} refined this result to \textbf{stable} fix point of BP are local minima of the Bethe approximation.}.
This result not only gave meaning to BP fix points, it also allowed theoretical analysis of BP. 
I will now present this result; prior to this, some definitions are necessary.

As mentioned earlier, the approximation of \eqref{eq:variation_A} includes two parts.
First, let us define the Bethe entropy - an approximation of the true entropy,
\bean
H_B(\tauv) &=& -\sum_{i} (1-d_i)\sum_{x_i}\tau_i(x_i)\log\tau_i(x_i) -\sum_{ij}\sum_{x_i,x_j}\tau_{ij}(x_i,x_j)\log\tau_{ij}(x_i,x_j)\label{eq:bethe_entropy}\\
&=&-\sum_{i}\sum_{x_i}\tau_i(x_i)\log\tau_i(x_i) -\sum_{ij}\sum_{x_i,x_j}\tau_{ij}(x_i,x_j)\log\frac{\tau_{ij}(x_i,x_j)}{\tau_i(x_i)\tau_j(x_j)} \label{eq:bethe_entorpy_information}
\eean
Second, define the approximation of the marginal-polytope with the local-polytope.
\be
\label{eq:local_polytope}
\lclmargpoly = \left\{\tauv \in \Re^d\left| 
\begin{array}{lr}
\forall i \in V & \sum_{x_i} \tau_i(x_i) = 1\\
\forall i \in V, \forall x_i \in \cX,\ \forall j \in \nei{j}& \sum_{x_j}\tau_{ij}(x_i,x_j) = \tau_i(x_i)\\
\forall i \in V,\ \forall ij \in E,\ x_i,x_j \in \cX &\tau_{ij}(x_i,x_j) \geq 0% \tau_i(x_i) \geq 0,\ 
\end{array}\right.\right\}
\ee 
The Bethe approximation to the partition function can now be written.
\be
\label{eq:bethe_approximation}
A_B(\thetav) = \sup_{\tauv \in \lclmargpoly} \left\{\thetav \cdot \tauv + H_B(\tauv)\right\}
\ee
and the Bethe energy is $-A_B(\thetav)$.\\
In contrast to the variational free energy, the Bethe free energy is simple to calculate, since the local polytope is described by a linear number of constraints and the Bethe entropy calculation is straightforward .
This, however, comes with a price.
First, the result does not necessarily belong to the marginal-polytope hence not a ``real'' marginal.
Second, the resulted optimization is no longer convex, as the Bethe entropy is a non-convex function while the true one is convex.
Hence, the time complexity of optimizing this objective is unknown - I am not aware of any result pertaining to the time complexity of Bethe approximation\footnote{In \cite{weller2012bethe} a polynomial algorithm is suggested that approximates the Bethe free energy up to arbitrary precision under some models constraints.}.

The claim by \cite{yedidia2000generalized} can now be quoted:
\begin{claim}
\label{thm:bp_bethe}
Let  $\mm$ be a set of messages as in \eqref{eq:belief_propagation}, and let $\tauv$ be the calculated pseudo-marginal as in \eqref{eq:bp_pairwise_marginal}.
Then the pseudo-marginals are a fix-point of BP, if and only if they are zero gradient points of the Bethe Free energy \eqref{eq:bethe_approximation}.
\end{claim}
%So the importance of \eqref{eq:bethe_approximation} to inference, is its connection to BP.

The proof is as follows. Writing the Lagrangian of this optimization,
\bea
\mathcal{L}(\thetav,\tauv,\lambdav) &=& -\thetav \cdot \tauv - H_B(\tauv) \\
&+& \sum_i \lambda_i \left(1-\sum_{x_i} \tau_i(x_i)\right) + \sum_{i} \sum_{j \in \nei{i}}\sum_{x_i}\lambda_{j \to i, x_i}\left(\tau_i(x_i)-\sum_{x_j} \tau_{ij}(x_i,x_j)\right)
\eea
Using \eqref{eq:bethe_entorpy_information} for the Bethe entropy and remembering the derivative of $x\log\frac{x}{a}$ is $\log\frac{x}{a}+1$, the derivatives compare to zero,  giving\footnote{And using the constraint $\sum_{x_i}\tau_{ij}(x_i,x_j) = \tau_j(x_j)$},
\bea
\log{\tau_i(x_i)} &=& \theta_i(x_i)+ \sum_{j \in \nei{i}} \lambda_{j \to i,x_i}-(d_i-1)+\lambda_i\\
\log{\tau_{ij}(x_i,x_j)} &=&  \theta_{ij}(x_i,x_j) - \lambda_{j \to i,x_i} -  \lambda_{i \to j,x_j} +\log \tau_{i}(x_i) +\log \tau_{j}(x_j) -1
%\frac{\partial \mathcal{L}(\thetav,\tauv,\lambdav)}{\partial \tau_i(x_i)} &=& \theta_i(x_i) - \log{\tau_i(x_i)}-1 - \sum_{j \in \nei{i}} \lambda_{i \to j,x_i}\\
%\frac{\partial \mathcal{L}(\thetav,\tauv,\lambdav)}{\partial \tau_{ij}(x_i,x_j)} &=& \theta_{ij}(x_i,x_j) + \log{\tau_{ij}(x_i,x_j)} + 1 + \lambda_{i \to j,x_i} + \lambda_{j \to i,x_j}
\eea
taking the exponent and rearranging\footnote{$\lambda_i$ are part of the normalization along with the constants.} we have,
\bea
\tau_i(x_i) &\propto& \exp{\theta_i(x_i)}\prod_{j \in \nei{i}} \exp{\lambda_{j \to i,x_i}}\\
\tau_{ij}(x_i,x_j) &\propto&  \exp{\theta_{ij}(x_i,x_j)+\theta_i(x_i)+\theta_j(x_j)} \prod_{k \in \nei{i}\setminus j} \exp{\lambda_{k \to i,x_i}} \prod_{k \in \nei{j}\setminus i} \exp{\lambda_{k \to j,x_j}}
\eea
which is exactly as \eqref{eq:bp_single_marginal} and \eqref{eq:bp_pairwise_marginal} when BP converges.
Hence, BP fix points are local minima of the Bethe energy.

\claimref{thm:bp_bethe} promotes a wide research.
This research can be divided to three categories:
Extensions of BP, convergence of BP and bounding the error of BP.
Some of the direct implications of this claim will now be listed, after which a short review of each category will follow.

As was written before, BP may have more than one fix point.
This empirical fact now has theoretical reasoning. 
\claimref{thm:bp_bethe} states that local minima of the Bethe free energy is a stable fix point of BP.
In addition, the Bethe free energy is a non-convex function, therefore multiple minima may exist.
Taken together, this brings to the result that BP may have more than one fix point.
Note that not all fix points have the same quality, at least regarding the solution of \eqref{eq:bethe_approximation}.
Two fix points where one has a lower Bethe free energy should not be treated the same - the one with the lower energy could be a solution to \eqref{eq:bethe_approximation} while the other could not.
This suggests against the common practice of initializing the messages $\boldsymbol{m}^0$ to uniform distribution and running BP only once.
Multiple running of BP with random initialization may improve the quality of BP results, at least when calculating the partition function)\footnote{How to use the resulted pseudo-marginals of BP when multiple maximum exist can be an interesting research direction.}.

The most immediate extension to BP using \claimref{thm:bp_bethe}, was the Generalized Belief Propagation \cite{yedidia2000generalized}. 
Instead of maximizing the Bethe energy, it maximizes the Kikuchi free energy - an extension from only pairwise interaction to larger areas.
One of the drawbacks of the Bethe approximation is the fact that the Bethe entropy is not a concave function.
Tree Re-Weighted Belief Propagation (TRW) \cite{wainwright2003tree} suggests a surrogate entropy which is concave.
This is acheived by giving weights to the information part (the second summation) in \eqref{eq:bethe_entorpy_information}.
Its name comes from the interpretation of these weights.
The model parameter can be decomposed: $\thetav=\sum_{\mathfrak{t}} t_{\mathfrak{t}}\thetav^{\mathfrak{t}}$ such that each $\thetav^{\mathfrak{t}}$ is a sub-model\footnote{$\theta^{\mathfrak{t}}_{ij}$ is either $0$ or $\theta_{ij}$, and $\theta^{\mathfrak{t}}_i =\theta_i$} the structure of which is a tree, and $t_{\mathfrak{t}}\geq 0$, $\sum_{\mathfrak{t}} t_{\mathfrak{t}} = 1$.
The probability on trees induces a probability for each edge to appear in any one of the trees;
these probabilities are weights of the information part.
This gives another advantage to TRW - the resulted log partition function is an upper bound to the true partition function. 
This can easily be seen by using Jensen inequality\footnote{Remember that the partition function is a convex function of $\thetav$.} $Z(\thetav) = Z\left(\sum_{\mathfrak{t}} t_{\mathfrak{t}}\thetav^{\mathfrak{t}}\right) \leq \sum_{\mathfrak{t}} t_{\mathfrak{t}} Z(\thetav^{\mathfrak{t}})$ which is exactly the TRW approximation to the partition function.
Both algorithms suggest a variation of BP messages to fit the change in energy.
A unified view of these two algorithms (and others) may be found in \cite{meshi2009convexifying}.

Another line of inference algorithms attempts to optimize the Bethe free energy directly.
For the binary case, \cite{welling2001belief} and later \cite{shin2012complexity} give a similar algorithm, but the latter gives time complexity guarantees.
A general algorithm was given by \cite{yuille2002cccp}, which uses CCP\cite{yuille2002concave} to optimize the Bethe or Kikuchi energies.

One of the basic requirements of any algorithm is to be certain that after some time it will return a result.
In case of BP, it means that the messages will converege.
Since a model that BP does not converge on is easy to construct, for BP this translates into finding model constraints that guarantee BP convergence. 
%So the question remain, can convergence be guaranteed under some model's constrains.
The first work to do so was \cite{tatikonda2002loopy}, where a bound on the pairwise interaction was given to insure convergence.
For proving the bound, the computation tree of BP was used.
The computation tree describes the running of BP where each time is a level in the tree.
The root value is the pseudo-marginal of a specific vertex at time $t$ , its direct descendents are the neighbors that send it a message $t-1$, and their values are the pseudo-marginal at that time.
It continues to go back in time, until arriving at the leaves which are the initial messages at time $0$.
Taking the time to infinity, if the marginals in time $t$ are independent of the initial messages at time $0$, BP will converge.
This method comes from the physics world, where it is called uniqueness of Gibbs measure, or independence of boundary conditions\footnote{These notions are important to the understanding of our second paper \cite{heinemann2014inferning}, where each marginal should be independent of nodes which are at distance $l$.}.

\claimref{thm:bp_bethe} is used in \cite{heskes2004uniqueness} for finding convergence guarantees.
Since insuring the convexity of Bethe free energy implies convergence of BP, they define a set of conditions that guarantee this convexity.
Another method to guarantee BP convergence, is the ability to bound the speed at which the messages get closer to each other.
In other words, if the distance (in any distance measure) between any two vectors of messages at time $t+1$ is smaller than the distance at time $t$ by a rate that is always strictly smaller than one, then convergence of BP can be guaranteed $ K d(\mm^{t+1}, \hat{\mm}^{t+1}) \leq d(\mm^t, \hat{\mm}^t)$ where $0\leq K<1$ and $d(\xx,\yy)$ is some distance function.
In both papers \cite{mooij2007sufficient} and \cite{roosta2008convergence} this intuition is used to provide bounds for BP convergence.
Note that all the listed methods do not find conditions to BP convergence, but rather that BP will have a single fix point.
This fact will be important in my paper \cite{heinemann2012cannot}.

The relation between the Bethe partition function \eqref{eq:bethe_approximation} and the exact one \eqref{eq:variation_A} is still an open question.
It is clear that a relation cannot be given in the general case - the Bethe approximation is sometimes bigger and sometimes smaller than the exact partition function.
One of the first results was by \cite{AlanNips2007}, proving that if all the pseudo-marginals are in the same orientation (all bigger or smaller than $0.5$), then the Bethe partition function lower bounds the exact partition function.
Later, \cite{RuozziNips2012} gave a stronger result, showing that if the pairwise interactions are log sub-modular, then the Bethe partition function lower bounds the exact partition function (the case of attractive models is an instance of this class).
In their proof, they used the important work of \cite{vontobel2013counting}, in which a combinatorial meaning was given to the Bethe entropy.

To conclude, the connection between BP and Bethe contributes to the understanding of BP and helps improve it.
The biggest shortcoming of this connection is that it says nothing with respect to BP messages before conversion.
Another interpretation of BP is seeing it as a re-parametrization algorithm, that tries to change the parametrization of $\thetav$ such that it will belong to the local-polytope.  
For full proof please refer to \cite{wainwright2002stochastic}.
\ignore{
\be
\mu_k(x_k;\thetav) = \frac{1}{Z(\thetav)}\sum_{\substack{\xx \\
s.t.\  \xx_k=x_k}}e^{\theta_k(x_k) + \sum_{j \in \nei{k}}\theta_{k,j}(x_k,x_j)}e^{\sum_{i \in V \setminus k}\theta_{i}(x_i) +\sum_{\substack{ij \in E\\
 s.t.\  i,j \ne k}}\theta_{ij}(x_i,x_j)}
%} {\sum_{\hat{x}_k}e^{\theta_k(\hat{x}_k) + \sum_{j \in \nei{k}}\theta_{k,j}(\hat{x}_k,x_j)}e^{\sum_{i \in V \setminus k}\theta_{i}(x_i) +\sum_{\substack{ij \in E\\
% s.t.\  i,j \ne k}}\theta_{ij}(x_i,x_j)}}
\ee
Denote by $\thetav^{\setminus k}$ the model where we remove all factors involve the vertex $k$.
Now the marginal of the neighbors of $k$ in  that model is
\be
\muv_{\nei{k}}(\xx_{\nei{k}}; \thetav^{\setminus k}) \approx \sum_{\substack{\hat{\xx}\\
s.t. \hat{\xx}_{\nei{k}} = \xx_{\nei{k}}}}  e^{\sum_{i \in V \setminus k}\theta_{i}(\hat{x}_i) +\sum_{\substack{ij \in E\\
 s.t.\  i,j \ne k}}\theta_{ij}(\hat{x}_i,\hat{x}_j)}
\ee
 With this we can write
\bea
\mu_k(x_k;\thetav)  &\approx& \sum_{\xx_{\nei{k}}} e^{\theta_k(x_k) + \sum_{j \in \nei{k}}\theta_{k,j}(x_k,x_j)} \muv_{\nei{k}}(\xx_{\nei{k}}; \thetav^{\setminus k})\\
 &\approx& e^{\theta_k(x_k)}  \prod_{j \in \nei{k}} \sum_{ x_j } e^{\theta_{k,j}(x_k,x_j)} \muv_{j}(x_j; \thetav^{\setminus k})\\
\eea
}
